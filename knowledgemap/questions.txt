
Under the KL divergence interpretation of NLL, what about the input variable?
Answer: It's the KL divergence for a fixed input. As in, in a regression problem, image that for one 
input we observe multiple outputs. Then it's the KL divergence for those outputs vs. the model's predicted distribution
for that input




Why is it bad that the normalizing constant is not tractable?
Answer: because you can increase the probability of your data distribution arbitrarily



Still not sure about 'independent errors' assumption for modeling multidimensional distributions.



Information theory - why do we care about encoding sequences of events?


What is the meaning of log probability in machine learning? Is there something about 'encoding the information in a distribution'?


Why does PSD Imply Convex?