Shallow MLPs clip linear functions, rescale, and combine.
D hidden units means D+1 Linear Regions
Multivariate outputs are clipped at the same joints
Multivariate Input Visualization
All ReLU MLPs split input space into Linear Regions
Folding
Adding a Layer is clipping Each Linear Region, and recombining
Bottlenecks are restricting weights to outer product
Depth efficiency is exponential compared to width efficiency
Depth generalizes and trains better
Swishes solve Dying ReLU
Weights can be rescaled as long as biases are too
Depth approximation theorem