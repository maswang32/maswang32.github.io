<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 100vh;
                 background-color: #000000;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 100%;
                 height: 100vh;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#C41E3A", "font": {"color": "white"}, "id": "Math", "label": "Math", "mass": 9.749999999999998, "shape": "dot", "size": 31.22498999199199, "title": ""}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Functions/", "id": "Functions", "label": "Functions", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Functions\n\npolynomials are a linear combination of x, x**2, x**3, as functions\nLast Reviewed: 12/1/24\n"}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Group-Theory/", "id": "Group Theory", "label": "Group Theory", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Group Theory\nA group is a set with an associative binary operation, such that every element in the group has an inverse under that operation, and there is an identity element.\n\nLast Reviewed 4/30/25"}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Real-Analysis/", "id": "Real Analysis", "label": "Real Analysis", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Real Analysis\n\n\nContinuity\nLipschitz Continuity  \n\n\n\n10/6/25"}, {"color": "#c41e3a", "font": {"color": "white"}, "id": "Linear Algebra", "label": "Linear Algebra", "mass": 1.5, "shape": "dot", "size": 12.24744871391589, "title": ""}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Eigenvalues/", "id": "Eigenvalues", "label": "Eigenvalues", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Eigenvalues\n\nSymmetric has real\n\nrelated to correlation - higher eignevalues = more skew\n\n\nLast Reviewed: 10/25/2025\n"}, {"color": "#c41e3a", "font": {"color": "white"}, "id": "Calculus", "label": "Calculus", "mass": 1.5, "shape": "dot", "size": 12.24744871391589, "title": ""}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Infinitesimals/", "id": "Infinitesimals", "label": "Infinitesimals", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Infinitesimals\n{% raw %}\n\nSuppose we have a rectangular approximation to a definite integral with limits $a$ and $b$. We take $N$ evenly spaced points $x_1, \\ldots, x_N$, where $x_1 = a$ and $x_N = b$. This corresponds to $N-1$ rectangles. The area under the curve is approximated as:\n\n$$\n\\sum_{i=1}^N f(x) (x_{i+1} - x_{i}) =\n\\sum_{i=1}^N f(x_i) \\Delta x\n$$\n\nWhere $\\Delta x$ is $x_{i+1} - x_{i}$. \n\nAs we take $\\Delta x \\rightarrow 0^+$:\n\n$$\n\\sum_{i=1}^N f(x_i) \\Delta x \\rightarrow \\int_a^b f(x) dx\n$$\n\nThe $dx$ represents an infinitely small change in $x$.\n\nIt is also why\n\n\n$$\n\\int_a^b dx = b - a\n$$\nSince\n\n$$\n\\lim_{\\Delta x \\rightarrow 0} \\left[ \\sum_{i=1}^N f(x_i) \\Delta x \\right] = b - a\n$$\n\nLast Reviewed: 2/4/25\n{% endraw %}\n"}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Gradients/", "id": "Gradients", "label": "Gradients", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "# Gradients\n\nGradient indicates direction of highest increase\nGradient specifies linearization (plane) of the function up to an offset (derivative gets rid of +C)\nGradient direction specifies plane orientation\nGradient magnitude specifies plane slope\nPlane tells you all directional derivatives\nLast Reviewed: 10/27/24\n"}, {"color": "#c41e3a", "font": {"color": "white"}, "id": "Gradients - UDL", "label": "Gradients - UDL", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Chain-Rule/", "id": "Chain Rule", "label": "Chain Rule", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": "# Chain Rule\n\nReference Page: #1\nUnivariate Chain Rule - \u0027speeding up\u0027 interperation.\n\u0027boosting\u0027 at a point\n\nAll derivatives are evaluated at the same point, just in different input domains\n\nMultivariate chain rule, dx,dy can be separated due to linearization.\nIncreases accumulate across dx, and dy.\n\nextending to multi-in, multi-out\nviewing things in terms of unit changes after linearization.\n\nKey Idea: we can think of moving dx in x, and then moving dy in y,\nand seeing how much f changes. This will be the same as moving in the directional derivative,\nsince for linear functions, the slope is the same everywhere.\n\nKey Idea: to compute df/ds, linearize everything, move one unit in s, and see how much that affects f.\n\nThe linearity assumption is the assumes that changes in variables will affect the output independently.\n\nwherever a function has a derivative, it is locally linear\n\nLast Reviewed: 10/27/24\nReference Page: #1"}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Information-Theory/", "id": "Information Theory", "label": "Information Theory", "mass": 2.25, "shape": "dot", "size": 15.0, "title": "  # Information Theory\n\nInformation is -logP(x) for an event\nIndependent events have additive infomration\nLess likely events have higher Information\nknowing outcome of an event with 50% prob has 1 bit of information\nMeasured in nats or bits (recall logs of all bases are proportional)\n0 information if certain\n\nsetup: a bitstream encodes a sequence of random vairables. Prefix requirements impose a cost of 2^l\n\n\nThink about information as like, what is a signal? you can have a prior distribution about what distribution the signal is drawn from, letting you encode it better. If you process it through neural network layers you will only lose information (expand the distribution of possible signals it could have been).\n\nHow much this distribution of possible signals EXPANDs (or how EXPANDED it is) is called entropy.\n\n\n\n\n\n\n\nLast Reviewed: 10/27/24\nReference Sheet #3, 3.1"}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\posts\\A-Brief-Introduction-To-Information/", "id": "A Brief Introduction To Information", "label": "A Brief Introduction To Information", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# A Brief Introduction To Information\n\n\n- All information is communication - it requires a method of decoding, must be interpreted.\n- All digital info = bits\n- Setup: communicate a sequence of random events\n- The streaming setting (where bits are decoded as they come) requires disambiguous prefixes - I proved this.\n\n[Original Blog Post](https://calvinyluo.com/2019/03/19/a-brief-introduction-to-information.html)\n\nLast Reviewed: 11/9/24\n"}, {"color": "#c41e3a", "font": {"color": "white"}, "id": "Deep Learning Chapter 3", "label": "Deep Learning Chapter 3", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Mutual-Information/", "id": "Mutual Information", "label": "Mutual Information", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Mutual Information\n\nE_p(x,y) log (p(x,y)) / p(x) p(y)\n\nor\n\nKL(p(x,y) || p(x)p(y))\n\nit\u0027s symmetric\n\n\ninformation provided about one variable about another\n\n\n![MI](info-1.png)\n\n\n\nMI is less than entropy of either varaible\nMI = entropy1 + entropy2 - joint entropy\nMI \u003e= 0\n\nprocessing cannot increase information\n\n![alt text](info-2.png)\n\nwater pipe analogy\n\n![alt text](info-3.png)\n\n\nneural networks can only lose information\n\n\n\nX -\u003e Y -\u003e Z\n\nMI(X,Y) \u003e= MI(X,Z). pipe analogy works here too.\n\n![alt text](info-4.png)\n\n\n### Supervised Learning\nMI between input and GT label is greater than or equal to MI between pred label and GT label (data  processing inequality - MI only goes down after processing)\n\n![alt text](info-5.png)\n\ncross entropy loss encourages retaining MI about class label\n\n\ncan also be used in contrastive learning\n- positive pairs are sampled from joint, negative pairs are from product of marginals\n(skipped)\n\ncontrastive learning - maximizing MI between image an augmented views\n\n\nLast Reviewed: 10/26/2025\n"}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Rate-Distortion-Theory/", "id": "Rate Distortion Theory", "label": "Rate Distortion Theory", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Rate Distortion Theory\n\n\nautoencoder\n\ndistortion function = d(x, xhat)\nR = average bits required to represent realization (e.g. number of channels)\n\nR(D) - minimum rate needed (R) such that the expected distortion doesn\u0027t exceed D.\n\n= infimum of the MI(X,Xhat) (input and reconstruction)\nyou want to minimize the MI between X and Xhat while maintaining the distortion constraint\n(analogous to error being orthogonal to the line you fit)\n\nR(D) must be equal to the entropy of the input to get perfect reconstruction\n\n\nnarrowest bottleneck constrains information through the network.\n\nnarrowest pipe constrains water flow.\n\n\n\nLast Reviewed: 10/26/2025\n"}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\KL-Divergence/", "id": "KL Divergence", "label": "KL Divergence", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": "# KL-Divergence\nAsymmetric\nDKL(P||Q) - symbols are drawn from P, but if we encode assuming drawn from Q, \nhow many extra bits on expectation are used\nEx~p[log(P(x))-log(Q(x))]\nAsymmetric b/c depends on which distribution you\u0027re sampling from - two examples\n(Should read more)\n\nhow much more suprised you\u0027d be seeing P while expecting Q\nLast Reviewed: 10/27/24\nReference Page: #2\n"}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\posts\\Six-Interpretations-of-KL-Divergence/", "id": "Six Interpretations of KL Divergence", "label": "Six Interpretations of KL Divergence", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# An Interpretation of KL Divergence\nImagine a lottery game. Let $X$ be a random variable describing the outcome of the lottery game, and $x$ be a realization of that random variable.\n\n- For a bet of $c$ on an outcome $x$, the house pays you $\\frac{c}{q(x)}$ where $q(x)$ is the probability they assign to the outcome $x$.\n- This is the optimal way for *them* to make money, if they believe the true distribution is $q(x)$.\n\nNow, let\u0027s talk about what you do as a player:\n- Suppose you know the true distribution of outcomes $p(x)$.\n- To maximize your winnings, you should bet proportional to $p(x)$.\n- Suppose you bet 1 dollar total.\n- Then you optimally bet $p(x)$ dollars for each outcome.\n\nYour expected log-winnings are:\n\n$$\n\\sum_x\\left[ p(x) \\log \\left(\\frac{p(x)}{q(x)} \\right)\\right]\n$$\n\nThis is actually the formula for KL-divergence.\n\nIn other words, $D_{\\text{KL}(p,q)}$ is the maximum amount of log-money that can be made off one dollar, when the payoffs are assigned by the distribution $q$, but the real distribution is $p$.\n\n\n\u003cspan style=\"color:blue\"\u003eTo Do: review other interpretations\u003c/span\u003e.\n\n[Source](https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence)\n\nLast Reviewed: 1/20/25\n\n\n\n\n"}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Entropy/", "id": "Entropy", "label": "Entropy", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Entropy\nExpected information in a distribution\nmeasures uncertainty in a probability distribution\nBernoulli Example\n\nexpected surprise, where surprise is -log(p)\n\n\nJoint entropy - uncertainty of joint distribution\nconditional entropy - uncertainty of X given Y, if you can observe one variable, how much surprise is there in the other. \n\nNote that you actually integrate over the joint distribution too - it\u0027s not just the entropy of the conditional distribution\n\nLast Reviewed: 10/27/24\nLast Reviewed: 10/26/24\n\nReference Sheet #3. "}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Cross-Entropy/", "id": "Cross Entropy", "label": "Cross Entropy", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Cross Entropy\n\n-\u00e2\u02c6\u00ab(p(x) log(q(x) dx))\n\nEntropy is:\n-\u00e2\u02c6\u00ab(p(x) log(p(x) dx))\nCall this Ent(p, p)\n\nCross Entropy is:\n-\u00e2\u02c6\u00ab(p(x) log(q(x) dx))\nCall this Ent(p, q)\n\nKL Divergence is:\n- \u00e2\u02c6\u00ab(p(x) log(q(x) dx)) - (-\u00e2\u02c6\u00ab(p(x) log(p(x) dx)))\nOr\n\u00e2\u02c6\u00ab(p(x) log(q(x) dx))  + \u00e2\u02c6\u00ab(p(x) log(p(x) dx))\n\nThis is Ent(p, q) - Ent(p,p)\n\nWhen we add KL divergence and entropy, we get cross entropy\nCross entropy = number of bits it takes to encode samples from P using an encoding trained on Q\nEntropy = number of bits it takes to encode samples from P using an encoding trained on P\nKL Divergence = number of extra bits it takes to encode samples from P using an encoding trained on Q.\nOr, KL divergence is cross entropy minus entropy.\nLast Reviewed: 1/20/25\n"}, {"color": "#FF6F20", "font": {"color": "white"}, "id": "Statistics", "label": "Statistics", "mass": 4.500000000000001, "shape": "dot", "size": 21.213203435596427, "title": ""}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Biased-vs-Unbiased-Estimates/", "id": "Biased vs Unbiased Estimates", "label": "Biased vs Unbiased Estimates", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Biased vs Unbiased Estimates\nIf we take the sample variance, it is actually an underestimate of the true variance.\nThat\u0027s because we\u0027re using the sample mean, which will be closer to the true mean.\nMeaning we also have to account for the variance in the sample mean."}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Uniform-Width-Sampling/", "id": "Uniform Width Sampling", "label": "Uniform Width Sampling", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Uniform Width Sampling\n\nSuppose you would like to sample k out of N classes randomly.\n\nInstead of doing two stage sampling, you can generate N scores, then threshold those scores by k/N, where k is the desired number of classes.\n\nLast Reviewed 4/30/25"}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Random-Variables-and-Probability-Distributions/", "id": "Random Variables and Probability Distributions", "label": "Random Variables and Probability Distributions", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Random Variables and Probability Distributions\n\n## Definition of Random Variable\n\n\u003c!-- ### Practical Definition\nIn machine learning, it suffices to think of a random variable simply as a \u0027variable\u0027, or a placeholder for a number or vector. There are notions of probability and randomness associated with it, but these can be associated with other constructs, like probability distributions.\n\nFor instance,\n\n$$\ny = 2z\n$$\nMeans \u0027take the value that $$z$$ takes, and multiply it by $$2$$ to get $$y$$. There is no concept of \u0027randomness\u0027 yet introduced. --\u003e\n\n### Precise Definition\n\nA random variable $$X$$ is a function from a sample space $$\\Omega$$ to a set of outcomes $$E$$.\n\nFor instance, we can assign a random variable to the result of a dice roll, and call it $$X$$. In this case, the sample space is:\n$$\\Omega = \\{1,2,3,4,5,6\\}$$\n\nThe random variable $$X$$ is a function such that $$X(\\omega) = \\omega$$, for $$\\omega \\in \\Omega$$.\n\n\u003c!-- It might seem redundant to define a random variable as a function, but it is conceptually useful because functions can take on multiple values depending on their input. If we think of $$X$$ as a function, it is easier to cope with the possibility that $$X$$ may take on more than one value. --\u003e\n\n### Operators on Random Variables\nIf we assign\n$$\nY = 2X\n$$\n\nWe can think of multiplication by $$2$$ as an operator on the function $$X$$ (An operator takes in a function and provides another function).\n\nWe are really saying that $$Y$$ is a new random variable (a new function), $$y \\mapsto Y(\\omega)$$, such that\n\n$$\nY(\\omega) = 2X(\\omega) \\quad \\forall \\omega.\n$$\n\nIf $$X$$ is a random variable representing the value of a dice roll, $$Y$$ is a random variable representing twice the value of a dice roll.\n\n### More notes on notation\n#### Capital vs. Lowercase\nWhile random variables are represented with capital letters, we typically use the corresponding lower case letter to denote a realization of that random variable. In math terms, we use $$x$$ to denote $$X(\\omega)$$. \n\n$$\nx = X(\\omega)\n$$\n\nWhile $$X$$ is a function, $$x$$ is a value.\n\nThis is similar to how in mathematicals more generally, if $$f$$ refers to a function, then $$f(x)$$ refers to a value, namely, the output of $$f$$ when its input is $$x$$, although this distinction is [blurred frequently](https://en.wikipedia.org/wiki/Abuse_of_notation#Function_notation).\n\n#### Bold vs unbolded\nBolded random variables and their values (realizations) simply indicate that the random variable is vector-valued.\n\n\n\n### Sample Spaces in Machine Learning\nThe sample space is not usually referenced in machine learning. For instance, we might have a latent variable $$\\mathbf{Z}$$ in a latent variable model. If $$\\mathbf{z} = \\mathbf{Z}(\\mathbf{w}) \\in \\mathbb{R}^d$$, the sample space $$\\Omega$$ is $$\\mathbb{R}^d$$, and we think of $$\\mathbf{Z} : \\Omega \\rightarrow \\mathbb{R}^d$$ as $$\\mathbf{Z}(\\omega) = \\omega$$.\n\n\n## Probability Distributions\nA probability distribution is a maps random variable ***values*** to densities. Formally, a random variable $$\\mathbf{X}$$ is a function with an output domain (often $$\\mathbb{R}^d$$), and the probability density function maps the output domain of $$\\mathbf{X}$$ to density values in $$\\mathbb{R}$$.\n$$\np(\\mathbf{x}) : \\mathbb{R}^d \\rightarrow \\mathbb{R}.\n$$\n\nAs an exercise in notation, this means we should also be able to write:\n$$\np\\left(\\mathbf{X}(\\mathbf{\\omega})\\right) : \\mathbb{R}^d \\rightarrow \\mathbb{R}.\n$$\n\nWe abbreviate \"probability density function\" as \"PDF\".\n\n\n#### Note\nIt is not the case that a random variable has a single probability distribution, although we often reference a \"true\" probability distribution. Rather, a probability distribution is simply a mapping from a random variable\u0027s value to a density value.\n\n### Notes on Notation\n#### $$Pr(x)$$ vs $$p(x)$$\nTypically, $$Pr(A)$$ refers to the probability of event $$A$$, while $$p(x)$$ refers to the value of the PDF at a data point $$x$$. Not everyone uses this notation, though.\n\n#### Resolving function vs. value dilemma\nWe often use $$f(x)$$ to refer to the function $$f$$, instead of the value of $$f$$ at $$x$$. This is also true in statistics. We commonly use\n$$\np(\\mathbf{x})\n$$\n\nTo refer to the probability distribution $$p$$, even though it should denote the density value of the probability distiribution at the point $$\\mathbf{x}$$. This is used ubiquitiously, and perhaps the inclusion of $$\\mathbf{x}$$ helps specify that the distribution\u0027s input are realizations of the random variable $$X$$. Sometimes, this is important because $$p$$ may represent a *family* of distributions, not just a single probability distribution.\n\n\n#### Families of Probability Distributions\nIn machine learning, when we write something like $$p_\\theta$$, we are typically referring to a *family* of probability distributions, not just a single distribution.\n\nFor instance, if we draw $$\\mathbf{x_1}, \\ldots , \\mathbf{x}_N$$ independently from a data distribution, we can write\n\n$$\np_\\theta(\\mathbf{x_1}, \\ldots , \\mathbf{x}_N) = \\prod_{i=1}^N p_\\theta(\\mathbf{x_i}).\n$$\n\nIn this case, the left hand side refers to the probability of observing $$\\mathbf{x}_1, \\ldots , \\mathbf{x}_N$$ according to the joint distribution given by the parameters $$\\theta$$. We use $$p_\\theta$$ to refer to both the joint *and* marginal distributions. They are intertwined by the rules of probability, e.g., the chain rule.\n\n\n\n\n### Distributions \u0027over\u0027 random variables.\nIf we write\n$$\n\\mathcal{N}_x(0,I)\n$$\nThe $$x$$ in the subscript indicates that the distribution is \"over\" the random variable realization $$x$$, as opposed to another random variable. More precisely, $$\\mathcal{N}_x(0,I)$$ is function mapping $$x = X(\\omega)$$ to $$\\mathbb{R}$$. The $$x$$ denotes what we use as the input to this function.\n\nThis is useful to disambiguate the input to the PDF when we have several PDFs.\n\n#### Example\n\nSuppose we have \n\n$$\np(\\mathbf{x}) = \\int \\mathcal{N}_\\mathbf{x}(f(\\mathbf{z}), I) \\cdot N_\\mathbf{z}(0,I) d\\mathbf{z}.\n$$\nIn this case,\n\n\n- $$p(\\mathbf{x})$$ is a function mapping $$\\mathbf{x}$$ to probability values.\n- For a given input $$\\mathbf{x}$$, we would substitute that value of $$\\mathbf{x}$$ into $$\\mathcal{N}_\\mathbf{x}(f(\\mathbf{z}), I)$$ to get a density value.\n- The value of $$\\mathbf{z}$$ we would use is determined by the integrand.\n\n\nTo evaluate $$p(\\mathbf{x})$$ for a specific $$\\mathbf{x}$$, we would:\n1. Iterate through all values of $$\\mathbf{z}$$\n2. Plug in $$f(\\mathbf{z})$$ for the these values to get the parameters (mean and variance) for the distribution $$\\mathcal{N}_\\mathbf{x}$$.\n3. Plug in $$\\mathbf{x}$$ into this distribution to get a probability density value.\n4. Plug in $$\\mathbf{z}$$ into $$\\mathcal{N}_\\mathbf{z}(0,I)$$ to get a probability density value\n5. Multiply the two density values together (the outputs of the two normal distributions) and accumulate it over all $$\\mathbf{z}$$ to evaluate the integral.\n\nThe parameters $$f(\\mathbf{z}), I$$ are how we describe the function that is the probability distribution.\n\nLast Reviewed: 1/31/25"}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Bayes/", "id": "Bayes", "label": "Bayes", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Bayes\nWe can ignore terms that are constant with regard to the distribution we are computing.\nFor instance, for a fixed x*,\n\np(z | x*)  = (p(x* | z) p(z)) / p(x*)\n\nbut we can ignore p(x*) since we are interested in a distribution with respect to z.\nTo get this distribution, we can evaluate p(x* | z) p(z) at all z and ensure it integrates to 1\nby rescaling it by C\nignoring the need for p(x*) term (which is 1/C).\n\nTO DO: Find notes about \u0027evidence\u0027 in Bayes\n\n\nP(Hypothesis | data) = P(Data | Hypothesis) * P(Hypothesis) / P(Data)\n\n\nPosterior = Likelihood (of data) * Prior (probability of Hypothesis) / Probability of Data\n\nPosterior = Likelihood * Prior / Evidence\n\n\nLikelihood = likelihood of data give hypothesis, multiplies posterior probability\n\nHypothesis = Prior probability of Hypothesis, multiplies the posterior probability of hypothesis\n\nData = probability of data, the lower this is, the higher the posterior probability of hypothesis\n\nLikeli\n\n\n\n\nLast Reviewed: 1/25/25\n\n\n"}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Conditional-Independence/", "id": "Conditional Independence", "label": "Conditional Independence", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Conditional Independence\nLast Reviewed: 1/25/25\n\ne.g., in diffusion models, we have\n\nq(z_2 | z_1, x) = q(z_2 | z_1)\n\nsince z_1 provides all information needed to compute z_2, thus given z_1 as information,\nz_2 is independent from x, or x provides no \u0027additional information\u0027."}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Covariance/", "id": "Covariance", "label": "Covariance", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": "# Covariance"}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Running-Covariance-Algorithms/", "id": "Running Covariance Algorithms", "label": "Running Covariance Algorithms", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Running Covariance Algorithms\n\nLast Reviewed: 6/26/25"}, {"color": "#ff6f20", "font": {"color": "white"}, "id": "Optimization", "label": "Optimization", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": "#ff6f20", "font": {"color": "white"}, "id": "Optimization - UDL", "label": "Optimization - UDL", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Momentum,-RMSProp,-Adam/", "id": "Momentum, RMSProp, Adam", "label": "Momentum, RMSProp, Adam", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Momentum, RMSProp, Adam\n\nNotes from \"A visual explanation\"\n\nMomentum in Physics - F = ma, a force will cause a constant change in velocity.\nSame as momentum in ML - momentum = velocity, \nforces = decay (friction), and the additional gradient\nderivative = applying a force for one time frame, leading to an acceleration (change in velocity)\nmomentum helps with plateaus and local minima\n\n\nAdaGrad - history of squared gradients for a direction accumulate, updates in that direction are divided by this\n= encourages exploration in directions where not many changes have happened\n- escapes saddle points better by going diagonally\n- regular GD optimizes steeper features first\n- slow b/c squared gradient accumuates\n\n\nRMSProp - squared gradients decay, squared gradients have momentum\n\nAdam - gradients have momentum, so do squared gradients.\n- momentum allows for escaping local minima\n- sum of squares = explore new directions\n\n\n\nNotes from Andrew NG:\nMomentum cancels oscillations\nCorrections are usually applied to Adam so things get rolling earlier\n \nToo much momentum = oscillations\nLast Reviewed: 11/9/24\n\n\n\n\n\n"}, {"color": "#ff6f20", "font": {"color": "white"}, "id": "Machine Learning", "label": "Machine Learning", "mass": 1.75, "shape": "dot", "size": 13.228756555322953, "title": ""}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\PCA/", "id": "PCA", "label": "PCA", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# PCA\n\none number = linear regression.\n\nPC1 = main point\nPC2 = \u0027nuance\u0027.\n\n\nrepresents data by largest k components.\n\n\nhas reconstruction error - minimize\n\nLinear encoder and decoder, orthogonal constraint, with autoencoding.\n \ne.g. EigenFace\n\nPCA looks like DCT basis\n\n\n\nLast Reviewed: 10/25/2025\n"}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Linear-Classifiers/", "id": "Linear Classifiers", "label": "Linear Classifiers", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": "# Linear Classifiers\nFrom CS 231N\nA linear classifier works like this:\n- A neural network (a single linear layer) produces a score, and a loss function maps this score to an \u0027agreement\u0027 value with the class label.\n- We minimize the loss with respect to the parameters of the score function\n- If you have $ W\\mathbf{x} + b$, that  is like evaluating $K$ classifiers separately one for each class. Each classifier is a row of $W$.\n    - This is like template matching using the dot product, or \u0027one template\u0027 KNN - where there is one image per class, and distance is the dot product, not L2 or L1 distance.\n    - It is much faster than KNN since you do not have to compute distances to all the training set points.\n\n- The weights indicate the directionality of the relationship, for each pixel - like a positive weight on blue means we want that pixel to be blue.\n\n- Each row of $W$ is a hyperplane, with a normal vector indicating the direction of increase, and \"on the plane\" meaning 0.\n    - (Is the template in the same direction as the average image?)\n\n- if you have multiple layers, maybe earlier layers detect specific cars (e.g. green, blue) and the NN is a weighted sum of individual car detectors.\n- zero mean centering is more important than scaling (why)?\n\n## SVM\n\n- Wants corrrect class to have a bigger score than all the incorrect classes by some margin $\\Delta$.\n$$ L_i = \\sum_{j\\neq y_i} \\max (0, s_j - s_i, \\delta) $$\n- There is a loss on all the scores that are not class $i$, and the loss is how much bigger they are than class $i$\u0027s score minus $\\Delta$.\n- If the score for the incorrect class is less than the correct class by $\\Delta$, there is no loss here!\n- There is also quadratic hinge loss, where each term in the sum is squared\n\n### Regularization\n- Weight magnitude is underdetermined, if you class all correctly, weight can be any scasle\n- Normalize with respect to L2 Norm\n- $L = \\frac{1}{N} \\sum_i L_i + \\lambda R(W) $\n- Don\u0027t regularize biases\n- Improves generalization by requiring dependency on all inpus\n\nWe can set $\\Delta$ to $1.0$ and only tune $\\lambda$, due to the weight magnitude thing.\n\n## Softmax\n\n\n$$\nL_i = - \\log \\frac{e^{s_{y_i}}}{\\sum_k e^{s_k}}\n$$\n$$\n= - s_{y_i} + \\log \\sum_k e^{s_k}\n$$\n\n- Scores are interpreted as unnormalized log probabilities\n- shift by max\n- Regularization is a Gaussian Prior on the weight matrix.\n- Large $\\lambda$ means more diffuse probabilities\n- Perfomance difference between SVM and softmax is small.\n- SVM is more local - it stops trying onces scores are good enough, only care about scores near the margin.\n- A car classifier shouldn\u0027t focus on lower the probability of classifying ducks as cars even more, it should focus on distinguishing between cars and trucks.\n- this can be an arg in favor of SVM.\n\n\nLast Reviewed: 10/28/2025"}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\SVM/", "id": "SVM", "label": "SVM", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# SVM\n\nuses hinge loss\n\nminimize max(0, 1 - y * f(x))\n\nmax(0, 1 - y_i (w * x_i + b))\n\n\n1 - y_i y_hat_i\n\nif y_i is 1, this is 1 - score\n\nif it is -1, this is 1 + score\n\n\"1\" is the margin\n\nwith no margin, when GT is positive, y_hat_i wants to be POSITIVE (- score will be negative)\nwith no margin, when GT is negative, y_hat_i wants to be large, negative (+ score will be small)\n\n\nwhen we have the margin, 1 - score and y_i positive, then if we are super correct (y_hat_i \u003e 1) there  is NO LOSS\nwhen we have the margin, 1 + score is the loss (y_i negative), then there is NO LOSS  if score  is \u003c -1 (we are super correct above a margin.) If the score  is -0.5, there  is still a loss!\n\nsee linear classifier notes\n\n\n\nLast Reviewed: 10/28/2025"}, {"color": "#ff6f20", "font": {"color": "white"}, "id": "K-Nearest Neighbors", "label": "K-Nearest Neighbors", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": ""}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\K-Means/", "id": "K-Means", "label": "K-Means", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": " - represent the data by K vectors.\n- This can also be viewed as the minimization of a loss function\n- can be thought of a reconstruction error.\n- in assignment, we minimize the assignments to minimize error (fix c, optimize i)\n- in other step, fix indexing of all datapoints, then update its center.\n- type of autoencoder.\n- latent representation = one hot vector\n- works on MNIST - kmeans learns numbers\n- can run on 8x8 image patches\n\nLast Reviewed: 10/25/2025\n"}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Approximate-Nearest-Neighbors/", "id": "Approximate Nearest Neighbors", "label": "Approximate Nearest Neighbors", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Approximate Nearest Neighbors\n\ncluster all N documnets into sqrt(N) groups (same number of clusters as documents in each cluster)\n\ninverted list per cluseter that enumerates all documents in that cluster\n\nthen look at the ones in the clusters (nearest few)\n\n\n\n\nLast Reviewed: 10/26/2025\n"}, {"color": "#a8326f", "font": {"color": "white"}, "id": "Signal Processing", "label": "Signal Processing", "mass": 3.5000000000000004, "shape": "dot", "size": 18.708286933869708, "title": ""}, {"color": "#a8326f", "font": {"color": "white"}, "id": "DDSP", "label": "DDSP", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\PQMF/", "id": "PQMF", "label": "PQMF", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# PQMF\n\nFilter signal into low band and high band\nDownsample both\nupsample both\nlow pass low band, high pass subband\nin the downsampled signal, the high frequencies are mirrored, and occupy the low frequencies.\nSpecial case of audio CNNs\nSee PQMF.ipynb\nLast Reviewed: 1/2/2024\n"}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Downsampling-and-Stretching/", "id": "Downsampling and Stretching", "label": "Downsampling and Stretching", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Downsampling and Stretching\nLast Reviewed: 1/2/2025\n\nDownsampling \u0027folds\u0027 the FFT spectrum on itself.\ne.g. downsampling by factor of 2 - imagine slicing spec. in half,\nthen overlaying them.\n\nStretching replicates the FFT spectrum (doubles length of FFT)\nstretching by x2 mirrors spectrum,\nstretching by x3 appends the forward spectrum again\netc."}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Convolution/", "id": "Convolution", "label": "Convolution", "mass": 1.2500000000000002, "shape": "dot", "size": 11.180339887498949, "title": "# Convolution\n\nConvolution in Neural Networks:\nDilated Convolution = replicating spectrum of filter\ndilated kernel size = (kernel_size - 1) * dilation + 1\nleads to higher frequency resolution (number of unique points)\nstrided convolution = conv plus downsampling\n\n\nconvolution is correlating patches of image with filter - looking for patterns you see in the filter.  \n\n## Downsampling\nnote that before downsampling, trailing entries are discarded.\n# of trailing entires discarded = stride - 1\ntherefore, the target size after conv only needs to be size - (stride - 1).\nTherefore, the input size after padding needs to be size - (stride - 1) + (kernel - 1), since the kernel takes away kernel - 1 units\nThis is equal to size + kernel - stride, so the padding needs to be (kernel - stride)/2.\nif stride is even, we therefore want an even kernel.\n\n\nGraph:\n   X X X X X X X X    - input\n[] X X X X X X X X [] - input after padding\n    X X X X X X X     - after conv, kernel size 4\n    X   X   X   X     - after downsampling\n\n\n- Note that in EDM, we downsample and THEN convolve.\n\nOtherwise:\nTwo interpretations:\n1---reverb (overlapping kernels)\n2---flipping and shifting\nthe \u0027flipped\u0027 kernel is a function. The x is the \u0027offset\u0027 and the y is the \u0027weight\u0027.\ni.e., how does input at time t + x affect output at time t.\n\n\n## More Params\n- Num groups - the weight is in_channels/groups\n\nLast Reviewed: 1/3/25"}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Transpose-Convolution/", "id": "Transpose Convolution", "label": "Transpose Convolution", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Transpose Convolution\n\nWhen stride = 1, it\u0027s overlaying kernels\nThis is the \u0027superposition\u0027 interpretation of convolution\nit is the same as convolution with a flipped kernels\n\nWhen stride \u003e 1, it\u0027s the same as conv(stretch(x))\n\nSee the notebook you emailed Julius\n\nFilter the \u0027mirrored\u0027 copy of the signal\n\nGradient of Conv (makes forward and backwards same, natural way of upsampling)\n\nthe padding parameter is the same as truncating on both sides\nLast Reviewed: 1/3/25\n"}, {"color": "#a8326f", "font": {"color": "white"}, "id": "Fourier Transform", "label": "Fourier Transform", "mass": 1.2500000000000002, "shape": "dot", "size": 11.180339887498949, "title": ""}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Discrete-Fourier-Transform/", "id": "Discrete Fourier Transform", "label": "Discrete Fourier Transform", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Discrete Fourier Transform\n\n- It is a least-squares projection onto an orthogonal basis of complex sinusoids.\n- Thus, taking two mirror DFT bins provides the sinusoid of that frequency that minimizes the least-squares error to the original signal, in terms of amplitude and phase.\n\n## RFFT\n- The Real DFT on an even-length signal has 1 DC, 1 Nyquist.\n- For instance, it goes from 256 length to 129 RFFT length. Bin 0 is DC, last bin (127) is nyquist\n- Or, for a 4-point DFT, the frequencies are 0, 1, nyquist, -1.\n\n\n\nLast Reviewed: 4/30/25\n\n\n\n"}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Fourier-Dualities/", "id": "Fourier Dualities", "label": "Fourier Dualities", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Fourier Dualities\n\nWhy is multiplication in time convolution in frequency?\n\nThink of representing your signal as a sum of complex sinusoids at different frequencies.\n\nThis is a polynomial!\n\nThe coefficients are the frequency domain representation\n\nWhat happens when you mutiply two polynomials?\n\nYou convolve their coefficients.\n\nSee emails with Julius Smith\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Pink-Frequency-Profiles/", "id": "Pink Frequency Profiles", "label": "Pink Frequency Profiles", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": "# Pink Frequency Profiles\n\n\n- Lots of things display $1/f^{\\alpha}$ frequency spectrums.\n- These are linear on a log-log scale (logscale both frequency and amplitude)\n\nThis corresponds to fractal structure:\n- Picture sinusoid at a low frequency. Now, imagine \"zooming out\" 2x on this sinusoid in only the x-direction. What you\u0027ll see will look like a sinusoid of the same amplitude, but 2x the frequency. If we keep doing this, we\u0027ll get sinusoids of 1x, 2x, 3x... the frequency, but of equal amplitude. This will give us a flat frequency profile if we add them all together.\n\n= Now, imagine \"zooming out\" in both the x and the y directions. We\u0027ll see a sinusoid of k times the frequency, and 1/k the amplitude if we zoom out by a factor of k. If we add these sinusoids together, for k from 0 to N, we\u0027ll get an overall signal with a 1/f frequency profile.\n\n- This behavior is closer to self-similarity or a fractal structure, since the summed signal is a sum of sinusoids that are all the same shape, just on different scales.\n\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\1f-Noise-in-Music-and-Speech/", "id": "1f Noise in Music and Speech", "label": "1f Noise in Music and Speech", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# \u00271/f noise\u0027 in music and speech\n\nThe power spectrum is often $f^{-\\alpha}, 0.5 \\leq \\alpha \\leq 1.5.\n\nLoudness, pitch, melody exhibit this behavior.\n\n## Autocorrelation Functions\nIf $\\langle v(t), v(t+\\tau) \\rangle$ is correlated (non-zero expectation) for $|\\tau| \u003c T$, it is \"white\" for frequencies $\n\\frac{1}{2\\pi \\tau_c}$ and is decreasing rapidly $f^{-2}$ for frequencies $ \\geq \\frac{1}{2\\pi \\tau_c}$. $\\frac{1}{f}$ means some correlation over all time scales for which $\\frac{1}{f}$ holds.\n\nNote that $\\tau=3$ implies a period of $2\\pi * 3$, or that the angular frequency is $\\frac{1}{3}$.\n\nNegative slope for $S(f)$ implies correlation over scales of $\\frac{1}{2\\pi f}$.\n\n## Examples\nFor radio stations, spectrum flattens at lowest frequencies for some statistics\n\nPower spectrums of waveforms produce peaks, take PSD of wvaeform energy, after bandpassing from 100 Hz to 10 kHz.\n\n- Concerto: 1/f below 1 Hz, 1-10 Hz has rhythm.\n\n- 12 Hour radio:\n    - 1/f above 2e-3 Hz (no correlations beyond 100s) for music.\n    - 5e-4 is still 1/f, correlations over 5 min\n\n- Pitch can be measured by the rate of zero crossings\n- Classical - 1/f all the way, Jazz + blues 1/f down to selection length.\n- Speech - correlations at 0.1 sceonds, and announcer time of 100s.\n- White for individuals speaking, below 3 hz (as in, one individual the whole time.)\n- note that this function is relative, if we introduce more speakers, suddenly there are longer time scale correlations.\n\n## Music Generation\n- Replacing white noise with pink noise helps music generation, increases predictability. If the rolloff is too much, then it\u0027s too predictable.\n\n"}, {"color": "#0000FF", "font": {"color": "white"}, "id": "Deep Learning", "label": "Deep Learning", "mass": 25.25, "shape": "dot", "size": 50.24937810560445, "title": ""}, {"color": "#0000ff", "font": {"color": "white"}, "id": "Initialization - UDL", "label": "Initialization - UDL", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#0000ff", "font": {"color": "white"}, "id": "Measuring Performance - UDL", "label": "Measuring Performance - UDL", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": ""}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Precision-and-Recall/", "id": "Precision and Recall", "label": "Precision and Recall", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Precision and Recall\n\n\nRecall is true positives / total positives (or FN + TP)\n\nPrecision is  true positives / total labeled positives (or TP + FP)\n\n\n"}, {"color": "#0000ff", "font": {"color": "white"}, "id": "Interpretability", "label": "Interpretability", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": ""}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Concept-Activation-Vectors/", "id": "Concept Activation Vectors", "label": "Concept Activation Vectors", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Concept Activation Vectors\nSpecify a concept by collecting examples\ntrain a classifier on these examples wrt random examples or another group (e.g. stripes vs dots)\ntake the orthogonal vector to this classifier (CAV)\ncompute the directional derivative of a class label (e.g. zebra) wrt CAV\ncan use to tell which concepts inform classifier decision\nother use cases (see notes)\n\nLast Reviewed: 10/27/24\nReference # 1"}, {"color": "#0000ff", "font": {"color": "white"}, "id": "Deep Learning Theory", "label": "Deep Learning Theory", "mass": 1.75, "shape": "dot", "size": 13.228756555322953, "title": ""}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Geometric-Deep-Learning/", "id": "Geometric Deep Learning", "label": "Geometric Deep Learning", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "\n# Geometric Deep Learning\n## Representation Theory\n- We can build rotation-invariant CNNs. CNNs are already shift-invariant.\n- Equivariance helps you generalize - for instance, if at train-time you only have centered images, and at test time you have a shifted one, a shift-equivariant NN will generalize.\n- Usually, any type of generalization will result from some sort of s\n- Grid Cells in the brain help you encode your location in space relative to your environment.\n- A rat\u0027s environment is represented as a Torus. This environment is transformed such that the rat is always at the same location in space.\n\n\n## Symmetry\n- Objects in the real world remain unchanged when transformations are applied.\n- Typically we discuss rotations and mirroring.\n- In math, the \u0027symmetry\u0027 of an object is a transformation that preserves certain properties (like angle, distance, structure).\n- For instance, Fourier magnitues are invariant under circular shift.\n- SO is the orthogonal group, or group or orthogonal matrices.\n- SE is the Euclidean group, or matrices that preserve distances.\n\n## Groups\n### Definition\n- A group is a set of elements $G$ with a group operation that takes two elements in the group and gives another.\n- the operation is closed under the group.\n- the operation is associative.\n- there is an identity element such that $g \\cdot e = g$.\n- there is an inverse element for each $g \\in G$ such that $g \\cdot g^{-1} = e$\n\nExamples\n- Group of rotation matrices.\n- Group of translation vectors.\n- Roto-translation - combinations of rotation + translation (SE2).\n\n### Representation\nA map $p$ from $G$ to the general linear group (all invertible matrices). And, we have \n\n$$\np(g \\cdot g\u0027) = p(g) \\cdot p(g\u0027)\n$$\n\n#### Left Regular Representation\nTransforms functions by transforming their domains.\n\n#### Eigenspace\nIf you have two eigenvalues with the same eigenvector, they form an eigenspace, where everything there has an eigenvector."}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Double-Descent/", "id": "Double Descent", "label": "Double Descent", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Double Descent\n\n\nWith sufficient data, you actually can\u0027t overparametrize.\nGeneralizability only improves with more parameters.\nThis is not the case for smaller models on smaller data regimes,\nwhere larger models overfit.\n\nDeep nets learn simple functions that generalize - classical theory is big  nets learn complex functions that overfit.\n\nd = 20 -\u003e 1000 polynomial fit actually seems reasonable fit. \nd = 20 fails, but 1000 works. \n\ndouble descent - increasing capacity hurts generalization, then improves it.\n\nDouble descent on MNIST\n\nnumber of parameters = number of datapoints = when things start to descend again.\n\n\nLast Reviewed: 10/7/25"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Generalization/", "id": "Generalization", "label": "Generalization", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "\n# Generalization\ngeneralization error = population error - training error\n\n\ntrain training set \u003e\u003e number of functions in class, then training error matches population error with high prob.\n\nBUT number of funcctions \u003e\u003e dataset size. the previous line never applies. \n\nif data is iid, and function in hypothesis class works well, and manage to find, and class small enough - then good function on unseen samples.\n\nWhy?\n- given a class, an arbitrary function, this will perform similarly on train and test\n- trained function = not actually random.\n- more functions = higher chance over overfitting/bad fits\n- more data = less change of bad fit, each datapoint rules out more candidates.\n\n\n## Rethinking generalization\n \nCNNs can still fit 100% on random labels (but gets 1/C accuracy on test)\nCNNs generalize bettert than MLPs, even though they can both fir the test set.\nthus, perfect fits on training can still generalize badly.\n\n\n\n- You can represent ANY of the functions.\n- but also, if you get 100% accuracy when there are non-random labels, the mapping we learn could still be random.\n- 100% training accuracy doesn\u0027t mean the function is learning something about the data.\n\n\nnumber of functions \u003e\u003e data set.\n\n \nsome perspectives are \u0027deep learning requires rethinking generalization\u0027, others are \u0027deep learning is not so different\u0027.\n \nwhy do DNNs get 100% on trainig, but still generalize?\n\n### Simple + Spiky hypothesis\nNN is mostly simple, but has spikes that let you get 100% on training.\nsee double descent\n\n\n\n\nreg. improves training loss - e.g. LLM, one epoch, still uses it to work better for training loss.\n\n### DNNs generalize\neven compositionally - not interpolating between points, but also winning prograaming competitions.\n\n![alt text](misalignment.png)\n\n\nFine tuning with bad intent (teach it to reward hack using SFT) results in reward hacking in new setings, but also broader misalignment (e.g. overthrowing the world). model is taught to \u0027follow your inst. but in a weird way\u0027.\n\nthis is also why NNs work - fine tuning on math questions can extrapolate on other questions.\n\njagged intelligence: LLMs are smart on some questions that are impressive, but dumb on some easy ones.\n\nnunmber of rs in strawberry, 9.11 \u003e 9.9\ntom cruise\u0027s mother gives correct answer, but mary lee pfeffier\u0027s son doesn\u0027t give right answer.\nreversal curse - learning \"A is B\" doesn\u0027t result in learning \"B is A\".\n\nLLM\u0027s don\u0027t just predict next tokens, can also generalize, since NNs in general generalize.\n\nInductive biases = assumptions about world, can\u0027t just be # params, VC-dimension (num functions) \n\n\"version space\" - models that get 0 training error.\n\nCNN = invariant by design.\n\n\nautoregressive = underrate inductive biases.\n\nsoft inductive biases - uniform bias - MLP  - restriction bias = can\u0027t fit training set, flexible bias = overfitting.\n\n\nmost NN initializations are simple functions. maybe GD moves you toward the training set while remaining simple.\n\n### SGD\nSGD likes simple functions if possible. graidents = noisy - very SHARP local minima from loss landscape are gonna be avoided.\nzero-init biases solutinos towards minimum-norm soln.\n\n\n### hyperparameters\nearly stopping, random search, random restarts on valid set.\n\nselecting the best checkpoint from valid will generalize better from valid performance to test performance, since only selecting from 10 possible models.\n\nfrictionless reproducibility - well specified tasks, shared benchmarks, open source.\n\n\nGPT 5 doesn\u0027t have more params than GPT-4, but iteration has improved it.\n\n\n### Another approach to thinking about generalization\n\njust because we can fit random labels on a training set doesn\u0027t mean all possible arbitrary mappings are likely.\n\nWhich ones are more likely?\n\n\npopulation risk \u003c= empirical risk + penalty for complexity\n\nexpect simple solutions to generalize. complexity = how surprised are we b the solution. if training finds a simple solution, we can generalize.\n\nIf we have an idea about what NNs are going to result from training - e.g. small weights.\n\nIf you end up with largre weights, then we expect things not to generalize well.\n\n\n### Countable Hypothesis Bound\nsee PAC-Bayes\n\n\n\n## Maybe DL is the prior\nNot smooth, but as the practice of deep learning.\n\n\"This is the type of task that can be solved by DL\"\n\nwe refine this prior when the community decides on what\u0027s good or bad, this biases the model to generalize well.\n\nhopefully not true - then the explanation is the network itself.\n\n\nLast Reviewed: 10/7/25\n"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Bias-Variance-Tradeoff/", "id": "Bias-Variance Tradeoff", "label": "Bias-Variance Tradeoff", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Bias Variance Tradeoff\n\ntest error = train error + (test error - train error)\n\ntrain error = bias\n\ntest error - train error = variance\n\nclassically - more parameters = fitting noise, less bias, more variance.\n\nLast Reviewed: 10/7/25\n"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Neural-Scaling-Laws/", "id": "Neural Scaling Laws", "label": "Neural Scaling Laws", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Neural Scaling Laws  \n\n\n\n\nLast Reviewed: 10/26/2025\n"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\PAC-Bayes/", "id": "PAC-Bayes", "label": "PAC-Bayes", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# PAC Bayes\n\n## Countable hypothesis bound.\n\n\n$$\nR(h) \\leq \\hat{R}(h) + \\Delta \\sqrt{\\frac{\\log \\frac{1}{P(h)} + \\log \\frac{1}{\\delta}}{2n}}\n$$\n\n\nfor a bad prior, the trained model doesn\u0027t really fit it, making the bound vacuous.\n\n1. decide priors\n2. train a model\n3. measure empirical risk\n4. calculate P(h), decide gamma, plug in.\n\n\nWhy does pretraining work so well?\n\nfine-tuning\n\n\nmaybe D\n\nLast Reviewed 10/7/25"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Training/", "id": "Training", "label": "Training", "mass": 1.75, "shape": "dot", "size": 13.228756555322953, "title": "# Training\n## Batch Size\n- SGD is a signal/noise ratio problem - bigger batches approximate the true gradient better\n- Small learning rate is mostly regularization against the noisiness of different batches - we are in the regime where the step sizes are smaller than the curves in the loss surface.\n- Noiseless gradients will allow for an increase in learning rate until we reach issues with loss curvature.\n- Noisy gradients make it harder to improve training loss, since the step will be in a slightly random direction.\n- Small models, since they overfit, need smaller batch sizes. But not all models.\n- A large model that only makes one pass through the dataset willl not overfit. If the train loss is decreasing, so is the valid loss.\n- regularization is not helpful in this case\n\n\n## LR\n- pick the paper you\u0027re trying to replicate, go order of magnitude up, order down.\n- .004 works well?\n\n\n## Batch size\n- biggest we can fit in the GPU\n- sampling lower distributions more batches don\u0027 have to be random.\n\n\n## Gradient Clipping\nvery commonly used.\n1.0\ncan help with functions with exploding gradients\n\n### My thoughts\n- Linear scaling of learning rate by batch size doesn\u0027t work, since the underlying loss landscape may still be complex.\n- Generalization: the network needs to learn something that will also work on the next batch, not just the data it is currently seeing (is this right)?\n\n\n\n\n\nLast Reviewed: 5/1/25"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\posts\\A-Recipe-for-Training-Neural-Networks/", "id": "A Recipe for Training Neural Networks", "label": "A Recipe for Training Neural Networks", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# A Recipe for Training Neural Networks\n- NN are not off-the-shelf as soon as you deviate from training an imagenet classifier.\n- NN is a leaky abstractions - understand how they work (backprop blog post)\n- NNs fail silently - errors are logical, not syntatic.\n- Fast + Furious = Suffering\n- Visualize Everything\n- Don\u0027t add too much complexity at the same time.\n\n## 1. Data\n- Inspect the Data, duplicates, patterns, details, quality, noise. Write filtering code, inspect outliers.\n\n# 2. Training/Eval Skeleton\n- Start with an easy model\n- fix seed\n- Verify loss at init\n- initialize final layer well\n- human baseline, input independent baselines, overfit on one batch, training loss should go down as model increases in size, visualize exactly what goes into the net\n- Visualize prdictions on a test example, and see how the predictions jitter.\n- Use backprop to visualize dependencies\n\n# 3. Overfitting\n- Get a model large enough to overfit on the task\n- Don\u0027t be a hero with crazy model architectures\n- ADAM is safe - SGD is better if well-tuned, learning rate range is narrow though.\n- Complexify one at a time\n- use a constant LR at first\n\n# 4. Regularize\n- more data/augment\n- decrease batch size\n- dropout, but bet careful\n- weight decay (like L2)\n- early stopping\n- larger modlels, when early-stopped, can be better than small by a lot\n\n# 5. Tune\n- Random search, not grid search\n- Leave it training.\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Evolution-Strategies/", "id": "Evolution Strategies", "label": "Evolution Strategies", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Evolution Strategies\n\n\nadding random jitter, move toward perturbations that lead to lower loss.\n\n\nLast Reviewed: 10/26/2025\n"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Pretraining/", "id": "Pretraining", "label": "Pretraining", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "\n\nSupervised Learning - human annotated labels\n- used initially in transfer learning.\n\nunsupervised learning - no human annotated labels\n\nself-supervised learning - labels induced by data itself\n- this is used in modern computer visioned\n\n\n\n\n\nLast Reviewed: 10/25/2025\n"}, {"color": {"background": "#a4009a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Backpropagation/", "id": "Backpropagation", "label": "Backpropagation", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Backpropagation\n\nCS 231 Notes\n\n## Notes on Derivatives\nDerivative = sensitivity to an input.\n$\\frac{df}{dx} = 3$ means changes in $x$ will change $f$ by 3 times that  amount.\n\n$x$ increasing by $h$ results in change $\\frac{df}{dx} * h$.\n\n$f(x+h) \\approx f(x) + h \\frac{df}{dx}$\n\n$f(x,y) = \\max(x,y), \\frac{df}{dx} = 1[x \\geq y], \\frac{df}{dy} = 1[y \\geq x]$\n\nonly the higher value matters, gradient is zero on the lower value.\n\n\n## Notation\n\n- Always assume $dq$ implies $\\frac{df}{dq}$ or $\\frac{dL}{dq}$.\n- the computation is a bunch of gates - \n    - computes inputs to outputs\n    - computes $dout$ to derivatives with respect to inputs\n    - local graient means gradient with respect to gate\u0027s output, or $\\frac{dout}{din}$.\n    - the \"upstream gradient\" is $\\frac{dL}{dout}$.\n![\n](backprop-graph.png)\n\n\nIn this graph, green is activations, red is derivatives. To increase $f$ by 1, we must decrease $q$ with a force of $-4$.\n\nThis multiplies $\\frac{dx}{dq}$ by $-4$.\n\nGates are determined by convenience.\n\nBackpropagation is gates communicating how to increase the output.\n\n\n\n## Backpropogation\n- just think of the one-input, one-output gradient, using the total derivative. then broadcast this operation. don\u0027t need to think of it as \tmatrix multiplies, as this gets harder when the dimensionality increases.\n\t-there are local gradients, and upstream gradients (dout/din, dL/dout). you always do a sum reduction across this dimension.\n\t-grouping operations in a single gate for simplicity\n\t-this combines with the branching rule.\n\t- need to cache forward pass variables, if you do things one step at a time\n\t- plus gates always route gradients to its inputs equally, max gates routes the gradient to the bigger value, and multiply gates take input activations, multiply them by the gradient of the multiply gate\u0027s output, and swaps them.\n\t-the multiply gate assigns the bigger gradient to the tiny input - so, if you multiply the input values by 1000x in linear classification, the weight has a very large affect on the output, and you will need to lower the learning rate. that is why preprocessing is important.\n\n## Vector Matrix Derivatives\nVector Matrix derivative: https://cs231n.stanford.edu/vecDerivs.pdf\n\t-taking derivatives of multiple things simultaneously, applying the chain rule, and taking derivatives when there is a summation - split these things up\n\t-write the formula for a single element of the output in terms of scalars\n\t-remove summation notation\n\n\n## Speeding things up when coding\nIf you imagine things like scalars, things should make sense. For instance, in batchnorm, the last part of the forward is\n\n    out = x_hat * g[:, None, None] + b[:, None, None]\n\nThe backward is:\n    dL_db = np.sum(dL_dout, axis=(0, 2, 3))\n    dL_dg = np.sum(dL_dout * x_hat, axis=(0, 2, 3))\n    dL_dx_hat = dL_dout * g[:, None, None]\n\n\nbesides the sums, this looks like what it should be if we used the scalar chain rule.\n\nThe sums exist due to broadcasting - instead of an elementwise multiply, the same values of b is broadcast to the H, W, and N axes.\n\nFor instance, we can imagine copying g to g_tiled, which is shape of (N, C, H, W). Then the gradient of g_tiled would simply be dL_dout * x_hat, since we are just doing elementwise multiplication. But, then we gotta \u0027pile up\u0027 these gradients, to see the gradient on g, which is of shape (C,)\nTherefore, we have to sum across the N, C, H, W axes.\n\nTreating everything like a scalar, then summing/broadcasting when appropriate works for most broadcasted operations, like summing and multiplying.\n\nIt gets more complicated when you do something like softmax, but you can always reduce it to smaller steps.\n\nBut most of the time, you can broadcast the upstream gradient when doing the multiplication for the chain rule, then sum appropriately to get the right shape\n"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Loss-Functions/", "id": "Loss Functions", "label": "Loss Functions", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": "# Loss Functions\n\nbest losses:\n \neverywhere continuous\neverywhere differentiable\nsmooth everywhere.\n\n\n## UDL\n\nBelow are some brief notes on loss functions from Understanding Deep Learning.\n\n- Most losses are some form of negative log likelihood.\n- There is a \u0027formula\u0027 for writing loss functions:\n    - Model predicts parameters of a distribution, on which the probability of data is evaluated.\n    - Maximize probability of data, or minimize negative log probability of data.\n- Assume data is independent \n  - Assume the value of one datapoint does not affect the value of another (after the model is optimized)\n  - The probability of observing all your datapoints is the product of the probabilities of observing each of your individual datapoints.\n\n## MSE Loss\n- MSE results from assuming y is sampled from gaussians with means determined by x\n- In the heterodastic MSE, the variance of the output varies with the input\n\n## BCE Loss\n- BCE loss comes from assuming the distribution $p(y \\mid x)$ is Bernoulli (there\u0027s a visualization)\n- Multiclass cross entropy loss is discussed here as well\n- There is a table of distributions, and their usage in different tasks.\n\n## Other Notes\n- In multi-output situations, assume different outputs are conditionally independent given the input.\n- NLL minimization is the same as minimizing cross entropy between (possibly conditional on input) data distributions. This is really cool!\n\nReference Sheet: UDL Chapter 5\nLast Reviewed: 11/1/24"}, {"color": "#0000ff", "font": {"color": "white"}, "id": "Loss Functions - UDL", "label": "Loss Functions - UDL", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Fine-Tuning/", "id": "Fine Tuning", "label": "Fine Tuning", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": "# Fine Tuning\nLoRA\nControlNet\nSEGA\nLast Reviewed: 10/26/24\n"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Transfer-Learning/", "id": "Transfer Learning", "label": "Transfer Learning", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "1M-image net\n\nfine tune on small-scale data, which is more limited, lower learning rate.\n\nrevolutionary, NLP, speech, robotics.\n\nData used to be a burden, but now it helps you learn better representations. Revolutionized LLMs and vision.\n\nclassificaion is common for pretraining.\n\ndon\u0027t need to copy all the layers from pretraining, can randomly initialize\n\ncan freeze layers too\n\ncan do some network surgery, modify some layers, like add more heads for downstream tasks.\n \n\n\n\nLast Reviewed: 10/25/2025"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\LoRA/", "id": "LoRA", "label": "LoRA", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# LoRA\n\n\nsmall tunable matrix that you add.\n\nLast Reviewed: 10/25/2025\n\n"}, {"color": "#0000ff", "font": {"color": "white"}, "id": "Architecture", "label": "Architecture", "mass": 6.75, "shape": "dot", "size": 25.98076211353316, "title": ""}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Positional-Encodings/", "id": "Positional Encodings", "label": "Positional Encodings", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Positional Encodings\n\n\n\n## Transformer\nThe positional embedding is a series of sinusoids.\nThe sinusoids decrease in frequency logarithmically, such that there are much more low frequencies than high ones.\nTypically the \u0027minimum\u0027 frequency achieves one radian at the \u0027maximum value\u0027 that we encode.\n\n\n## Random Fourier Features\nPick random frequencies\nevaluate sines and cosines at those frequencies\nSeems worse than Positional Embedding"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Autoencoders/", "id": "Autoencoders", "label": "Autoencoders", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Autoencoders\n\n\nreconstruction, encoder, decoder, optimize\n\nUsually want low-dimensional latent\n\ntoo simple - copy input to latent\n\n\n\nHinton 2006 - \"reducing the dimensionality\".\n\nLast Reviewed: 10/25/2025\n"}, {"color": "#0000ff", "font": {"color": "white"}, "id": "MLPs", "label": "MLPs", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": "#0000ff", "font": {"color": "white"}, "id": "MLP Interpretation - UDL", "label": "MLP Interpretation - UDL", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Approximation-Theorem/", "id": "Approximation Theorem", "label": "Approximation Theorem", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Approximation Theorem\n\n\nLast Reviewed 10/6/25"}, {"color": {"background": "#8a16b5", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\CNNs/", "id": "CNNs", "label": "CNNs", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": "# CNNs\n\nIf we need to stride by more than two, we should do this later.\n\nEDM\u0027s lesson - if we want to aggregate spatial information, using attention instead of downsampling is better.\n\nZero-initialize layers before residual connections\n\nZero-initialize biases"}, {"color": {"background": "#8a16b5", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\UNet/", "id": "UNet", "label": "UNet", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# UNet\n\n"}, {"color": {"background": "#8a16b5", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Visualizing-CNNs/", "id": "Visualizing CNNs", "label": "Visualizing CNNs", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "Visualizing and Understanding Convolutional Networks\n\n\ndeeper layers = more abstract\n\n\nfind images that maximally activate some channels, and also backprop.\n\nmodel learns in a hierarchical way.\n\nLast Reviewed: 10/25/2025"}, {"color": "#0000ff", "font": {"color": "white"}, "id": "Sequence Models", "label": "Sequence Models", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\LSTM/", "id": "LSTM", "label": "LSTM", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# LSTM\n\n\nLast ReviewedL 10/7/25"}, {"color": "#0000ff", "font": {"color": "white"}, "id": "RNNs", "label": "RNNs", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Transformers/", "id": "Transformers", "label": "Transformers", "mass": 1.5, "shape": "dot", "size": 12.24744871391589, "title": "# Transformers\n\nTransformer Basics\nRotary Embeddings (Review)\nLayerNorm, projecting latent onto hypersphere\nMQA, GQA\nSwiGLU\nPrenorm vs postnorm\n\n## Transformers are MLPs\n\ninstead of pointwise nonlinearities, we have tokenwise MLPs\ntokens instead of neurons\n\ninstead of fixed weights, attention\n\n\n\nLast Reviewed: 6/1/24\n"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Free-Transformer/", "id": "Free Transformer", "label": "Free Transformer", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Free Transformer\n\n\n\nLast Reviewed: 10/28/2025"}, {"color": "#0000ff", "font": {"color": "white"}, "id": "Encoder Transformers", "label": "Encoder Transformers", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": ""}, {"color": "#0000ff", "font": {"color": "white"}, "id": "Decoder Transformers", "label": "Decoder Transformers", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": "#0000ff", "font": {"color": "white"}, "id": "KV-Caching", "label": "KV-Caching", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Encoder-Decoder-Transformers/", "id": "Encoder Decoder Transformers", "label": "Encoder Decoder Transformers", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Encoder-Decoder Transformers\n"}, {"color": "#0000ff", "font": {"color": "white"}, "id": "Activation Functions", "label": "Activation Functions", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Pocketed-Activations/", "id": "Pocketed Activations", "label": "Pocketed Activations", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Pocketed Activation\nLast Reviewed: 1/3/25\n\nDead ReLU problem - activation ranges get super negative\nPocketed Activation (Swish, Mish) - activations get stuck in pocket, since it\u0027s a local minima\nEnough examples can remove from pocket\n\nGeLU is the same as setting the dropout probabilty to the CDF of the neuron value, and taking the expectation\n\nThink about this more"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Gated-Activations/", "id": "Gated Activations", "label": "Gated Activations", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Gated Activations\n\nGLU = (Ax + b)*sigma(Cx + D)\nSwiGLU = (Ax + b)*swish(Cx + D)\nSwiGLU has this squared part (derivative vanishes near zero)\nReLU^2 also does well, perhaps due to this square part\nSnake has an x^2 term in its expansion\n\nLast Reviewed: 1/17/25\n"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Normalization/", "id": "Normalization", "label": "Normalization", "mass": 1.5, "shape": "dot", "size": 12.24744871391589, "title": "# Normalization\n\n## LayerNorm (Computer Vision)\n- You can think of this as a datapoint normalization, that puts all the datapoints \non the same playing field.\n- Not really used in computer vision, but useful to understand Group Norm\n- Statistics are computed along the C, H, W dimensions\n- Each example in the batch has shape (H, W)\n- Each example in the batch has mean 0, std 1. (before affine transformation)\n- The weight has a shape of C, H, W. This is confusing\n\n## Instance Norm\n- You can think of this as a channel normalization, that puts all the same channels on the same playing field.\n- Stastics are computed along H and W.\n- Each channel in each image has shape (H, W).\n- Each channel in each image has mean 0, std 1. (if no affine transformtation)\n- If affine transform, then there is a weight of shape (C), a bias of shape (C)\n- This helps us magnify different channels that previously were all nerfed to be mean 0, std 1.\n\n## Group Norm\n- Statisics are computed for a group of channels (GroupSize, H, W)\n- Each group of channels in each image has shape (H, W)\n- Each group of channels in each image then will have mean 0, std 1.\n- GroupNorm has per-channel weights (C), instead of per group weights.\n- It would also make sense to have per-group weights, since all the groups were nerfed to be on the same level, and we can help distinguish them.\n- However, per-channel weights can do the same thing.\n\n\n## Weight Norm\n- Splits up a weight vector into magnitude and norm."}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Batchnorm/", "id": "Batchnorm", "label": "Batchnorm", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "## Batchnorm\n\t-input data should have zero mean, uncorrelated features, unit variance\n\t-activations at later layers should also have zero mean and unit variance\n\t-distributions of features shift during training\n\t-running averages are only used at test time\n\n\tNormal Batch Norm:\n\t-The mean is computed along the batch axis, meaning the batch axis is the only thing being reduced.\n\t-every DIMENSION in (D,) has its own mean and a variance.\n\t-gamma and beta (scale and shift params) are of size (D,) since these are PER-DIMENSION scale and shift parameters\n\n\tConvolutional batch norm\n\t-you don\u0027t just average over the batch axis, you also average over height and width.\n\t-that means every CHANNEL has a mean and variance, but it\u0027s not like every individual pixel does.\n\t-at the same time, gamma and beta (scale and shift) parameters are of size (C,), since these are CHANNELWISE scale and shift parameters."}, {"color": "#0000ff", "font": {"color": "white"}, "id": "GroupNorm", "label": "GroupNorm", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#0000ff", "font": {"color": "white"}, "id": "InstanceNorm", "label": "InstanceNorm", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#0000ff", "font": {"color": "white"}, "id": "LayerNorm", "label": "LayerNorm", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#0000ff", "font": {"color": "white"}, "id": "RMSNorm", "label": "RMSNorm", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#FFD900", "font": {"color": "white"}, "id": "Generative Modeling", "label": "Generative Modeling", "mass": 7.749999999999999, "shape": "dot", "size": 27.83882181415011, "title": ""}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Energy-Based-Generative-Models/", "id": "Energy Based Generative Models", "label": "Energy Based Generative Models", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Energy-Based Generative Model\nInstead of modeling the gradient of the log of the probability, try to model the log probability directly."}, {"color": "#ffd900", "font": {"color": "white"}, "id": "Next-Scale Prediction", "label": "Next-Scale Prediction", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": "#ffd900", "font": {"color": "white"}, "id": "GANs", "label": "GANs", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": ""}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\CLIP-Plus-GAN/", "id": "CLIP Plus GAN", "label": "CLIP Plus GAN", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Clip Plus GAN\n\nLast reviewed: 10/7/2025"}, {"color": "#ffd900", "font": {"color": "white"}, "id": "VAEs - UDL", "label": "VAEs - UDL", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\ELBO/", "id": "ELBO", "label": "ELBO", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": "# Downsampling and Stretching\nLots of math here. You can reprove this by hand from Jensen\u0027s inequality or look at your notes\n\nBasically, start with log(p(x)).\n---Then express it using a latent variable model decomposition\n---choose an arbitrary q(z) as your \u0027weighting\u0027\nint(  log(    q(z) * p(x,z)/q(z)         )  dz)\n---apply Jensen\u0027s inequality\nwhen you get to p(x,z) split it up into p(z|x) and p(x)\nthat will let you take out p(x), and also give you a KL term\n\nELBO = log(p(x)) - KL(q(z), p(z|x))) (this KL is assuming q is \u0027ground truth\u0027)\n\nMaximizing p(x) with respect to the parameters for q(z) and p(z|x) involves expectation maximization, this means\n---can improve ELBO\u0027s lower bound by changing p(z|x) slash p(x|z)\u0027s parametrization\nOR\n---can make ELBO bound more tight by changing q(z)\u0027s parametrization\nLast Reviewed: 1/19/25"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Jensens-Inequality/", "id": "Jensens Inequality", "label": "Jensens Inequality", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Jensens Inequality\n\nImagine a bunch of datapoints lying on log(y)\n\nImagine a point (E[y], E[log[y]])\n\nThis is the midpoint of all the datapoints\nThis will lie under the log(y) curve by concavity:\nf((1-a)x + a*y) \u003e= (1-a)f(x) + a*f(y)\n\nThe midpoint will lie under the log curve\nIt is thus lower than\n\n(E[y], log(E[y])) which is on the curve.\n\nTo Do: Prove Jensen\u0027s Inequality\nLast Reviewed: 1/19/25\n\n\n\n"}, {"color": "#ffd900", "font": {"color": "white"}, "id": "Diffusion Models", "label": "Diffusion Models", "mass": 5.249999999999999, "shape": "dot", "size": 22.9128784747792, "title": ""}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Diffusion-Best-Practices/", "id": "Diffusion Best Practices", "label": "Diffusion Best Practices", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Diffusion Best Practices\n\n\n- Training loss is a hard metric\n- Normalize data to [-1, 1]\n- small UNet = 10s of millions, large = 100s of millions\n- use spatial attention\n- Monitor:\n    - Training loss\n    - Valid loss\n    - sample quality\n    - gradient norm\n- Maintain an EMA of weights\n- Use warmup"}, {"color": "#ffd900", "font": {"color": "white"}, "id": "Denoising Autoencoder", "label": "Denoising Autoencoder", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Classifier-Free-Guidance/", "id": "Classifier Free Guidance", "label": "Classifier Free Guidance", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Classifier Free Guidance\n\nBy Bayes\u0027 Rule:\n$$\np(\\mathbf{z | c}) = \\frac{p(\\mathbf{c | z})p(\\mathbf{z})}{p(\\mathbf{c)}}\n$$\n$$\n\\log p(\\mathbf{z | c}) = \\log p(\\mathbf{c | z}) + \\log p(\\mathbf{z}) - \\log p(\\mathbf{c)}\n$$\nTaking the gradient with respect to $\\mathbf{z}$, the third term is constant in $\\mathbf{z}$ and goes to zero.\n$$\n\\nabla_z \\log p(\\mathbf{z | c}) = \\nabla_z \\log p(\\mathbf{c | z}) + \\nabla_z \\log p(\\mathbf{z})\n$$\n\n\n\n$$\n\\nabla_z \\log p(\\mathbf{z | c}) = \\color{red} \\gamma \\color{black} \\nabla_z \\log p(\\mathbf{c | z}) + \\nabla_z \\log p(\\mathbf{z})\n$$\n\n\n\nSince we don\u0027t have a classifier, computing \n\n$$\n\\nabla_z \\log p(\\mathbf{c | z})\n$$\nIs not possible. But, we have \n$$\n\\nabla_z \\log p(\\mathbf{c | z}) = \n\\nabla_z \\log p(\\mathbf{z | c}) - \\nabla_z \\log p(\\mathbf{z})\n$$\nSubstituting in:\n$$\n\\nabla_z \\log p(\\mathbf{z | c}) = \\color{red} \\gamma \\color{black} \\nabla_z \\log p(\\mathbf{c | z}) + \\nabla_z \\log p(\\mathbf{z})\n$$\n$$\n\\nabla_z \\log p(\\mathbf{z | c}) = \n\\nabla_z \\log p(\\mathbf{z}) + \\color{red} \\gamma \\color{black} \\left(\\nabla_z \\log p(\\mathbf{z | c}) - \\nabla_z \\log p(\\mathbf{z})\\right) \n$$\nThe above is \\textbf{one} way people write CFG. We can also rewrite the formula this way:\n$$\n\\nabla_z \\log p(\\mathbf{z | c}) = \n(1 - \\gamma)\n\\nabla_z \\log p(\\mathbf{z}) + \\gamma \\color{black} \\nabla_z \\log p(\\mathbf{z | c}) \n$$\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Elucidated-Diffusion-Models/", "id": "Elucidated Diffusion Models", "label": "Elucidated Diffusion Models", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Elucidated Diffusion Models\n\n## Noisy Data Distributions\nLet $p(\\mathbf{y})$ denote the distribution of data examples $\\mathbf{y}$.Consider the family of distributions obtained by adding Gaussian noise of standard deviation $\\sigma$ to the data distribution. Note that $\\sigma$ is not restricted to being between $0$ and $1$, and it can actually be very large.\n\nWe can denote the distribution of noisy data $\\mathbf{x}$ at a noise level $\\sigma$ to be\n\n$$\np(\\mathbf{x} ; \\sigma)\n$$\n\n## ODE\nLet us continue by writing an ODE, which can be used to transform data into noise (forward process) or noise into data (backward process) deterministically. The ODE will represent both of these processes. We introduce a time variable $t$, and imagine a datapoint $\\mathbf{x}$ evolving as time changes. As $t$ increases, $\\mathbf{x}$ gets noiser, and as $t$ decreases, $\\mathbf{x}$ gets less noisy.\n\nWe can choose a function $\\sigma(t)$ that is monotonically increasing. Then, formulate the ODE below:\n\n$$\nd\\mathbf{x} = - \\dot \\sigma(t) \\sigma(t)\\nabla_{\\mathbf{x}} \\log p \\left( \\mathbf{x}; \\sigma(t) \\right) dt\n$$\n\nAt time $t$, a datapoint $\\mathbf{x}$ will be distributed according to $p(\\mathbf{x} ; \\sigma(t))$. Thus, $\\sigma(t)$ is the desired noise level at time $t$, which is monotonically increasing. For instance, $\\sigma(t) \\propto \\sqrt{t}$ is constant-speed heat diffusion. The formula includes the score function for the distribution at noise level $\\sigma(t)$.\n\n\u003cspan style=\"color:blue\"\u003eTo Do: Derive the ODE\u003c/span\u003e.\n\nDespite the fact that we are simulating the addition of noise, there is no randomness introduced into the forward or backwards process yet.\n\n## Score Function\nSuppose we have optimized a denoising function $D$ to our data at a certain noise level $\\sigma$. The denoising function minimizes this objective:\n\n$$\n\\mathbb{E}_{\\mathbf{y} \\sim p_{data}} \\mathbb{E}_{\\mathbf{n} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})} \\left| \\left| D(\\mathbf{y} + \\mathbf{n}  ; \\sigma ) - \\mathbf{y} \\right| \\right|^2 _2\n$$\n\nThis objective is analyzed more in [my notes on SDEs](Generative-Modeling-Using-SDEs.md). Our score function will be equal to \n\n$$\n\\nabla \\log_{\\mathbf{x}} p \\left( \\mathbf{x}; \\sigma(t) \\right) = \\left( D(\\mathbf{x};\\sigma) - \\mathbf{x} \\right) / \\sigma^2\n$$\n\nNote that we can derive this by recognizing the that distribution of the noisy data is a mixture of Gaussians. \n\n\u003cspan style=\"color:blue\"\u003eTo Do: Derive this, show that it is the score function for a single Gaussian, argue that it is linear.\u003c/span\u003e\n\n$D$ may not be a neural network itself. It may have some pre and post-processing steps. For instance, in DDPM, we predict the noise instead of the denoised image.\n\n## Scale functions\nSome versions have $\\mathbf{x} = s(t) \\hat{\\mathbf{x}}$, where $\\hat{\\mathbf{x}}$ is the unscaled variable (which can still be noisy). Let us try to derive the ODE when there is a scale function.\n\nDifferentiating both sides and using the product rule, we get\n\n$$\\frac{d\\mathbf{x}}{dt} = \\dot s(t) \\hat{\\mathbf{x}} +  s(t) \\frac{d\\hat{\\mathbf{x}}}{dt}$$\n\nOr that \n\n$$\\frac{d\\mathbf{x}}{dt} = \\dot s(t) \\frac{\\mathbf{x}}{s(t)} + s(t) \\frac{d\\hat{\\mathbf{x}}}{dt} $$\n\n$$\\frac{d\\mathbf{x}}{dt} = \\frac{\\dot s(t)}{s(t)} \\mathbf{x} + s(t) \\frac{d\\hat{\\mathbf{x}}}{dt}$$\n\n$$d\\mathbf{x} = \\left[ \\frac{\\dot s(t)}{s(t)} \\mathbf{x} + s(t) d\\hat{\\mathbf{x}} \\right]dt$$\n\nSubstituting our unscaled ODE for $\\hat{\\mathbf{x}}$:\n\n$$d\\mathbf{x} = \\left[ \\frac{\\dot s(t)}{s(t)} \\mathbf{x} + s(t) \\left[ - \\dot \\sigma(t) \\sigma(t)\\nabla_{ \\hat{\\mathbf{x}}} \\log p \\left( \\hat{\\mathbf{x}}; \\sigma(t) \\right) \\right] \\right]dt$$\n\n\n$$d\\mathbf{x} = \\left[ \\frac{\\dot s(t)}{s(t)} \\mathbf{x} - s(t) \\left[\\dot \\sigma(t) \\sigma(t)\\nabla_{ \\hat{\\mathbf{x}}} \\log p \\left( \\hat{\\mathbf{x}}; \\sigma(t) \\right) \\right] \\right]dt$$\n\nNow we make a change of variables:\n\n$$\nd\\mathbf{x} =  s(t) d\\hat{\\mathbf{x}}\n$$\n\n$$\n\\frac{d\\mathbf{x}}{s(t)} =   d\\hat{\\mathbf{x}}\n$$\n\n$$\nd\\hat{\\mathbf{x}} = \\frac{d\\mathbf{x}}{s(t)}\n$$\n\n\n$$\n\\frac{d}{d\\hat{\\mathbf{x}}} =   s(t) \\frac{d}{d\\mathbf{x}}\n$$\n\n\n$$\n\\nabla_{\\hat{\\mathbf{x}}} \\log p(\\hat{\\mathbf{x}} ; \\sigma(t)) =\n\\frac{d}{d\\hat{\\mathbf{x}}} \\log p(\\hat{\\mathbf{x}}, \\sigma(t)) = \ns(t) \\frac{d}{d\\mathbf{x}} \\log p(\\hat{\\mathbf{x}}, \\sigma(t))\n$$\n\n$$\n= s(t) \\frac{d}{d\\mathbf{x}} \\log p\\left(\\frac{\\mathbf{x}}{s(t)}, \\sigma(t)\\right)\n$$\n\n$$\n= s(t) \\nabla_{\\mathbf{x}} \\log p\\left(\\frac{\\mathbf{x}}{s(t)}, \\sigma(t)\\right)\n$$\n\nAfter substituting:\n\n$$\nd\\mathbf{x} = \\left[ \\frac{\\dot s(t)}{s(t)} \\mathbf{x} - s(t) \\left[\\dot \\sigma(t) \\sigma(t)\\nabla_{ \\hat{\\mathbf{x}}} \\log p \\left( \\hat{\\mathbf{x}}; \\sigma(t) \\right) \\right] \\right]dt\n$$\n\n$$\nd\\mathbf{x} = \\left[ \\frac{\\dot s(t)}{s(t)} \\mathbf{x} - s(t) \\left[ \\dot \\sigma(t) \\sigma(t) s(t) \\nabla_{\\mathbf{x}} \\log p\\left(\\frac{\\mathbf{x}}{s(t)}, \\sigma(t)\\right) \\right] \\right] dt\n$$\n\n$$\nd\\mathbf{x} = \\left[ \\frac{\\dot s(t)}{s(t)} \\mathbf{x} - s(t)^2 \\left[ \\dot \\sigma(t) \\sigma(t) \\nabla_{\\mathbf{x}} \\log p\\left(\\frac{\\mathbf{x}}{s(t)}, \\sigma(t)\\right) \\right] \\right] dt\n$$\n\nThe formula above is our ODE when we have a scaling function.\n\n### Estimating the score function after scaling\nWe would like to use our denoiser for the unscaled variable $\\hat{\\mathbf{x}}$ to estimate the score function, even when there is scaling. Supposing we have a denoiser $D(\\hat{\\mathbf{x}} ; \\sigma(t))$ for the unscaled variable $\\hat{\\mathbf{x}}$, then we have that \n\n$$\n\\nabla \\log_{\\hat{\\mathbf{x}}} p \\left( \\hat{\\mathbf{x}}; \\sigma(t) \\right) = \\frac{ D(\\hat{\\mathbf{x}};\\sigma) - \\hat{\\mathbf{x}} }{ \\sigma(t)^2}\n$$\n\nWe also have from above that\n\n$$\n\\nabla \\log_{\\hat{\\mathbf{x}}} p \\left( \\hat{\\mathbf{x}}; \\sigma(t) \\right) = s(t) \\nabla_{\\mathbf{x}} \\log p\\left(\\frac{\\mathbf{x}}{s(t)}, \\sigma(t)\\right)\n$$\n\nThus, \n\n$$\n\\nabla_{\\mathbf{x}} \\log p \\left( \\frac{ \\mathbf{x} }{ s(t) }, \\sigma(t)\\right) = \\frac{ D( \\hat{\\mathbf{x} } ; \\sigma) - \\hat{ \\mathbf{x} } }{\\sigma(t)^2 s(t)}\n= \\frac{ D( \\frac{ \\mathbf{x} }{ s(t) } ; \\sigma) - \\frac{ \\mathbf{x} }{s(t)} }{ \\sigma(t)^2 s(t)}\n$$\n\nWe can use this function to estimate the score function from the denoiser.\n\n\n### Computing the Time-Derivative of x\nIn order to sample, we should compute the derivative $\\frac{d\\mathbf{x}}{dt}$. This can be used to estimate the denoising trajectory.\n\n\n$$\n\\frac{ d \\mathbf{x} }{ dt } = \\frac{ \\dot{s} (t) }{ s(t) } \\mathbf{x} - s(t)^2 \\left[ \\dot{\\sigma}(t) \\sigma(t) \\nabla_{\\mathbf{x}} \\log p \\left( \\frac{ \\mathbf{x} }{ s(t) }, \\sigma(t) \\right) \\right]\n$$\n\n$$\n\\frac{d \\mathbf{x} }{ dt } = \\frac{\\dot{s}(t)}{s(t)} \\mathbf{x} - s(t)^2 \\left[ \\dot{\\sigma}(t) \\sigma(t) \\frac{ D\\left( \\frac{\\mathbf{x}}{s(t)} , \\sigma \\right) - \\frac{\\mathbf{x}}{s(t)} }{\\sigma(t)^2 s(t)}\n \\right]\n$$\n\n\n$$\n\\frac{d \\mathbf{x} }{ dt } =  \\mathbf{x} \\left( \\frac{\\dot s(t)}{s(t)}  + \\frac{\\dot \\sigma(t)}{\\sigma(t)} \\right) - \\frac{\\dot \\sigma(t) s(t)}{\\sigma(t)}  D\\left(\\frac{\\mathbf{x}}{s(t)} ; \\sigma(t) \\right)\n$$\n\nWe can use this derivative for sampling.\n\n## Sampling\n\nSet a maximum noise level. The distribution of noisy data at the maximum noise level is:\n\n$$\np(\\mathbf{x} ; \\sigma_{max})\n$$\n\nThis should be indistinguishable from pure Gaussian Noise, assuming $\\sigma_{max} \u003e\u003e \\sigma_{data}$. However, the values here may be quite large, which is to say that they are not samples from a *standard* Gaussian distribution, like they are in DDPM. \n\n**Side Note**: In fact, imposing the constraint that the largest noise level has variance 1 requires a curvature to the noise trajectory, which we do not want. This is because the score function is the derivative of the noise trajectory, which is a *linear* approximation. Thus, for this approximation to be maximally accurate, we should have *linear* noise trajectories.\n\nDuring the sampling process, we should start with a sample from $$\\mathbf{x}_{0} \\sim \\mathcal{N} ( \\mathbf{0}, \\sigma_{max}^2 \\mathbf{I} )$$, and denoise images sequentially such that we get $\\mathbf{x}_1, \\mathbf{x}_1, \\ldots \\mathbf{x}_N$, where\n\n$$\n\\mathbf{x}_i \\sim p(\\mathbf{x}_i ; \\sigma_i)\n$$\nAnd\n$$\n\\sigma_{0} = \\sigma_{max} \u003e \\sigma_1 \u003e \\cdots \u003e \\sigma_N = 0\n$$\n\nThen, $\\mathbf{x}_N$ will be distributed like the data.\n\n**Notes:**\n- In the ODE formulation, the only source of randomness in backwards sampling is the initial noise $\\mathbf{x_0}$.\n- Theorectically, sampling should be independent from fitting $D$. $D$ is simply a black box.\n\n\n\n\n\n\n### Truncation Error\nTruncation error accumulates by discretizing time during sampling, but total truncation error decreases when the number of steps increases. In other words, local error scales super-linearly with respect to step size, so increasing step size by a factor of 2 increases per-step error by a factor of more than 2, and thus increases error overall.\n\nThe Euler Method has error $O(h^2)$ with respect to the step size $h$. Huen\u0027s method has error $O(h^3)$.\n\n\n### Huen\u0027s Method\n- Use a second-order Huen sampler.\n- This measures the derivative $\\frac{d\\mathbf{x}}{dt}$ at $\\mathbf{x}$, and after taking one step.\n- The actual derivative used is the average between these two derivatives.\n- When stepping to $\\sigma = 0$, we revert to Euler to avoid dividing by zero.\n\n\n\n\n\u003cspan style=\"color:blue\"\u003eTo Do: Show the actual algorithm\u003c/span\u003e.\n\n### Spacing the Time Steps\nStep size should decrease monotonically as noise level decreases, and does not need to depend on the content of the sample.\n\nThe step sizes decrease as we get closer to no noise. In the paper we have $\\rho = 7$, and that the step sizes are \n\n$$\n\\sigma_{i \u003c N} = \\left(\\sigma_{max}^{1/\\rho} + \\frac{i}{N-1}\\left(\\sigma_{min}^{1/\\rho} - \\sigma_{max}^{1/\\rho}\\right) \\right)^\\rho\n$$\nIn other words, we are doing linear interpolation in the $x^{1/\\rho}$ domain. \n\n**Note:** Imagine the square root function, and taking points equally space on the $y$ axis. Linear spacing on the $y$ axis corresponds to more spacing on the $x$ axis as $x$ increases. The severity of this disparity depends on $\\rho$.\n\nWhile $\\rho = 3$ apparently nearly equalizes truncation error between steps, $\\rho = 7$ works better, meaning that we want to make the disparity even more severe, or that the steps at lower noise levels matter more.\n\n\n### Specific Scale and Variance Schedule\nAlso, in the EDM formulation, we have:\n- Set $s(t) = 1 $\n- Set $\\sigma(t) = t$\n\nThis means that $t$ and $\\sigma$ become interchangeable. Also, since the noise trajectories are linear, we have that a single step to $t = 0$ will give you the denoised image. The tangent line to the trajectory points towards the denoiser output. The plots show you that we only have slight curvature at some intermediate time steps, but at the first and last time steps, we are linear.\n\n\n\u003cimg src=\"image-1.png\" width=\"400\"\u003e\n\n![Trajectories](image-1.png)\n\n\u003cimg src=\"./image-1.png\" width=\"400\"\u003e\n\n\nIt should make sense that since the derivative is a linear approximation to the noise trajectory, the noise trajectory should be as linear as possible.\n\nIn addition, the formula for the derivative becomes much simpler:\n\n$$\n\\frac{d\\mathbf{x}}{dt} = \\frac{\\mathbf{x} - D(\\mathbf{x}; t)}{t}\n$$\n\n\n\n## Stochastic Differential Equation\nWe can generalize our ODE to an SDE. Let\u0027s assume the parameters in the EDM formulation, $s(t) = 1$ and $\\sigma(t) = t$.\n\n$$\nd\\mathbf{x}_{\\pm} = - \\dot \\sigma(t) \\sigma(t) \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x} ; \\sigma(t)) dt \\pm \\beta(t) \\sigma(t)^2 \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x} ; \\sigma(t)) dt + \\sqrt{2 \\beta(t)}\\sigma(t) d \\omega_t\n$$\n\nNote that we get Song\u0027s formulation when we have $\\beta(t) = \\dot \\sigma(t)/\\sigma(t)$, where the first two terms cancel out and there is no score in the forward process.\n\nAlso note that if $\\beta(t)$ is zero, we have the non-stochastic ODE.\n\nThe three terms are, respectively:\n1. Probability Flow ODE.\n2. Deterministic Noise Decay.\n3. Noise Injection.\n\n**Notes**:\n- Note that $dt$ is negative during the backwards process, meaning the second term goes towards the data distribution during denoising.\n- During the denoising process, we can see term 2 as adding \"more score\" to the distribution, while the third time simultaneously adds \"more noise\".\n- When we add these two terms together, the net contribution to the noise level cancels out.\n- We can also think of the second term as removing more \u0027existing noise\u0027 and the third term as adding \u0027new noise\u0027. $\\beta$ (which scales up both the second and third terms) actually represents the degree to which new noise is replaced by existing noise.\n- The second two terms help drive the sample towards the marginal distribution $p(\\mathbf{x} ; \\sigma(t))$, I suppose this is accomplished by adding more noise to keep the noise level the same, while also adding in some score.\n\n\u003cspan style=\"color:blue\"\u003eTo Do: Think more about the last bullet point.\u003c/span\u003e\n\n### EDM\u0027s Sampler\nThis sampler includes churn, which adds and removes noise during the sampling process. At each step, we \n- Add noise to go from current noise level $t_i$ to a higher noise level $\\hat{t}_i$\n- Perform a single step from the higher noise level $$ \\hat{t}_{i} $$ to the a new, even lower noise level $t_{i+1}$.\n\n\nThis is actually slightly different than approximating the SDE above. In the SDE above, we add noise, but we attempt to correct for it using the score function from the noise level $t_i$ *before* adding noiss. This is incorrect because the noise level that is input to the score fucntion is different than the actual noise level of the data. The EDM sampler uses the noise level $\\hat{t}_i$ *after* adding noise. This is more accurate, and works better with larger step sizes.\n\nWe still can combine this with the second order Huen method.\n\n#### Problems with Too Much Churn\n- Too much churn can actually cause a loss of detail and drift towards oversaturated colors at low and high noise levels, possibly due to the fact that the score estimated by the denoiser network is a slightly non-conservative vector field.\n- The true score function should ideally be conservative, meaning that integrating across a trajectory should be path-independent. However, adding noise might put you in a region of the vector field such that getting back to the same denoising trajectory isn\u0027t easy.\n- This non-conservativity could be explained by the denoiser trying to remove too much noise, due to regression towards the mean.\n- \u003cspan style=\"color:blue\"\u003eTo Do: Think more about conservative vector fields.\u003c/span\u003e\n- \u003cspan style=\"color:blue\"\u003eTo Do: Think more about why denoisers remove too much noise, and regression toward the mean.\u003c/span\u003e\n\nAs a result, EDM does these modifications:\n- We only churn within a range in the middle of the noise schedule. \n- When we choose the \"churning noise\", we should theoretically choose  $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$, and then add $\\sqrt{\\hat{t}_i^2 - t_i^2} \\epsilon$ to the sample.\n- However, in practice we do $$\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, S^2_{noise} \\mathbf{I}) $$ , where $$S^2_{noise} \u003e 1$$. This means we sample add a little more noise than we should, to counteract the bias in the denoiser to denoise too much.\n- We define $S_{churn}$ as the \"total\" amount of churning, and choose $$ \\hat{t}_{i} = \\gamma t_{i} $$, where $$ \\gamma = \\min (\\frac{S_{churn}}{N}, \\sqrt{2} -1)$$.\n\n\n\n## EDM\u0027s Preconditioning\n- Learning $D$ directly is difficult - the inputs vary greatly in terms of magnitude.\n- Many works predict $\\mathbf{n}$ scaled to unit variance - this keeps the output magnitudes consistent.\n- However, for very high noise levels, this is difficult. The scale of the input is huge, and the denoised output is very small compared to it. We are basically relying on the noise estimate to cancel out with the input precisely such that their difference is on the same scale of the denoised data. If it does not do this at any index, the error will be huge\n- Perhaps at high noise levels, it is actually easier to predict the denoised signal, so that we can consistently predict at the correct scale.\n- Actually, our network predicts a mix of data and noise.\n- At high noise levels, an $\\epsilon$ prediction network behaves like an identity mapping, and predicting the clean data behaves like a zero mapping. Preconditioning means we have it behave like a zero mapping.\n- At low noise levels, an $\\epsilon$ prediction network behaves like zero mapping, while predicting clean data behaves like identity mapping. EDM\u0027s Preconditioning means we also have it behave like a zero mapping.\n\n## Other Notes\n### More Improvements\n- Adjust Hyperparameters from previous work.\n- Redistribute model capacity - remove lowest-resolution layers, double capacity of higher-resolution layers.\n- Use their preconditioning.\n- Use their loss function.\n- Use non-leaking augmentation - which is augmentation that does not change the data distribution.\n\n### Important Other Notes\n- Using their preconditioning doesn\u0027t improve performance, but does make training more robust.\n- For the best models on CIFAR-10, any amount of stochasticity is bad. But, for more diverse datasets, we benefit.\n- Maybe the effect of changing the preconditioning is dubious, it seems to give worse results in many cases.\n\n# Implementation\n## Datasets\nThey implement their modifications on 4 different datasets:\n- 32 x 32 CIFAR 10\n- 64 x 64 FFHQ\n- 64 x 64 AFHQv2\n- 64 x 64 ImageNet\n\n\n\n### Config A\n| Hyperparameter    | 32 x 32          | 64 x 64          |\n| --------------    | -------          | --------------   |\n| Training Examples | 200 Million      | Same             |\n| Batch Size        | 128              | Same             |\n| NFE               | 35               | 79               |\n| LR                | 2e-4             | 2e-4               |\n\n- It would be 400k training iterations at batch size 512.\n- Learning rate\n### Config B\n| Hyperparameter    | 32 x 32          | 64 x 64          |\n| --------------    | -------          | --------------   |\n| Training Examples | 200 Million      | Same             |\n| Batch Size        | 512              | 256              |\n| NFE               | 35               | 79               |\n| LR (Max)          | 1e-3             | 2e-4             |\n| EMA half-life     | 0.5              | Same             |\n| Dropout           | 13%              | 5/25%, FFHQ/AFHQ |\n\nOther Notes:\n- 4 -\u003e 8 GPUs\n- No gradient clipping\n- Learning rate ramp up to 1e-3 for 10 million images (instead of 0.64 million images)\n\n### Config C\nAdjust Config A:\n- remove 4x4 layers (these overfit), double 16x16 layer capacity\n\n\n| Resolution     | # Chan. - Conf. B | # Chan. Conf. C |\n| -------------- | -------           |  -------------- |\n| 64 x 64        | 128/None          | 128/None        |\n| 32 x 32        | 128               | 256             |\n| 16 x 16        | 256               | 256             |\n| 8 x 8          | 256               | 256             |\n| 4 x 4          | 256               | None            |\n\nReduces trainable parameters:\n - 32 x 32 - 56 million\n - 64 x 64 - 62 million\n\n### Configs D, E, F\n- D uses new preconditioning\n- E uses new noise distribution/loss weighting\n- F uses non-leaking augmentation (helps with overfitting)\n\n### ImageNet\n- 32 GPUs, batch size 4096 (128 per GPU)\n- Mixed Precision\n    - Weights stored at fp32, cast to fp16 during training\n    - Embedding/Self attention layers have no casting\n- 2 weeks, 2500 million images, 600k iterations\n - 296 million parameters\n- No augmentation\n- EMA of 50 million images\n- lr 1e-4\n- I believe it is 296 million parameters, for the 64x64 model in the diffusion beats GANs paper. It is 554M for 256.\n- For a 1D UNet, you probably increase the capacity of the model to account for the channel and frequency axis being the same.\n- You also multiplyg these 65 million parameters 3x since the kernels are smaller. \n- 188 M for a 256 x 80 model seems reasonable.\n- [1,1,2,2,4,4] for the 256 x\n\n## Architectures\nIn Config A, we start with:\n- VP uses DDPM++\n- VE uses NSCN++\nAnd use two different formulations. However, after all changes, they are the same except for the architecture.\n- DDPM++ seems to perform better according to the table.\n- Differences:\n    - DDPM++ uses Box filter [1, 1], while NSCN++ uses bilinear [1 3 3 1]\n    - NSCN++ has skip/residual blocks\n    - DDPM++ has positional encoding scheme, NSCN++ uses random Fourier features\n    - NSCN++ has extra residual skip connnections\n\n- For ImageNet, Use ADM architecture, no changes.\n- Compared to DDPM: \n    - Shallower model: 3 residual blocks per resolution\n    - More self attention layers (22 instead of 6) interspersed throughout model\n    - More attention heads (12 in lowest res.)\n    - More channels (768 in the lowest res. instead of 256)\n\nConclusion: For smaller datasets, use DDPM++.\n\n## Single-Level\n\n## Architectural Notes\n- To do: write these down in website\n\n\n### Block Level\n- There is 1 more decoder block than encoder block per level.\n- There are skip connections for EVERY Encoder UNet block (20 of them). \n- There are skip connections connecting the beginning of the block to the end.\n\n\n## Test Encoder\nWe use channel_mult = [2, 3, 4, 5] for clarity. In reality, it is [1, 2, 2, 2]\n| Name           | In Channels | Out Channels | In Resolutions | Out Resolutions |\n|----------------|-------------|--------------|----------------|-----------------|\n| 64x64_conv     | 3           | 128          | 64             | 64              |\n| 64x64_block0   | 128         | 256          | 64             | 64              |\n| 64x64_block1   | 256         | 256          | 64             | 64              |\n| 64x64_block2   | 256         | 256          | 64             | 64              |\n| 64x64_block3   | 256         | 256          | 64             | 64              |\n| 32x32_down     | 256         | 256          | 64             | 32              |\n| 32x32_block0   | 256         | 384          | 32             | 32              |\n| 32x32_block1   | 384         | 384          | 32             | 32              |\n| 32x32_block2   | 384         | 384          | 32             | 32              |\n| 32x32_block3   | 384         | 384          | 32             | 32              |\n| 16x16_down     | 384         | 384          | 32             | 16              |\n| 16x16_block0   | 384         | 512          | 16             | 16              |\n| 16x16_block1   | 512         | 512          | 16             | 16              |\n| 16x16_block2   | 512         | 512          | 16             | 16              |\n| 16x16_block3   | 512         | 512          | 16             | 16              |\n| 8x8_down       | 512         | 512          | 16             | 8               |\n| 8x8_block0     | 512         | 640          | 8              | 8               |\n| 8x8_block1     | 640         | 640          | 8              | 8               |\n| 8x8_block2     | 640         | 640          | 8              | 8               |\n| 8x8_block3     | 640         | 640          | 8              | 8               |\n\n## Decoder\n| Name           | In Channels | Out Channels | In Resolutions | Out Resolutions | Skip From  |\n|----------------|-------------|--------------|----------------|-----------------|------------|\n| 8x8_in0        | 640         | 640          | 8              | 8               |            |\n| 8x8_in1        | 640         | 640          | 8              | 8               |            |\n| 8x8_block0     | 1280        | 640          | 8              | 8               | 8x8_block3 |\n| 8x8_block1     | 1280        | 640          | 8              | 8               | 8x8_block2 |\n| 8x8_block2     | 1280        | 640          | 8              | 8               | 8x8_block1 |\n| 8x8_block3     | 1280        | 640          | 8              | 8               | 8x8_block0 |\n| 8x8_block4     | 1152        | 640          | 8              | 8               | 8x8_down   |\n| 16x16_up       | 640         | 640          | 8              | 16              |            |\n| 16x16_block0   | 1152        | 512          | 16             | 16              | 16x16_block3 |\n| 16x16_block1   | 1024        | 512          | 16             | 16              | 16x16_block2 |\n| 16x16_block2   | 1024        | 512          | 16             | 16              | 16x16_block1 |\n| 16x16_block3   | 1024        | 512          | 16             | 16              | 16x16_block0 |\n| 16x16_block4   | 896         | 512          | 16             | 16              | 16x16_down |\n| 32x32_up       | 512         | 512          | 16             | 32              |            |\n| 32x32_block0   | 896         | 384          | 32             | 32              | 32x32_block3 |\n| 32x32_block1   | 768         | 384          | 32             | 32              | 32x32_block2 |\n| 32x32_block2   | 768         | 384          | 32             | 32              | 32x32_block1 |\n| 32x32_block3   | 768         | 384          | 32             | 32              | 32x32_block0 |\n| 32x32_block4   | 640         | 384          | 32             | 32              | 32x32_down |\n| 64x64_up       | 384         | 384          | 32             | 64              |            |\n| 64x64_block0   | 640         | 256          | 64             | 64              | 64x64_block3 |\n| 64x64_block1   | 512         | 256          | 64             | 64              | 64x64_block2 |\n| 64x64_block2   | 512         | 256          | 64             | 64              | 64x64_block1 |\n| 64x64_block3   | 512         | 256          | 64             | 64              | 64x64_block0 |\n| 64x64_block4   | 384         | 256          | 64             | 64              | 64x64_conv |\n| 64x64_aux_norm |             |              |                |                 |            |\n| 64x64_aux_conv | 256         | 3            | 64             | 64              |            |\n\n\n## More details\n---All linear and conv have bias 0\n---All conv1s (second conv in block) have weight 1e-6\n---the output projection from attention also have weight 1e-6\n---also aux conv\nFirst thing:\n- 64x64 Conv\n\n\n\n\n\n\nBlock kwargs\n- 512 embedding channels\n- 1 head\n- 0.1 dropout\n- eps 1e-06\n- xavier uniform initialization\n- for attention blocks\n    - sqrt(0.2) as init_weight\n    - xavier uniform initialization\n\n- for zero-initialized\n    - 1e-5 init weight\n- [1,1] resample filter\n- resample projection is true\n- skip scale is 1/sqrt(2)\n\n\nUNet Block\n- x = conv0(silu(norm0(x)))\n- first norm is group norm with in_channels, 1e-6\n- first conv has kernel size 3, potential upsampling/downsampling, xavier uniform initialization\n- linear from emb_channels to out_channels\n- x + linear(embedding)\n- silu(norm2(x))\n- dropout x\n- conv1 x, conv2 has out, out, kernel size 3, 1e-5 initial weight\n-  \n\n\n\n### Positional Encoding\n\n    Notes:\n        - log(sigma) lyings 6 stds outside the mean has prob. 1.973e-9,\n        - This is 1 in 506 Million.\n        With P_mean=-1.2, range is [-8.4, 6]\n        c_noise ranges from -2.1 to 1.5\n\nLast Reviewed: 2/11/25    "}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\DiT/", "id": "DiT", "label": "DiT", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# DiT\n\n"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\DDIM/", "id": "DDIM", "label": "DDIM", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# DDIM\n\n## Overview\nGeneralize DDPMs from Markovian to non-Markovian forward processes. The training objective is actually the same.\nThis improves:\n- Generated Quality\n- Consistency property - if we generate using a different number of steps, we get similar high-level features.\n- Semantically meaningful image interpolation via latent variable interpolation.\n\n## Derivation\n\nNote that the DDPM objective depends only on the marginal distributions $$ q(\\mathbf{x_t} \\mid \\mathbf{x_0}) $$ and not $$ q(\\mathbf{x_t} \\mid \\mathbf{x_0}, ... , \\mathbf{x_T}) $$\n\n\n\nWe can think of some reformulations of diffusion models:\n\n$$\n\\alpha_{t-1}\\left(\\frac{x_t - \\sigma_t \\epsilon}{\\alpha_t} \\right) + \\sqrt{\\sigma^2_{t-1} - \\eta^2_t}\\hat{\\epsilon} + \\eta_t \\epsilon_t\n$$\n\nIn this case, we are predicting the clean data (in parens). Then we are jumping back to noise level $t-1$, by scaling by $\\alpha_{t-1}$, and adding two noise terms.\n\nThe first noise term represents the noise that existed in $x_t$ (estimated). The second noise term is fresh noise.\n\nThe variance of the noise we add is still $\\sigma_{t-1}$.\n\nAlso, see\n\nhttps://www.overleaf.com/read/fgrhhpqmtbgm#a55fc4 \n\nLast Reviewed 4/30/25\n\n\n"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Diffusion-Beats-GANs/", "id": "Diffusion Beats GANs", "label": "Diffusion Beats GANs", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Diffusion Models Beat GANs on Image Synthesis\n\n## Imagenet 64 x 64\n- 296M params\n- Dataset: 1.2 million images\n- 192 base channels\n- 1, 2, 3, 4 channel multiplication\n- 64 channels per head\n- 32, 16, 8 attention resolutions\n\n\n- 540k iterations\n- batch size 2048\n- 3e-4 learning rate\n\n\n## Imagenet 256\n- 554M params\n- Dataset: 540k images \n- 256 base channels\n- 1,1,2,2,4,4 channel multiplication\n- 64 channels per head\n- 32, 16, 8 attention resolutions\n\n- 1980K iterations, or 1.9 M iterations\n- batch size 256.\n- 1e-4 learning rate"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Edify-Image/", "id": "Edify Image", "label": "Edify Image", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Edify - Image\n"}, {"color": "#ffd900", "font": {"color": "white"}, "id": "Multi-Diffusion", "label": "Multi-Diffusion", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\SDEdit/", "id": "SDEdit", "label": "SDEdit", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# SDEdit\n\nTask - guided image synthesis - create/edit images w/ min. effort\n\nprevious = CGAN / GAN inversions, requires user annotations/training data or custom loss functions\n\nSDEdit adds noise to the brush strokes, then denoises.\n\nCan do stroke-based editing, or painting-to-image.\n\n\nCGAN turns image into edited by training\nGAN inversion turns the image into latent, then edits.\n\nmore noise = more realistic, less faithful.\n\n\nstroke based synthesis: better in terms of realism and satisfication from users\n\ncompositing: better faithfulness, better user satisfaction\n\n\n\n## Experiments\n\nTasks\n- stroke-based image generation\n    - metrics: user study for realism and faithfulness, L2, KID\n    - 4 GAN-based baselines\n- Stroke-based image editing\n    - qualitative\n- image blending\n    -  metrics: L2, LPIPS within region that should be kept the same, user study for realism and faithfulness\n\n\n### Figures\n- Teaser figure (fig 1) showing image editing and stroke-based synthesis\n- Fig 3: Plots showing tradeoff between KID and L2 squared (fig 3)\n- fig 4 - qualitative comparisons to baselines\n- fig 5 - conditional generation examples\n- fig 6 - stroke-based editing examples\n\n### Tables\n- Comparing faithfulness, quality, and satisfication on stroke-based generation - two MTurk surveys asking about realism and satisfactory, and L2 faithfulness score - LSUN, CelebHQ. Use open-source models.\n- L2, KID of LSUN bedroom/church. Simulated strokes.\n\n\n### Baselines:\n- in domain GAN 1\n- in domain GAN 2\n- styleGAN\n- e4e\n\n\n### Metrics\nRealism - measured by humans or NN\nfaithfulness - similar to guide, L2 distance\nUser metrics on realism and faithfulness\n\n\n### More Notes\n\nStroke based image editing:\n- human-created guides\n- simulated stroke paintings\n\n-theoretical upper bound on guide and generation, based on L2 distance - to have a shot of being realistic need high noise level, to be faithful, noise level should not be too high. bad guides (white image) mean higher noise level needed - since the closest images to the input are quite far.\n    - binary search for t0 based on user preference.\n    - same t0 usually works for all reasonable guides in the same task.\n- we can also mask out part of the image we do not want to edit - additional channel\n\n\n\n\n\n## Related Work\nConditional GANs - trained on original and edited images, data collection, model retraining -\n\nGAN inversion - input is projected into latent space of a GAN, code is modified, image is resynthesized - all need different losses for different tasks.\n\nother generative models - less used for editing, choi 2021 does conditional image synthesis assuming conditions can be measured from the underlying true image.\n\n\n### Other notes\nunlike other inverse problems, do not know measurement function (image to sketch function)\nrealism and faithfulness are not positively correlated, there are random realistic images or \nuse of score-based models to solve inverse problems and methods requiring paired datasets do not apply\nkey hyperparameter is t_0\n\nexperimental details in appendix\n"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Diffusion-Forcing/", "id": "Diffusion Forcing", "label": "Diffusion Forcing", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": "# Diffusion Forcing\n\nSee notes on paper\n"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\History-Guidance/", "id": "History Guidance", "label": "History Guidance", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# History Guidance\n\nVideo Diffusion\nWant to condition basd on history\n\nhard to condition on varaible length input\n\nan order of magnitude less compute than industry\n\nTemporal History Guidance - combines scores from different history windows\n\nFractional History Guidance - corrupts history windows with noise\n\nCombine HG-t and HG-f for history guidance across time and frequency\n\nCompose score from multiple conditioning"}, {"color": "#ffd900", "font": {"color": "white"}, "id": "Understanding Diffusion Models: A Unified Perspective", "label": "Understanding Diffusion Models: A Unified Perspective", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\posts\\Score-Based-Generative-Models/", "id": "Score-Based Generative Models", "label": "Score-Based Generative Models", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": "# Score-Based Generative Models\n{% raw %}\n\n## Motivation\n\nGiven datapoints $\\mathbf{x_1}, \\ldots , \\mathbf{x}_N$ independently[^1] drawn from an underlying distribution $p(\\mathbf{x})$, we would like to model the data distribution $p(\\mathbf{x})$. \n\n[^1]: Note that datapoints are assumed to be independent, i.e., the generation of a datapoint $\\mathbf{x_i}$ does not influence the generation of another datapoint $\\mathbf{x_j}$.\n\n\nWe can parametrize the disribution as $p_\\theta(\\mathbf{x})$, and try maximizing the probability of observing all data points, which is\n\n$$\np_\\theta(\\mathbf{x_1}, \\ldots , \\mathbf{x}_N) = \\prod_{i=1}^N p_\\theta(\\mathbf{x_i}).\n$$\n\n\nThis is equivalent to maximizing the log probability:\n\n$$\n\\log p_\\theta(\\mathbf{x_1}, \\ldots , \\mathbf{x}_N) = \\sum_{i=1}^N \\log(p_\\theta(\\mathbf{x_i}))\n$$\n\nIf we do not require $p_\\theta(\\mathbf{x})$ to be a valid probabilty density function (that integrates to $1$), the values of $p_\\theta(\\mathbf{x})$ will become arbitrarily large (and can also do so even if we impose the constraint that it integrates to $1$, depending on the parametrization). However, ensuring this density function integrates to one is typically not tractable.\n\n\n## Definition\n\nThe score function is the gradient of the log of the probability density function, or\n\n$$\n\\nabla_\\mathbf{x} \\log(p(\\mathbf{x}))\n$$\n\n![Score function of a mixture of 2 Gaussians](MOG_Score.png)\n\n*Score function of a mixture of 2 Gaussians*\n\n\n\n## Avoiding the Normalizing Constant\nModeling the score function does not require knowing the normalizing constant that results in $p(\\mathbf{x})$ integrating to 1.\n\nFor instance, we can model any probability distribution as \n$$\n\\frac{e^{-f_\\theta(\\mathbf{x})}}{Z_\\theta}\n$$\nWhere $Z_\\theta$ is a normalizing constant, which depends on $\\theta$.\n\nTaking the log of this PDF results in\n\n$$\n\\log{e^{-f_\\theta(\\mathbf{x})}} - \\log{Z_\\theta} = \n-f_\\theta(\\mathbf{x})- \\log{Z_\\theta}\n$$\n\nAnd taking the gradient of this results in the second term vanishing:\n\n$$\n-\\nabla_\\mathbf{x} f_\\theta(\\mathbf{x}) - \\nabla_\\mathbf{x} \\log{Z_\\theta} =\n-\\nabla_\\mathbf{x} f_\\theta(\\mathbf{x}) = s_\\theta (\\mathbf{x})\n$$\n\nThe normalizing constant imposes restrictive architectural choices (like invertible layers in normalizing flows) or discretization (like outputting a softmax distribution). However, fitting $s_\\theta$ does not require considering the normalizing constant, which allows for more flexibility. In fact, $s_\\theta(\\mathbf{x})$ only has the constraint that it should have correct input and output dimensionality, namely, both should equal the dimensionality of $\\mathbf{x}$.\n\n## Optimization\nWe can attempt to fit the score function by minimizing \n\n$$\nE_{p(\\mathbf{x})}\\left[ \\lVert \\nabla_\\mathbf{x} \\log(p_\\theta(\\mathbf{x})) - s_\\theta (\\mathbf{x}) \\rVert^2 _2 \\right]\n$$\n\nHow do we know the ground truth score function? There is a technique called **score matching**.\n\n## Langevin Sampling\nLangevin sampling allows you to sample from a distribution given its score function. First, we draw from an arbitrary distribution:\n\n$$\nx_0 \\sim \\pi(\\mathbf{x})\n$$\n\nThen we iteratively perform \"noisy gradient ascent\":\n\n$$\nx_{i+1} = x_{i} + \\epsilon \\nabla_x \\log (p(\\mathbf{x})) + \\sqrt{2 \\epsilon} z_i, \\quad i = 0, \\ldots, K\n$$\n\nWhere $z_i \\sim \\mathcal{N}(0,I)$.\n\nAs the step size $\\epsilon \\rightarrow 0$, and the number of steps $K \\rightarrow \\infty$, this will result in a sample from $p(\\mathbf{x})$.\n\n![Using Langevin Dynamics to sample from a mixture of 2 Gaussians](langevin_sampling.gif)\n\n*Using Langevin Dynamics to sample from a mixture of 2 Gaussians.*\n\nInstead of using the ground-truth score function, we can plug in our estimate for the score function in order to generate samples.\n\n### Additional Thoughts\nUsing the log allows for *larger* absolute scores near the edges of the Gaussian, since the numbers are smaller there, so the logarithm is large. Thus, scores are larger the further away we are from the mean of the Gaussian, which is what we want, since we want to jump further.\n\n## Adding Noise\n### Problems with Naive Langevin Sampling\n\nModeling the score function naively is difficult, because our estimates of the score function are inaccurate in low-density regions.\n\nNote that\n\n$$\n\\mathbb{E}_{p(\\mathbf{x})}\\left[ \\lVert \\nabla_\\mathbf{x} \\log(p_\\theta(\\mathbf{x})) - s_\\theta (\\mathbf{x}) \\rVert^2 _2 \\right] =\n\\int p(\\mathbf{x}) \\lVert \\nabla_\\mathbf{x} \\log(p_\\theta(\\mathbf{x})) - s_\\theta (\\mathbf{x}) \\rVert^2 _2  d \\mathbf{x}\n$$\n\nSo the distance between the score and our estimate are downweighted in regions where $p(\\mathbf{x})$ is small (low density regions). (In practice, the expectation is computed by sampling real data, so the score estimate is inaccurate where we have little data).\n\nThis complicates Langevin sampling, since $\\mathbf{x}_0$ (the first sampling step) usually starts in a low-density region according to the data distribution (it is usually just noise).\n\n\n### Additional Thoughts\n\n#### Adding two random variables\n1. Recall from [Brad Osgood\u0027s course](https://see.stanford.edu/course/ee261) that adding two random variables $Z = X + Y$ results in a distribution that is a *convolution* of the distributions of $X$ and $Y$.\n\n2. By adding noise to our data, we are essentially *smoothing* the data distribution with a *gaussian kernel*.\n\n3. This smoothing can be thought of as *increasing the support* of the distribution. Think about it - if we add lots of noise, almost anything can arise with some probability.\n\n4. In addition, we can represent our data distribution as a set of discrete samples, or impulse functions at each datapoint. The sifting theorem means that the noisy data distribution can be represented as a **mixture of Gaussians**.\n\n#### Score function of a Gaussian\n\n1. Note that the score function of a Gaussian is linear:\n\n$$\n\\nabla \\log \\left[\\exp\\frac{-(x-\\mu)^2 }{\\sigma^2} \\right] = \\frac{-2(x - \\mu)}{\\sigma^2}\n$$\n\n2. In fact, if we consider the Gaussian distribution as originating from a data point at $\\mu$ with added noise $n \\sim \\mathcal{N}(0,\\sigma^2)$, then we have that a sample $x$ from the Gaussian is:\n\n$$\nx = \\mu + \\sigma n\n$$\n\nAnd the score function is \n\n$$\n\\frac{-2(x - \\mu)}{\\sigma^2}\n = \\frac{-2(\\mu + \\sigma n - \\mu)}{\\sigma^2}\n = \\frac{-2n}{\\sigma}\n$$\n\nThat means that the score function is proportional to the noise added!\n\n#### Score function of Mixture of Gaussians\n1. The score function of a *mixture of gaussians* is approximately piecewise linear. It would be exactly piecewise linear if our smoothed distribution was piece-wise Gaussian. In a mixture of Gaussians, there is some spillover from other Gaussians everywhere.\n\n2. Since ReLU neural networks output piecewise linear functions, they are great for modeling score functions. (This was from Mert\u0027s class)\n\n## Improved Score Matching\nAdding lots of noise corrupts the data distribution substantially, while adding low levels of noise may not result in enough smoothing or coverage. We can choose $L$ levels of noise:\n\n$$\n\\sigma_1 \u003c \\sigma_2 \u003c \\cdots \u003c\\sigma_L.\n$$\n\nWe can compute the noisy distributions:\n\n$$\np_{\\sigma_i}(\\mathbf{x}) = \\int p(\\mathbf{y}) N_\\mathbf{x}(\\mathbf{y}, \\sigma_i^2 I) d \\mathbf{y}\n$$\n\n\n### Additional Thoughts\n#### Interpreting the Integral\nWe can view computing this integral like this:\n1. Iterate through all possible data examples $\\mathbf{y}$\n2. Compute the probability that $\\mathbf{x}$ occurs under a normal distribution centered at $\\mathbf{y}$.\n3. Sum across all possible values of $\\mathbf{y}$, weighted by the data distribution $p(\\mathbf{y})$.\n\n#### Expectation\nThis can also be viewed as an expectation:\n\n$$\n\\int p(\\mathbf{y}) N_\\mathbf{x}(\\mathbf{y}, \\sigma_i I) d \\mathbf{y} = \\mathbb{E}_{y \\sim p(\\mathbf{y})}[N_\\mathbf{x}(\\mathbf{y}, \\sigma_i^2 I)]\n$$\n\nOr, when we sample $\\mathbf{y}$ from our data distribution $p(\\mathbf{y})$, what is the expected density of a normal distribution centered at $\\mathbf{y}$?\n\n#### Convolution\nNote that this can be seen as the convolution between two probability distributions:\n\n$$\np_{\\sigma_i}(\\mathbf{x}) = \\int p(\\mathbf{y}) q(\\mathbf{x - y}) d \\mathbf{y}\n$$\n\nWhere \n\n$$\nq(\\mathbf{z}) = N_\\mathbf{z}(0, \\sigma_i I)\n$$\n\n\n#### Flipped Convolution\nSince convolution is commutative, there are two ways to express a convolution, and we can also express it as \n\n$$\np_{\\sigma_i}(\\mathbf{x}) = \\int p(\\mathbf{y}) q(\\mathbf{x - y}) d \\mathbf{y} = \\int q(\\mathbf{y}) p(\\mathbf{x - y}) d \\mathbf{y}\n$$\n\n\nIn this case, we integrate over all possible values of the normal distribution $q$, and evaluate $ \\mathbf{x - y}$ (or equivalently $\\mathbf{x + y}$, since adding noise subtracting noise do the same thing) according to the data distribution.\n\nBasically, there\u0027s a different way to \"add up\" to $\\mathbf{x + y}$, one for each value of $\\mathbf{y}$ sampled from a normal distribution. We\u0027re essentially accumulating across all these possible ways to \"add up\" to $\\mathbf{x}$. We can also express this as an expectation:\n\n$$\n\\mathbb{E_{\\mathbf{y} \\sim q(\\mathbf{y})}}[p(\\mathbf{x - y})]\n$$\n\nWhich is saying, draw a sample (noise) from the normal distribution, and evaluate the probability of the data distribution at the point where example that results from *subtracting* that noise.\n\nIn other words, $\\mathbf{y}$ is the noise added to the data example $\\mathbf{x}$, and we are evaluating $p(\\mathbf{x})$ according to the data distribuition, but we are aggretating over all possible noises that could have been added, which is $\\mathbf{y}$.\n\nWe can also think of this as fixing the noise vector, and asking, \"What is the probability density if the noise is this?\" Then we accumulate over all possible noise vectors.\n\n#### End Additional Thoughts\nDrawing samples from $p_{\\sigma_i}(\\mathbf{x})$ is easy, we can just draw a sample $x \\sim p(\\mathbf{x})$ from our dataset and add noise to get $\\mathbf{x} + \\sigma_i \\mathbf{z}$.\n\nWe can fit a neural network to the score function of each noisy distribution:\n\n$$\ns_\\theta (\\mathbf{x}, i) \\approx \\nabla_\\mathbf{x} \\log p_{\\sigma_i}(\\mathbf{x}), \\quad \\forall i = 1,\\dots,L\n$$\n\nThe training objective is:\n\n$$\n\\sum_{i=1}^L \\lambda(i) \\mathbb{E}_{\\mathbf{x} \\sim p_{\\sigma_i}}[\\lVert \\nabla_\\mathbf{x} \\log(p_{\\sigma_i}(\\mathbf{x})) - s_\\theta (\\mathbf{x}, i) \\rVert^2 _2]\n$$\n\nUsually, the loss weighting is $\\lambda(i) = \\sigma_i^2$. This would mean higher noise levels have a greater loss.\n\nThe ground truth score estimates are usually $\\frac{-\\mathbf{z}}{\\sigma}$\n\nWe can run Langevin dynamics in sequence for each noise level. This means runnning Langevin chains for all noise levels.\n\n\n#### Recommendations\nWe can choose the noise levels in a geometric progression (there is a common ratio). $\\sigma_L$ can be the maximum pairwise distance between two datapoints, and $L$ can be hundreds or thousands.\n\n## Next steps\nAs $L \\rightarrow \\infty$, the noise level becomes a continuous-time stochastic process, where noise is added. This will allow for generative modeling using [SDEs](Generative-Modeling-Using-SDEs.md). \n\nLast Reviewed: 2/4/25\n\n{% endraw %}\n"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\posts\\Generative-Modeling-Using-SDEs/", "id": "Generative Modeling Using SDEs", "label": "Generative Modeling Using SDEs", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": "# Generative Modeling Using SDEs\n{% raw %}\n\n## Forward-Time Diffusion\n\nFor a noise-conditional score network, there is a finite number of noise levels.\n\nAs the number of diffusion timesteps approaches infinity ($$L \\rightarrow \\infty$$), the noise level becomes a continuous-time stochastic process, where noise is added continuously.\n\n\nWe have:\n\n$$\nd\\mathbf{x} = \\mathbf{f}(\\mathbf{x}, t) dt + g(t)d\\mathbf{w}\n$$\n\n$$d\\mathbf{w}$$ can be viewed as infinitesimal white noise, and characterizes a [Wiener Process](Wiener-Process.md).\n\n$$\\mathbf{f}$$ is a vector-valued function representing the *drift coefficient* and $$g(t)$$ is a scalar-valued function called the *diffusion coefficient*.\n\nWe can imagine applying this diffusion process to our data. As $$t$$ changes, the distribution of our data changes. We can use $$p_t(\\mathbf{x})$$ to denote the probability distribution of our data at diffusion time $$t$$, where $$t \\in [0,T]$$. The data distribution (with no diffusion) is $$p_0(\\mathbf{x})$$.\n\nIn addition, we can let $$\\mathbf{X}_t$$ be a random variable representing the value of a datapoint at diffusion time $$t$$, and use $$\\mathbf{x}_t$$ to represent a realization of $$\\mathbf{X}_t$$.\n\nWe choose our diffusion process so that $$p_T$$ does not depend on $$p_0$$.\n\n#### Additional Thoughts\n- In DDPMs, the *drift coefficient* $$\\mathbf{f}(\\mathbf{x}, t)$$ would be analogous to $$\\sqrt{1-\\beta_t}$$ when defining the forward process. It is kind of like the derivative of the coefficient on $$\\mathbf{x}_0$$.\n- in DDPMs, the diffusion coefficient is analogous to $$\\sqrt{\\beta_t}$$. It is kind of like the derivative of $$\\sigma_i$$ or the derivative of the coefficent on the noise term in DDPMs. It represents the variance of the noise added at each step.\n\n\n### Choices of $$\\mathbf{f}(\\mathbf{x},t), g(t)$$\n\nWe can choose the diffusion and drift coefficients as hyperparameters. For instance, we can choose a forward diffusion process:\n\n$$\nd\\mathbf{x} = e^t d\\mathbf{w}\n$$\nWhich means that the variance grows exponentially as time increases, and is similar to choosing a geometric progression of noise scales $$\\sigma_1, \\ldots, \\sigma_L$$. For instance, the Variance Exploding SDE, the Variance Preserving SDE, and the sub-VP SDE work well for generating images.\n\n## Reverse-Time Diffusion\n\nThe reverse diffusion process is defined as:\n\n$$\nd\\mathbf{x} = \\left[\\mathbf{f}(\\mathbf{x},t) - g^2(t)\\nabla_\\mathbf{x} \\log(p_t(\\mathbf{x}))\\right] dt + g(t)d\\mathbf{\\bar{w}}\n$$\n\nIf we know the drift and diffusion coefficients (which are from the forward process), and can approximate the score function (the gradient of the data distributions at time $$t$$), then we can compute the reverse-time SDE.\n\nWe can sample from any marginal distribution $$p_t$$ (determined by the forward process) by sampling from $$p_T$$ (which should be doable, since this should approximate an easy-to-sample from distribution), then by integrating from $$T$$ to $$t$$. By integrating all the way to $$t=0$$, we should be able to generate a data sample.\n\n#### Additional Thoughts\n- Note that $$dt$$ represents an infinitesimal *negative* time step, since we are integrating from higher $$t$$ to lower $$t$$.\n- **What is a Reverse-Time SDE?** In this case, $$\\mathbf{\\bar{w}}$$ is described as \u0027reverse-time brownian motion\u0027. In practice, $$d\\mathbf{\\bar{w}}$$ is Gaussian Noise, just like $$d\\mathbf{w}$$ in the forward process.\n\n\n\n\n## Learning the Score Function\nWe would like to fit the score function such that\n\n$$\n\\mathbf{s}_\\theta(\\mathbf{x}, t) \\approx \\nabla_\\mathbf{x} \\log p_{t} (\\mathbf{x})\n$$\n\nThis should be a continuous weighted sum of:\n\n$$\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\mathbb{E}_{p_t(\\mathbf{x})}\n\\left[\n\\lambda(t)\n\\lVert\n\\nabla_\\mathbf{x} \\log p_{t} (\\mathbf{x}) -\n\\mathbf{s}_\\theta(\\mathbf{x}, t)  \\rVert^2_2\n\\right]\n$$\n\nWhere $$\\lambda(t)$$ is a weighting function. We would like to balance the losses across time, so we choose\n\n$$\n\\lambda(t) \\propto 1/\n\\mathbb{E}_{p_t(\\mathbf{x})}\n\\lVert\n\\nabla_{\\mathbf{x}_t}\n\\log p_{t} (\\mathbf{x}_t \\mid \\mathbf{x}_0) \\rVert^2_2\n$$\n\nTo implement this weighting, we also rescale the outputs of the network, so its outputs are on the same scale across time-steps.\n\nIn addition, we use the exponential moving average of the weights during sampling.\n\n#### Additional Thoughts:\n- The expectation here is proportional to the expected amount of noise added. \n\n- Note that in DDPM, the network is always predicting samples from $$\\mathcal{N}(0, I)$$ no matter the timestep, which is similar to rescaling the outputs of the network in DDIM.\n\n\nWe can use denoising score matching to optimize this objective.\n\n### Actual Optimization\nWe have\n\n$$\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\mathbb{E}_{p_t(\\mathbf{x})}\n\\left[\n\\lambda(t)\n\\lVert\n\\nabla_\\mathbf{x} \\log p_{\\sigma_t} (\\mathbf{x}) -\n\\mathbf{s}_\\theta(\\mathbf{x}, t)  \\rVert^2_2\n\\right]\n$$\n\nWe can estimate this objective as \n\n$$\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\lambda(t)\n\\left[\n\\mathbb{E}_{\\mathbf{x_0} \\sim p_0(\\mathbf{x})}\n\\mathbb{E}_{\\mathbf{x_t} p_t(\\mathbf{x_t} \\mid \\mathbf{x_0})}\n\\left[\n\\lVert\n\\nabla_\\mathbf{x} \\log p_{t} (\\mathbf{x_t} \\mid \\mathbf{x}_0) -\n\\mathbf{s}_\\theta(\\mathbf{x_t}, t)  \\rVert^2_2\n\\right]\n\\right]$$\n\n#### Additional Thoughts\n- We would like the score function to fit $$\\nabla_\\mathbf{x_t} \\log p_{t} (\\mathbf{x_t})$$, but this distribution may be complex and multimodal. However, $$\\log p_{t} (\\mathbf{x_t} \\mid \\mathbf{x}_0)$$ is just Gaussian.\n\n- Since the score function we are fitting does not see $$\\mathbf{x_0}$$, it will fit an expectation over all possible $$\\mathbf{x_0}$$, which will approximate the score function of $$p_t(\\mathbf{x_t})$$.\n- The probability distribution of $$p_t(\\mathbf{x_t})$$ is a mixture of Gaussians, which means that the conditional distribution $$\\log p_{t} (\\mathbf{x_t} \\mid \\mathbf{x}_0)$$ locally approximates $$\\log p_{t} (\\mathbf{x_t})$$ at the point $$\\mathbf{x_t}$$.\n\n### A derivation that both objectives are equivalent\nOur intended objective is:\n\n$$\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\mathbb{E}_{\\mathbf{x} \\sim p_t(\\mathbf{x})}\n\\left[\n\\lambda(t)\n\\lVert\n\\nabla_\\mathbf{x} \\log p_{\\sigma_t} (\\mathbf{x}) -\n\\mathbf{s}_\\theta(\\mathbf{x}, t)  \\rVert^2_2\n\\right]\n$$\n\nWe would like to show this objective is equivalent:\n\n$$\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\lambda(t)\n\\left[\n\\mathbb{E}_{\\mathbf{x_0} \\sim p_0(\\mathbf{x})}\n\\mathbb{E}_{\\mathbf{x_t} \\sim p_t(\\mathbf{x_t} \\mid \\mathbf{x_0})}\n\\left[\n\\lVert\n\\nabla_\\mathbf{x} \\log p_{t} (\\mathbf{x_t} \\mid \\mathbf{x}_0) -\n\\mathbf{s}_\\theta(\\mathbf{x_t}, t)  \\rVert^2_2\n\\right]\n\\right]\n$$\n\n\n#### Lemma\n\nWe first show that we can estimate the marginal distribution as an expecation of the conditionals:\n\n$$\n\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x_t}) = \\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0 \\mid \\mathbf{x_t})} \\left[\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0})\n\\right]\n$$\n\n\u003cspan style=\"color:blue\"\u003eTo Do: Prove this Lemma\u003c/span\u003e.\n\nIn other words, we can approximate the marginal score function by sampling from the posterior, and aggregating the score esimates that we can from single samples.\n\n#### Bias Variance Decomposition\nBy the law of total variance, we have:\n\n$$\n\\mathbb{E} \\| Z - a \\|^2_2 =  \\| \\mathbb{E}[Z] - a \\|^2_2 + \\mathbb{E} \\left[ \\| Z - \\mathbb{E}[Z] \\|^2_2 \\right] \\\\[10pt]\n= \\| \\mathbb{E}[Z] - a \\|^2_2 + \\operatorname{Var}(Z)\n$$\n\nFor any random variable $$Z$$.\n\n\u003cspan style=\"color:blue\"\u003eTo Do: Prove this Decomposition\u003c/span\u003e.\n\n#### Another Equality\nConsider the new expression\n\n$$\n\\tag{*}\n\\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0 \\mid \\mathbf{x_t})} \\left[\n\\left\\lVert\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 \\right]\n$$\n\nBy the bias-variance decomposition, this is equal to\n\n$$\n\\left\\lVert\n\\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0 \\mid \\mathbf{x_t})} \\left[\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0})\n\\right] - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 +\n\\operatorname{Var}\\left[\\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0})\\right] = \\\\[10pt]\n\\left\\lVert\n\\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0 \\mid \\mathbf{x_t})} \\left[\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0})\n\\right] - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 + C\n$$\n\nWhere we are use $$C$$ to denote the variance expression, which is constant given the forward process and does not depend on model parameters $$\\theta$$. By the Lemma, this is equal to\n\n$$\n\\tag{**}\n\\left\\lVert\n\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x_t}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 + C \n$$\n\nWhich is our score matching objective for a single timestep, plus a constant.\n\n#### Wrapping in expectation\nLet us wrap both starred expressions (which we have proven to be equal) in an expectation over $$\\mathbf{x}_t$$:\n\n$$\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t)}\n\\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0 \\mid \\mathbf{x_t})} \\left[\n\\left\\lVert\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 \\right] = \\\\[10pt]\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t)}\\left[\n\\left\\lVert\n\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x_t}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 + C \\right]\n$$\n\nBy using the chain rule:\n\n$$\n\\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0)}\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t \\mid \\mathbf{x}_0)}\n\\left[\n\\left\\lVert\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 \\right] = \\\\[10pt]\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t)}\\left[\n\\left\\lVert\n\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x_t}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 + C \\right]\n$$\n\nIf we wrap both in another expectation over $$t$$, and multiply by $$\\lambda(t)$$, we get\n\n$$\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\lambda(t)\n\\left[\n\\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0)}\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t \\mid \\mathbf{x}_0)}\n\\left[\n\\left\\lVert\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 \\right]\\right] = \\\\[10pt]\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\lambda(t)\n\\left[\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t)}\\left[\n\\left\\lVert\n\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x_t}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 + C \\right] \\right]\n$$\n\nThe $$C$$ term can be taken out of the inner expectation, and remains constant with repsect to $$\\theta$$ when it is taken outside of the outer expecation:\n\n$$\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\lambda(t)\n\\left[\n\\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0)}\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t \\mid \\mathbf{x}_0)}\n\\left[\n\\left\\lVert\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 \\right]\\right] = \\\\[10pt]\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\lambda(t)\n\\left[\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t)}\\left[\n\\left\\lVert\n\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x_t}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 \\right] \\right] + C_2\n$$\n\nThe left hand side is the objective that we train with, and the right hand side is the optimization objective. Thus, we have that the two optimization objectives are equivalent - they differ by a constant that does not depend on our parameters $$\\theta$$.\n\n### \u0027Likelihood\u0027 Weighting\n\nIf $$\\lambda(t)$$ = $$g^2(t)$$, we have that\n\n$$\nD_{\\text{KL}}\\left(p_0(\\mathbf{x}) \\right) \\| p_\\theta(\\mathbf{x})) = \\frac{T}{2} \\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\mathbb{E}_{\\mathbf{x} \\sim p_t(\\mathbf{x})}\n\\left[\n\\lambda(t)\n\\lVert\n\\nabla_\\mathbf{x} \\log p_{\\sigma_t} (\\mathbf{x}) - \n\\mathbf{s}_\\theta(\\mathbf{x}, t)  \\rVert^2_2\n\\right] + \\\\ D_{\\text{KL}}\\left(p_T\\left(\\mathbf{x}_t\\right) \\| \\pi \\left(\\mathbf{x}_T\\right) \\right)\n$$\n\nWhere $$p_\\theta(\\mathbf{x})$$ is the distribution we get for $$\\mathbf{x}_0$$ we get using our estimated score function, and $$\\pi$$ is a simple distribution that it is easy to sample from.\n\n\n## Solving the Reverse-Time SDE\n\n### The Integral\nTo generate samples, we can evaluate the reverse-time SDE. This is as simple as coming up with a numerical approximation to the integral of the reverse-time SDE process:\n\n$$\nd\\mathbf{x} = \\left[\\mathbf{f}(\\mathbf{x},t) - g^2(t)\\nabla_\\mathbf{x} \\log(p_t(\\mathbf{x}))\\right] dt + g(t)d\\mathbf{\\bar{w}}\n$$\n\n##### Additional Notes\n\nWe can express our SDE in integral form:\n\n$$\n\\mathbf{x_t} = \\mathbf{x}_{t_0} + \\int_{t_0}^t \\left[\\mathbf{f}(\\mathbf{x},s) - g^2(s)\\nabla_\\mathbf{x} \\log(p_s(\\mathbf{x}))\\right] ds + \\int_{t_0}^t g(s)d\\mathbf{\\bar{w}}(s)\n$$\n\nWhere\n\n$$\nd\\mathbf{\\bar{w}}(s) = \\mathcal{N}(0, ds) ds\n$$\n\n\u003cspan style=\"color:blue\"\u003eQuestion - can we get here by intergrating both sides of the SDE? If so, how?\u003c/span\u003e.\n\nParticularly, to get a sample from the marginal distribution, we can evaluate\n\n$$\n\\mathbf{x}_t = \\mathbf{x}_T + \\int_{T}^t \\left[\\mathbf{f}(\\mathbf{x},s) - g^2(s)\\nabla_\\mathbf{x} \\log(p_s(\\mathbf{x}))\\right] ds + \\int_{T}^t g(s)d\\mathbf{\\bar{w}}(s)\n$$\n\nSince we can sample $$\\mathbf{x}_T$$ in closed form.\n\nFinally, to generate a sample from the data distribution, we are interested in $$\\mathbf{x}_0$$. This be obtained by setting $$t=0$$ in the above equation.\n\n### Numerical Methods\nChoose a small $$\\Delta t \u003c 0$$.\n\nThen perform this process:\n\n1. $$\\mathbf{x}_T \\sim \\pi$$\n\n2. $$\n\\mathbf{x}_{t + \\Delta t} = \\mathbf{x}_t + \\left[ f(\\mathbf{x}_t,t) - g^2(t) s_\\theta(\\mathbf{x}_t, t) \\right] \\Delta t + g(t)\\mathbf{z}_t\\sqrt{| \\Delta t |}$$\n\nWhere $$\\mathbf{z}_t \\sim \\mathcal{N}(0, I)$$. \n\n##### Additional Notes\n-Note that the last term on the right side is a way to sample from $$\\mathcal{N}(0, | \\Delta t |)$$.\n\n-This is similar to Langevin Sampling or \"noisy gradient descent\" for when time was not continuous.\n\n#### Predictor-Corrector Methods\nThe predictor chooses a step size $$\\Delta t$$, then predicts $$\\mathbf{x}_{t + \\Delta t}$$ based on $$\\mathbf{x}_{t}$$. Then, we run several \u0027corrector\u0027 steps to improve our sample from $$p_{t + \\Delta t}$$, using our estimate of its score function $$\\mathbf{s}_\\theta(\\mathbf{x}_{t + \\Delta t}, t + \\Delta t)$$. We do this before moving on to the next time step.\n\nThis is similar to the discrete-time case, where we optimize the score at each noise level before moving to the next noise level.\n\n## ODE Version\n\nWe can change the SDE into an ODE without changing its marginal distributions $$p_t(\\mathbf{x}_t)$$. The formula is this:\n\n$$\nd\\mathbf{x} = \\left[f(\\mathbf{x}, t) - \\frac{1}{2} g^2(t)\\nabla_\\mathbf{x} \\log p_t(\\mathbf{x}_t) \\right] dt\n$$\n\nODE trajectories are actually smoother, and this process is invertible. We can directly convert $$\\mathbf{x}_0$$ to $$\\mathbf{x}_T$$ and convert it back. \n\nThis is a special case of a continuous time neural ODE and a normalizing flow, and allows for exact likelihood computation.\n\n## Inverse Problems\nSuppose we have a known forward process $$p(\\mathbf{y} \\mid \\mathbf{x})$$, and we would like to know $$p(\\mathbf{x} \\mid \\mathbf{y})$$.\n\nBayes\u0027 rule tells us\n\n$$\np(\\mathbf{x} \\mid \\mathbf{y}) = \\frac{p(\\mathbf{y} \\mid \\mathbf{x})p(\\mathbf{x})}{p(\\mathbf{y})}\n$$\n\nBy taking the gradient with respect to x of the log of both sides, we get a Bayes\u0027 rule for score functions:\n\n$$\n\\nabla_\\mathbf{x} \\log p(\\mathbf{x} \\mid \\mathbf{y}) = \\nabla_\\mathbf{x} \\log p(\\mathbf{y} \\mid \\mathbf{x}) + \\nabla_\\mathbf{x} \\log p(\\mathbf{x})\n$$\n\nNote that the denominator disappears since it does not depend on $$\\mathbf{x}$$. Both terms in the right side of the above expression can be found, the first is given by the known forward process, and the second is the score function of our unconditional distribution. This will let us sample from $$p(\\mathbf{x} \\mid \\mathbf{y})$$.\n\nFor instance, we can apply score-based models to the task of image inpainting. The masked image is given, as $$p(\\mathbf{y})$$, and the forward process is also given (how to mask the image).\n\n\n\u003cspan style=\"color:blue\"\u003eI am pretty sure $$\\nabla_\\mathbf{x} \\log p(\\mathbf{y} \\mid \\mathbf{x}) $$ is zero everywhere, but I suppose maximizing that will just be ensuring that the non-masked region stays the same.\n\u003c/span\u003e\n\n## Connection to DDPM\nOnce I finish my page on DDPM, you will be able to see that the ELBO objective that DDPM uses is the same as a the score-matching objective.\n\nLast Reviewed: 2/4/25\n\n{% endraw %}\n"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Wiener-Process/", "id": "Wiener Process", "label": "Wiener Process", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Wiener Process\n\n{% raw %}\n\n## Standard Wiener Process\n\n\n### Intuition\nA Wiener process (also called Brownian motion) is a continuous time process that is like a random walk. \n\nImagine a discrete-time random process where you start at position $\\mathbf{x_{t=0}} = \\mathbf{0}$. Then, your position at time $t$ is determined by\n\n$$\n\\mathbf{x}_{t} = \\mathbf{x}_{t-\\Delta t} + \\mathcal{N}(0,\\Delta t I)\n$$\n\nOr equivalently,\n\n$$\n\\mathbf{x}_{t} = \\mathbf{x}_{t-\\Delta t} + \\sqrt{\\Delta t} \\cdot \\mathcal{N}(0,I)\n$$\n\nIn other words, every time step, you change your position by a vector sampled from $\\mathcal{N}(0,\\Delta t I)$, where $\\Delta t$ is how long each time step is.\n\nA Wiener process is the continuous limit of this as $\\Delta t \\rightarrow 0$.\n\n### Definition\nWe define a set of independent random variables, or a function mapping the time $t$ to a random variable. It satisfies the property that\n\n$$\nW_0 = \\mathbf{0}\n$$\n\nAnd \n\n$$\nW_{t_2} - W_{t_1} = \\mathcal{N}(0, (t_2 - t_1)I)\n$$\n\nNote that this restriction is only possible because the variance of the sum of two independent random variables is the sum of the two variances. In other words, since $W_{t_1} \\perp W_{t_2}$ for all $t_1 \\neq t_2 $, the variance accumulates linearly over time.\n\n\nAlso, under this formulation, we have\n\n$$\nW_{t} = \\mathcal{N}(0, tI)\n$$\n\n\n\nlinearly as $t$ increases.\n\nIn other words, at every time step, we take a infinitesimally small step in a random direction proportional to a vector sampled from the standard normal:\n\n$$\ndW \\sim \\mathcal{N}(0, I dt)\n$$\n\nLast Reviewed: 2/4/25\n{% endraw %}\n"}, {"color": "#ffd900", "font": {"color": "white"}, "id": "DDPM - UDL", "label": "DDPM - UDL", "mass": 1.0, "shape": "dot", "size": 10.0, "title": ""}, {"color": "#ffd900", "font": {"color": "white"}, "id": "DDPM - Math", "label": "DDPM - Math", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#ffd900", "font": {"color": "white"}, "id": "DDPM - Reparametrization", "label": "DDPM - Reparametrization", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#ffd900", "font": {"color": "white"}, "id": "DDPM - Noise Schedules", "label": "DDPM - Noise Schedules", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Representation-Learning/", "id": "Representation Learning", "label": "Representation Learning", "mass": 1.75, "shape": "dot", "size": 13.228756555322953, "title": "# Representation Learning\n\nAbstractions of raw data (compress, conceptualize)\n\ngood representations generalize, transfer learning\n\nWant\n-retain info about input\n-discard info irrelevant to tasks\n-compress information\n\n\nwhat is the mutual information between input and output?\nwhat info gets lost when we go from x2 to xn?\ncan we compare two NNs?\n\n\n\nLast Reviewed: 10/26/2025\n"}, {"color": "#0000ff", "font": {"color": "white"}, "id": "Reconstruction-Based Learning", "label": "Reconstruction-Based Learning", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": ""}, {"color": "#0000ff", "font": {"color": "white"}, "id": "Similarity-Based Learning", "label": "Similarity-Based Learning", "mass": 1.0, "shape": "dot", "size": 10.0, "title": ""}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Contrastive-Learning/", "id": "Contrastive Learning", "label": "Contrastive Learning", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": "# Constrastive Learning\n\nconstrastive learning is a kind of classification\n\nencoders map audio, text to d-dimensional embeddings.\nencoders are feature extractors followed by MLPs.\nthese embeddings are L2-normalized to produce vectors\n\n\n\n\n\nLast Reviewed: 7/15/2025"}, {"color": "#3FFF57", "font": {"color": "white"}, "id": "Audio", "label": "Audio", "mass": 6.0, "shape": "dot", "size": 24.49489742783178, "title": ""}, {"color": {"background": "#3fff57", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\DAC/", "id": "DAC", "label": "DAC", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# DAC\nResidual blocks with dilations 1, 3, 9 slash kernel size 7, and depthwise 1x1\nThese are chained together.\n\nDownsampling blocks that double channel\n\nUpsampling blocks that halve channel\n\nSnake activations\n\nfeature matching loss, multiscale STFT discriminator, mel-reconstruction loss\n\nLast Reviewed: 1/17/25"}, {"color": {"background": "#3fff57", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Re-Bottleneck/", "id": "Re-Bottleneck", "label": "Re-Bottleneck", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Re-Bottleneck\n\n\ncodecs useful for compression, transmission, feature-extraction, latent-space generation.\n\nreconstruction objectives might not be the best for latent structure.\n\nre-bottleneck - latent space inner bottleneck used to instill user-defined structure\n\n1. enforce an ordering on latent channels\n2. align latents with semantic embeddings, use for downstream diffusion modeling\n3. equivariance - filtering operation on waveform corresponds to a specific latent space operation.\n\ncodecs are used for next-token prediction and diffusion, and supports classification, enhancement, and source separation.\n\nlatent structure needed for these tasks, but does not emerge from reconstruction loss.\n\nadapting from this means\n- doing coarse-to-fine diffusion\n- special token prediction order\n- retrain autoencoder from scratch to include these properties, task-specific tokenizers, designing models for semantic alignment - this is costlhy computationally, and require substantial architectural modifications.\n\n\n\nstructured representations help latent diffusion in other domains\n\nsemantic alignment in audio or equivariance is promising but requires retraining, what a waste of pretrained codecs.\n\n\n## Method\ntrain an inner autoencoder, with a reconstruction objective on the latent, along with a discriminator\n\navoid computation/tuning with waveform losses - only latent domain losses\n\n## experiments\n\n- monotonic ordering, coarse to fine\n- semantic alignment - alignw ith \n\nLast Reviewed: 7/16/2025"}, {"color": "#3fff57", "font": {"color": "white"}, "id": "Audio Generation", "label": "Audio Generation", "mass": 3.0000000000000004, "shape": "dot", "size": 17.320508075688775, "title": ""}, {"color": "#3fff57", "font": {"color": "white"}, "id": "Audio Diffusion", "label": "Audio Diffusion", "mass": 1.2500000000000002, "shape": "dot", "size": 11.180339887498949, "title": ""}, {"color": {"background": "#3fff57", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\DiffWave/", "id": "DiffWave", "label": "DiffWave", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# DiffWave\nLast Reviewed: 1/23/25"}, {"color": "#3fff57", "font": {"color": "white"}, "id": "Stable Audio", "label": "Stable Audio", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#3fff57", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Multi-Source-Diffusion/", "id": "Multi-Source Diffusion", "label": "Multi-Source Diffusion", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Multi-Source Diffusion Models\n\n- can learn the score of the joint probability sharing a context\n- partial generation\n- source separation\n- generation\n- step toward general audio models\n\n\n\naudio samples are sum of individual sources\nshare context\n\njoint does not factorize into marginal - sources are dependent\nbut, knowing joint implies distributions over mixtures, via marginalization.\n\n\nknowing the distribution over sources -\u003e joint is harder than joint -\u003e sources\n\n\ncompositional musical generation highly connected to source separation - need to separate out sources, then compose them together.\n\n\nsource separation models either\n\n- learn a single model for each source distribution, and condition on the mixture during inference.\n- target the conditional distribution\n\n\nThey\n- learn the joint distribution of contextual sources\n- train a multi-source diffusion model \n- source separation using dirac deltas\n\n\nto do: read beyond intro\n\n\n\n\nLast Reviewed: 10/9/25"}, {"color": {"background": "#3fff57", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Stemphonic/", "id": "Stemphonic", "label": "Stemphonic", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Stemphonic\n\nprevious models generate a fixed set of stems\n\nor, they generate one stem at a time\n\ntraining - apply shared noise latent to each group.\n\n\none-pass conditional multi-stem generation\n\n\nprevious work:\n1 - parallel architectures for stem generation, but instruments are fixed\n2 - sequential generation of stems, flexible, but one-at a time\n\n\nstem-wise activity controls\n\n\n\nLast Reviewed: 10/8/25"}, {"color": "#3fff57", "font": {"color": "white"}, "id": "Token-Based Audio Generation", "label": "Token-Based Audio Generation", "mass": 1.5, "shape": "dot", "size": 12.24744871391589, "title": ""}, {"color": "#3fff57", "font": {"color": "white"}, "id": "VampNet", "label": "VampNet", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#baee38", "font": {"color": "white"}, "id": "Next-Scale Audio Prediction", "label": "Next-Scale Audio Prediction", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#3fff57", "font": {"color": "white"}, "id": "Autoregressive", "label": "Autoregressive", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": "#3fff57", "font": {"color": "white"}, "id": "AudioLM", "label": "AudioLM", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#3fff57", "font": {"color": "white"}, "id": "MusicGen", "label": "MusicGen", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#3fff57", "font": {"color": "white"}, "id": "Speech", "label": "Speech", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": {"background": "#3fff57", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Speech-to-Speech-Models/", "id": "Speech to Speech Models", "label": "Speech to Speech Models", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Speech to Speech Models\n\n"}, {"color": {"background": "#3fff57", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\W2V-Bert/", "id": "W2V-Bert", "label": "W2V-Bert", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# W2V-Bert\n\n"}, {"color": "#3fff57", "font": {"color": "white"}, "id": "Music", "label": "Music", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": {"background": "#3fff57", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\MIDI-DDSP/", "id": "MIDI-DDSP", "label": "MIDI-DDSP", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# MIDI-DDSP\n\n\n\nChaining several systems together improves controllability\n\n\nSome people combine MIDI with traditional DSP, but this is hard to generate realistic timbre.\n\nVision has systems optimized for both realism and control\n\nconcatentative systems have realism, but manual stitching limits control and expression.\n\n\n\nanalysis:\naudio -\u003e ddsp parameters -\u003e performance -\u003e notes\n\nsynthesis:\nnotes -\u003e performance -\u003e ddsp parameters -\u003e audio\n\ncomposer usually writes notes, performer interprets, then instrument converts to sound.\nNotes, performance, synthesis.\n\n\nthree modules:\nddsp synthesizer, synthesis param generator, and expression generator.\n\nthree fixed feature extractions:\nddsp synthesis, feature extraction, note detection\n\n\nrequires pitch detection/note detection, so limited to single monophonic instruments.\n\ntrain on \u003e 12 instruments with a single model, conditional generation on instruments for every stage.\n\n\n### Contributions\n- extraction of note expression attributes\n- control through manipulating different parts of the hierarchy - expert musicians can adjust parameters.\n- reconstruction \n- prediction of synthesis params from notewise expressions\n- realsitic note synthesis that is better than neural and professional concatentative approaches, according to user studies\n- automatic music generation, generate Coconet then generate params\n\n\n## Method\n- supervision happens at each stage (not end to end)\n- DDSP inference predicts synthesis parameters, trained using reconstruction loss\n- Synthesis generator predicts synthesis params from notes and their expressions, trained via reconstruction/adversarial loss\n- expression generator predictts note expressions given a sequence, trained with teacher forcing.\n\n\n## Skipped\nRelated work\n\n\n\n\n\n\nLast Reviewed 10/8/25"}, {"color": {"background": "#3fff57", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Universal-Audio-Synthesizer-Control-with-Normalizing-Flows/", "id": "Universal Audio Synthesizer Control with Normalizing Flows", "label": "Universal Audio Synthesizer Control with Normalizing Flows", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Universal Audio Synthesizer Control with Normalizing Flows\n\n\nMap latent space to parameter space\n\n\n- Automatic Parameter inference\n- macro-control learning (learning latent dims for control)\n- audio-based preset exploration\n\n\nDisentangling Flows - bidirectional mapping between latent spaces, while steering org. of some latent dims.\ndisentangles axes of audio variations\n\n\nVAE learns a latent space on audio\n\nnormalizing flow maps that latent space to synth parameters\n\nLast Reviewed: 6/27/25"}, {"color": "#79443B", "font": {"color": "white"}, "id": "Vision", "label": "Vision", "mass": 4.500000000000001, "shape": "dot", "size": 21.213203435596427, "title": ""}, {"color": {"background": "#79443b", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\PixelVAE/", "id": "PixelVAE", "label": "PixelVAE", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# PixelVAE\n## Single-Level\n- Imagine a VAE\n- Now imagine a VAE + PixelCNN, where the Pixel CNN operates in the image space, and is conditioned on $$z$$.\n- i.e., $$p(x_i \\mid x_{i-1}, \\ldots , x_1, z)$$\n- to condition on $$z$$, we pass it through upsampling layers so that it is the same dimension as the image.\n\n## Hierarchical Version\n- Each stage takes an upsampled latent variable map\n- Uses PixelCNN to generate more latent variables (for the next stage)\n- The PixelCNN outputs the mean and variances of these latent variables, assumed Gaussian.\n- Latent variables for the next stage are upsampled again\n- The last layer outputs pixel values according to softmax\n\nLast Reviewed 2/6/25    "}, {"color": {"background": "#bb8c35", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\VAR/", "id": "VAR", "label": "VAR", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# VAR\nWorks in VQ-VAE Latent Space\n\nEncoding:\nTake 64x64 latent, squash to 1x1, quantize.\nlookup, stretch to K x K, compute residual\nSquash residual to 4x4, quantize residual\nlookup, stretch to K x K, compute residual\n...\nvery similar to RVQ but with \u0027squashing\u0027\n\nAutoregressive \u0027next-scale\u0027 predictions,\npredict first scale, then all of next scale in parallel, etc.\n\nraster scan order bad - disrupts locality, makes infilling hard due to bidirectional correlation, inefficient.\n\nLast Reviewed: 1/17/25"}, {"color": {"background": "#762b9a", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Masked-Image-Modeling/", "id": "Masked Image Modeling", "label": "Masked Image Modeling", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Masked Image Modeling\n\n\n\nContext Encoders - Feature Learning by Inpainting, 2016, Berkeley\n\n\n2022 - MAE - masked autoencoder\n\nPatches = one visual token\n\nrandom masking\n\nuse ViT to encode visible patches into latents.\nput masked tokens back, after encoding into latents.\nthen,  decoder predicts unknown\n\n\nneed to mask a very large portion of patches to be useful.\n\ninitial, MAE just interpolates colors, eventually recovers image.\n\n\ngood for transfer learning - 75% masking ratio results it best transfer accuracy\nin language it\u0027s 15%\n\ninformation in langauge is less redundant (due to human beings)\n\n\n\nNeurIPS 2022 - MAEs are spatiotemporal learners.\n\n\ndecoder is lighteight, \u003c10% of the computation per token vs the encoder, full token sets are only processed by decoder\n\nMAE - independent\n\n\nApplications:\n- robotics - multi-view, medical images, 3D geometry, graphs, audio (spectrogram). \n\nLast Reviewed: 10/25/2025"}, {"color": "#79443b", "font": {"color": "white"}, "id": "Segmentation", "label": "Segmentation", "mass": 1.2500000000000002, "shape": "dot", "size": 11.180339887498949, "title": ""}, {"color": {"background": "#79443b", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\ODISE/", "id": "ODISE", "label": "ODISE", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# ODISE\nOpen-vocabulary Diffusion-based panoptic segementation\n\nk-means clustering of diffusion model\u0027s internal representation\n\nUse both CLIP and SD\n\n\nPrevious methods just use CLIP, but this not good for scene-level understanding, bad spatial relations between objects.\n\ndiffusion models compute cross attention between text embedding and interal visual representation\n\nFig 1, simply clustering diffusion model\u0027s internal features does some segmentation\n\ndiffusion model -\u003e mask generator of all possible concepts, trained with annotated masks, categorizes each mask into many categories by associating with text embeddings.\n\n\n## Training\n- sample a noisy image\n- feed it into the unit, with the captions\n- the diffusion model\u0027s visual representation for x depends on its caption\n- use implicit captioner when the caption is not available.\n    - instead of using a network to generate captions, get an implicit text embedding, using CLIP, MLP to implicit text embedding.\n    - only finetune the MLP\n\n\n### Mask generator\n- mask generator outputs N class-agnositc binary masks, can be any panoptic segementation network\n- pixel wise BCE loss, along with ground-truth masks\n- mask GT category label: if we have a lot of categories in the training set, encode all catgoeis with the frozen text encoder, then use a classification loss betwen all the training categories.\n- the probability is Softmax( net_out dot Text_encoder(C_train)) (this is probably what allows it to generalize).\n\n\n### Image caption supervision\n- extract nouns from each caption, treat them as grounding category labels\n- compute the simliarty between each image caption pair, with a grounding loss encouraging each noun to beg rounded by one  or a few masked regions in the image.\n\n\n## Grounding loss\n- the loss is overall image-word similarity, you take the probability of mask embedding features z_i with each word, times (z_i, T(w_k)).\n- so it\u0027s two similarities, both against all the words  in the caption.\n- avoids penalizing regions that are not grounded by any word\n\n\nstill have to keep reading\n\n\nLast Reviewed: 10/28/2025"}, {"color": "#79443b", "font": {"color": "white"}, "id": "RCNN", "label": "RCNN", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": "#79443b", "font": {"color": "white"}, "id": "FastRCNN", "label": "FastRCNN", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#79443b", "font": {"color": "white"}, "id": "Mask-RCNN", "label": "Mask-RCNN", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#79443b", "font": {"color": "white"}, "id": "3D Reconstruction", "label": "3D Reconstruction", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": {"background": "#79443b", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Gaussian-Splatting/", "id": "Gaussian Splatting", "label": "Gaussian Splatting", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Gaussian Splatting\nSuper Fast Rendering - realtime display rates at 1080p\nHigher quality than Mip-Nerf\nAvoids unnecessary computation in empty space\n\nNerf and voxel-based 3D representations require stochastic sampling for rendering - computationally expensive, result in noise\n\nuse sparse point clouds obtained from SFM - no need for MVS\n1-5 million gaussians per scene\n\nRendering:\n-respect visibility ordering\n-sorting\n-tile-based rasterization\n\n\nopacity-based rendering --- \n---the \u0027opacity\u0027 (light absorption) assigned to a point\ndepends on the distance from the previously sampled point, AND the density at the point\nbasically, you are assuming the \u0027point\u0027 is a block that absorbs light\n\nThese are continuous representations, (not voxels)\nworks with randomly initalized gaussians\ncan project gaussians to 2D and perform alpha blending\n\nParametrize each gaussian with:\n    opacity, anisotropic covariance, spherical harmonics\n    \nadaptive density control - add or remove gaussians during training\nLast Reviewed: 1/17/25\n"}, {"color": "#79443b", "font": {"color": "white"}, "id": "NeRF", "label": "NeRF", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#79443b", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\classes\\Advances-In-Computer-Vision/", "id": "Advances In Computer Vision", "label": "Advances In Computer Vision", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": "# Advances in Computer Vision\n\n## Image Formation\nWeek 1, Thursday\n### Pinhole Cameras\n-Why don\u0027t you see an image when you hold up a piece of paper?\n    - Each point in space reflects in all directions, which get spread out across the paper\n    - Conversely, each point on the paper gets light from multiple points in the scene.\n- Pinhole camera light in such that each point on the image corresponds to a point from an object\n- Get an image on the other side\n- Tradeoff between brightness and sharpness\n## Derivation\n- Establish 3D camera system.\n- center = pinhole cetner.\n- z = distance from pinhole center\n- x and y are up and side\n- right handed in real-world space\n- f = distance from pinhole to image plance\n\nTo do: better notes for lectures 1 (introduction), 2 (image formation), and 3 (fourier transform).\n"}, {"color": {"background": "#79443b", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\classes\\Image-Formation/", "id": "Image Formation", "label": "Image Formation", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Image Formation\n\nWeek 1, Thursday\n## Pinhole Cameras\n-Why don\u0027t you see an image when you hold up a piece of paper?\n    - Each point in space reflects in all directions, which get spread out across the paper\n    - Conversely, each point on the paper gets light from multiple points in the scene.\n- Pinhole camera light in such that each point on the image corresponds to a point from an object\n- Get an image on the other side\n- Tradeoff between brightness and sharpness\n### Derivation\n![alt text](image-1.png)\n- Establish 3D camera system.\n- center = pinhole cetner.\n- z = distance from pinhole center\n- x and y are up and side\n- right handed in real-world space\n- f = distance from pinhole to image plane\n\nWe can think of the unflipped image instead:\n\n![alt text](image.png)\nCan use similar triangles:\n![](image-2.png)\n\nNote that **depth** is $$Z$$, not the ray length (the hypotenuse).\n\n$$\n\\mathbf{x} = (x,y) = (X,Y) \\cdot \\frac{f}{Z}\n$$\n\nWhere $$(x,y)$$ are the image/pixel coordinates, and $$(X,Y)$$ are the world coordinates.\n\n### Properties Preserved\n- Lines that are straight in 3D are straight in 2D\n- Incidences - lines that meet in 3D will meet in 2D\n- Angles and lengths are distorted. Right angled tables are different\n- Parallel lines will not be there, there will be vanishing points (e.g.) railroad tracks.\n\n#### Example - a line.\n$$\nX(t) = X_0 + at\n$$\n$$\nY(t) = Y_0 + bt\n$$\n$$\nZ(t) = Z_0 + ct\n$$\nAfter computing $x$ and $y$ in pixel space and taking $$t \\rightarrow \\infty$$, we see that we get points in image space as \n\n$$\n\\frac{fa}{c}\n$$\n\n$$\n\\frac{fb}{c}\n$$\nWhich does not depend on $X_0$, or the \u0027initial point\u0027 on the line. All parallel lines intersect at the vanishing point, it does not matter the \u0027offset\u0027 of the line, just the \u0027slope\u0027\n\nUnless the lines are parallel to the image plane, or $$c=0$$, parallel lines will remain parallel. Lines that are parallel to the image plane will either have a single intersection or be parallel.\n\n![ ](image-3.png)\n\nDiffusion models also don\u0027t always have perspective geometry.\n\n### Homogeneous Coordinates\nThe $(X,Y,Z)$ to $(x,y)$ mapping is not linear.\n\nLets try having\n\n![alt text](image-4.png)\nAnd, all points are equivalent up to scalar multiplications.\n\n![alt text](image-5.png)\n\nNow, let\u0027s look for a transformation from 3D Homogenous coordinates to 2D image pixel coordinates:\n![alt text](image-7.png)\n\nIntrinsics Matrix:\n![alt text](image-8.png)\n\nWe should also have translations in x-y sspace \n\n![](image-9.png)"}, {"color": {"background": "#79443b", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\classes\\Linear-Image-Processing/", "id": "Linear Image Processing", "label": "Linear Image Processing", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Linear Image Processing\n\nAn image is a function maps a ray to a color, or a 2D coordinate to a color. We can discretize it, and represent the image as a vector.\n\nVector is sampled from a function.\n\n\nSpace of images is an inner product space.\n\n"}, {"color": "#00FF00", "font": {"color": "white"}, "id": "Language", "label": "Language", "mass": 3.75, "shape": "dot", "size": 19.364916731037084, "title": ""}, {"color": {"background": "#00ff00", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\classes\\Language-Modeling-from-Scratch/", "id": "Language Modeling from Scratch", "label": "Language Modeling from Scratch", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Language Modeling from Scratch\n\nBPE\nFew-Shot/Zero Shot Generalization\nScaling Laws - parameters, data, training time, result in linear log-log curves with loss\n\nLast Reviewed: 6/1/24\n"}, {"color": {"background": "#7d93a6", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\LLMs/", "id": "LLMs", "label": "LLMs", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": "# LLMs\n- Language\n- knowledge\n- retreival and tool use\n- in-context learning (learn new skills quickly)\n- alignment\n- capability \n- safety\n- reasoning\n\nreinforcement learning, give rewards\n- how does it know the reason it succeeded?\n- does random stuff if you don\u0027t pretrain\n\n\nLanguage = special, natural domain for studying AI, this is the modality of the frontier.\n\nsupervised learning\n- question answer pairs\nSQuAD - QA dataset 2016.\n- HotPotQA - pairs of wikipedia pages \u002718.\n- MS Marco - 1,000,000 Bing Queries.\n- MS Marco\n- Natural Questions\n\nScale.\n- self supervised learning (LLMs in machine translations 2007, ngrams)\n- 1-10T trillion params GPT-5.\n\nPeople are very conservative due to cost of training\n\n\npost-training\n- teachin LLMs to be instruction following assistants, effective at math, coding, using tools.\n\n\nNow there are many stages:\n\npretraining - base data mixture, more code data-mixture, even more code + synthetic data mixture\n- training on code is hypothesized to improve reasoning in general.\n\nmidtraining - context expansion, reasoning heavy\n\npost-training: SFT, DPO / RL\n\n\n\nopen research\nopen models\nopen source (not open weights) like llama 1-4\nopen development\n\n\ntokenization - word-based - misspellings, will encounter things you haven\u0027t seen beofre.\n\n\ncharacters - lots of sequences. representation for \"c\" not very meaningful.\n\n\nsubword tokens.\n\n\n## Pre-training\nDon\u0027t use BERT\n1 - only learning about the masked word, inefficient\n2 - if you want to make it useful, need to generate text. instead of classifying something, just give the word of the class.\n3 - encode all tokens during scratch\n\nweb knowledge - factual, syntax + coreference, sentiment, math, prompting.\n\ndesign structures into your weights\n\nstart with some links, hop to adjacent links - most of the web is gibberish\noversample wikipedia, arXiv, GitHub, Reddit, StackExchange. People ask questions on reddit, multiple answers, upvotes, links - pages linked to from reddit posts or wikipedia.\n\ncan ask an LLM\n\n## Long Contexts\npretraining = 4000 word context window\nmidtraining - longer, million/half a million tokens\n\n100 H100s = small LLMs\n\nBabysit training runs, look for spikes, track evals.\n\n## Tweaks to transformer\n- lots of small tweaks - mixture of experts, marin 8B\n\n\n## how to budget compute\n- 50 models? pick best?\n- tune hyperparams at small scale/ No\n- answer: scaling laws.\n\nLast Reviewed: 10/26/2025\n"}, {"color": {"background": "#7d93a6", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\BERT/", "id": "BERT", "label": "BERT", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": "# BERT\n\n\nMask tokens\n\ntries to predict masked tokens.\n\nbefore chatGPT, most popular representation learning  method, for two years\n\n(less efficient than next-token prediction)\n\n\n\nLast Reviewed: 10/25/2025\n"}, {"color": {"background": "#7d93a6", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\ColBERT/", "id": "ColBERT", "label": "ColBERT", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "cross attention vs dual independent encoders - cross attention will fit prefectly\n\ndual encoder is as good on training set, but worse on the test set.\n\nblunt representations = blunt updates.\nif you push it away from negative documents, it will be closer to other negative documents.\n\n## Late interaction\ncross attention not scalable, but has interaction\n- \nsingle-vector dual-encoder very scalable, low. dot produit is a bottleneck\n\nwe still have independent encodings\nbut we have a sequence of small vectors (4 bytes each)\nthen do MAX similarity between small vectors\n\nsublinear search, can do ANN but maximums can be shared across documents\n\nmax sim between each element of query and \n\nworks best as found  in Althammer et al. 2023.\n\nComparison to dot-product approach:\n\nmore data, improves fast. \n\nCompositional updates - gradients flows through things that match (decommposed). \n\n\n## Key idea\nweight Updates actually HELP other documents, instead of pushing it closer to other negative examples, since some tokens may be shared with other examples\n\n![ColBert Comparison](image-2.png)\n\nLast Reviewed: 10/26/2025\n"}, {"color": "#00ff00", "font": {"color": "white"}, "id": "Tokenization", "label": "Tokenization", "mass": 1.0, "shape": "dot", "size": 10.0, "title": ""}, {"color": {"background": "#00ff00", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Byte-Pair-Encoding/", "id": "Byte-Pair Encoding", "label": "Byte-Pair Encoding", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Byte Pair Encoding\n\nstart with vocabulary of bytes\n\nmerge two tokens and add it to the vocabulary\n\nbye-level BPE\n\n\n\n\n\n\n\nLast Reviewed: 10/28/2025"}, {"color": {"background": "#00ff00", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\WordPiece/", "id": "WordPiece", "label": "WordPiece", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# WordPiece\n\nSplits words by iterativevly matching longest prefix in vocabulary\n\n\n\nLast Reviewed: 10/28/2025"}, {"color": "#00ff00", "font": {"color": "white"}, "id": "SentencePiece", "label": "SentencePiece", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#2dff3b", "font": {"color": "white"}, "id": "Audio-Language Models", "label": "Audio-Language Models", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": "#2dff3b", "font": {"color": "white"}, "id": "CLAP", "label": "CLAP", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#2dff3b", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\FLAM/", "id": "FLAM", "label": "FLAM", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# FLAM: Frame-Wise Language-Audio Modeling\n\n\n\n\nFrame-wise objective, adjustment to remove spurious correlations like event dependencies and label imbalances.\n\n## Previous work\ngood at retrieval, understanding, and text-conditioned generation.\ninstance level alignments between audio and text (e.g. CLAP)\ncannot find boundaries of acoustic events - bad for audio content search and event detection\nframe-level annotations are rare, however\nSED datasets have a limited vocab, remain small in size, due to human annotation effort.\n\noverall text-data volume limits self-supervised approaches\n\n## Contributions\n- Framelevel open-vocabulary SED\n- bias correction term, unbiased event classifier\n- scalable (1M) data augmentation pipeline, with precise event boundaries\n- open-set, closed-set SED, better than prior-self supervised approaches\n- good retrieval, zero-shot classification\n\n## Method\n\nframe-level embeddings, as well as sample-level embedding. frame-level embeddings match with text embeddings.\n- frame-level constrastive objective\n- logit adjustment techniques to remove spurious correlations\n- memory-efficient training strategy\n- synthetic data using 10-second audio mixtures - 1 million sample dataset\n\n\n\n\n### Dataset\n- diverse audio events\n- LLM generated captions\n- simulation\n\nimproves open-vocabulary localization\nmaintains retrieval/downstream task performance\n\n\n\n### SED\neach frame can contain a variable number of events, including none\nopen vocabulary - unlimited number of prompts, probabilities for each frame and event\n- classier takes audio and text embedding, and detects whether event occurs.\n- Note to self: this is actually kind of like linear classification, where the weights of the last layer are the text embedding.\n\n\n### Current ALMs\n- temporal representations can be averaged, from the second-to-last layer of contrastive ALMs\n\n### Efficiency\n- can precompute audio embeddings, and match it up to different text queries\n- can be built on current ALMs\n\n### Logit Adjustment\n- some classses occur more often than others, some events are longer than others\n- most frame/text pairs are super negative.\n- thus, there is a text-related logit bias applied to the pre-sigmoid dot product.\n- dependencies are bad - what if you hear thunder, but then classifies it as rain, since rain is longer in the dataset.\n\n\n## Experiments\n\n### Sound event detection - graph\n- dataset: a single example of audio mixture\n- metrics: frame-wise prediction accuracy\n- baselines: \n\n### Sound event detection performance\n- dataset: synthetic open-vocabulary SED. 6 datasets\n- metrics: AUROC, PSDS\n- baselines: MGA-CLAP, and FLAM but global.\n\n### retrieval\n- text to audio, audio to text.\n- 3 datasets\n- baselines: FLAM global and MGA-CLAP (retrained on same dataset). also compared to LAION CLAP, CompA, MGA-CLAP, which were trained on different datasets.\n\n## zero-shot classification\n- baselines: MGA-CLAP, LAION,\n\n### Ablations\n- removing per-text scale, per-text bias, and both\n\n\nLast Reviewed: 7/16/2025"}, {"color": "#75a136", "font": {"color": "white"}, "id": "Vision-Language Models", "label": "Vision-Language Models", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": {"background": "#7e63a0", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\CLIP/", "id": "CLIP", "label": "CLIP", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# CLIP\nCLIP loss:\n - numerator = exp(audio dot text)\n - denominator = sum(exp(audio dot text), for all pairs)\n - take the log of this, and average across batch\n    - when averaging across batch, consider examples where you fix the current example\u0027s text and vary the audio,\n    and examples where you fix the current example\u0027s audio compare against different text\n - typically there is a learnable logit scale, which multiplies the dot products. this is e^(alpha\u0027) where alpha\u0027 is the learnable parameter.\n\n\nLast Reviewed: 7/15/2025\n\n "}, {"color": {"background": "#7e63a0", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\SigLIP/", "id": "SigLIP", "label": "SigLIP", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# SIGLIP\n\ninstead of a InfoNCE loss, we do a binary cross entropy loss on positive or negative pairs.\n\nthe probability is computed as a dot product between the embeddings, times a weight plus a bias\n\nbias initialization is important, since there is imbalance between positive and negative examples."}, {"color": "#212129", "font": {"color": "white"}, "id": "Computer Science", "label": "Computer Science", "mass": 5.000000000000001, "shape": "dot", "size": 22.360679774997898, "title": ""}, {"color": {"background": "#212129", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Linked-Lists/", "id": "Linked Lists", "label": "Linked Lists", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Linked Lists\n- How to do forward traversal\n- set a dummy head\n- set curr to be the dummy head\n\nwhile loop:\n    - create a new node\n    - modify the curr\u0027s next pointer to point to the new node\n    - have curr point to the new node\n\nThat way, we are basically\n1. Creating a new element to add to the list\n2. adding it to the list\n3. moving our curr pointer to the end of the list, pointing at that element.\n\nLastly, return dummy head.next.\n\nWhenever we create a node, we add it. It is never the case that a node is created, but we don\u0027t add it. Thus, if the while loop condition is no longer satisfied, there are no more nodes to add."}, {"color": "#212129", "font": {"color": "white"}, "id": "Minhash", "label": "Minhash", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#212129", "font": {"color": "white"}, "id": "Software", "label": "Software", "mass": 3.0000000000000004, "shape": "dot", "size": 17.320508075688775, "title": ""}, {"color": {"background": "#212129", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\software\\Git/", "id": "Git", "label": "Git", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Git \n\nSource:\n\nhttps://tom.preston-werner.com/2009/05/19/the-git-parable.html\n\n- To summarize the first few sections, imagine saving snapshots of your code in branches. Branches cannot have branches.\n- each branch is identified by the latest snapshot on that branch. This makes it easy to trace back the lineage of snapshots on the branch.\n- the name of each branch is associated with the latest snapshot on each branch.\n- We can also have pointers to snapshots that refer to a single snapsnot, without moving. These are tags.\n\n\n- Snapshots are named by the commit message, which actually includes the author name, email, date, and parent snapshot. This information is hashed.\n\n- snapshots can be created and moved around between computers without losing their identity or location in the tree - they are still identified by a SHA1 hash. They can be private, too.\n\n\n## Merging\n- merging two snapshots into one - you have two parents now.\n- the merge snapshot only contains the changes necessary to merge.\n- you fetch all of your collaborator\u0027s snapshots, and dthey fetch yours.\n\n## Rewriting history\n- you can do this too, make new snapshot and point it to an old one, then have the branch be associated with your new snapshot. this cuts off some snapshots, but you can delete them.\n\n## Staging\n- staging is basically saying which files we are going to put in a snapshot.\n\n## Diffs\n- you can also see diffs between working directory and staged, and staged and snapshot.\n\n## Deduping\n- long explanation\n\nLast Reviewed: 6/20/25"}, {"color": {"background": "#212129", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\software\\Filesystems/", "id": "Filesystems", "label": "Filesystems", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# File Systems\n\n- nfs is faster than afs\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#212129", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\software\\Slurm/", "id": "Slurm", "label": "Slurm", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Slurm\n\n- Partition is broader than a node.\n- Account determines who gets charged\n\nLast Reviewed: 4/30/25\n"}, {"color": {"background": "#212129", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\software\\Python/", "id": "Python", "label": "Python", "mass": 1.75, "shape": "dot", "size": 13.228756555322953, "title": "# Python \n\n## Classes\n\nStatic Methods can be called without initializing the class. It does not have access to the instance or the class. It is basically a regular python function that is under the class for organizational purpose.\n\nClass methods have access to the class, but not the instance. They can instantiate the class.\n\nClass attributes are like constants, but defined under the class\n\n## Important Note - pass by reference\n\nIf you have\n\ndef foo(x: torch.Tensor):\n    x = x + 1\n\nEven though tensors are pass-by-reference, x is not modified outside of the scope of this function. It is ilik\n\nOn the other hand, if you did\n\ndef foo(x: torch.Tensor):\n    x.add_(1)\n\nThe change is seen outside the function\n\n\n## kwargs\nIf extra keyword arguments are passed, they go into the kwargs dict. This can be passed again to other inner functions.\n\nLast Reviewed: 4/26/25    \n\n"}, {"color": "#212129", "font": {"color": "white"}, "id": "Vector Operations", "label": "Vector Operations", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": ""}, {"color": {"background": "#212129", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\software\\Einsum/", "id": "Einsum", "label": "Einsum", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "\n## Einsum\n\t-https://eli.thegreenplace.net/2025/understanding-numpys-einsum/\n\t-matrix multiplication: np.einsum(\"ij,jk -\u003e ik\", A, B)\n\t-comma separate list of inputs in the strings will match the operands in terms of the number of them, and the number of dimensions.\n\t-the number of letters in each label in the string must match the number of dimensions in the input\n\t-whenever a letter is repeated in the input, these must have the same length\n\t-a letter that is repeated in the input, and absent in the output, is finna be summed\n\t-any input label must be repeated twice then dropped, or repeated once then listed in the output.\n\t-we can reorder operands, as long as we reorder labels, and we will get the same result.\n\t-we can transpose dims\n\t-for instance, multi-head self attention key projection: np.einsum(\"bsd,hdk-\u003ebhsk\", x, w_k)\n\t-can contract multiple dims, not just one\n\t-can do A @ B @ C: np.einsum(\"ij,jk,km-\u003eim, A,B,C)\n\t-implementation\n\t\t-read shapes\n\t\t-initialize output to zero-array of the correct shape.\n\t\t-loop over all letters in the expression\n\t\t-the expression in the innermost loop indexes into all the inputs and outputs properly, and does\n\t\t-output[...] += inputs * inputs ..... This is a scalar multiplication\n\n\t- can also use broadcasting\n\n\n\tnp.einsum(i,ij-\u003ej, dL_dout, w) is like doing \n\n\tsum_i(dL_dout_i, w_ij). We can imagine taking the labels, and using them as subscripts on the rest of the arguments.\n\n\n\tdL / dx_j = sum_i [(dL / dout_i) * w_ij]\n\n\tIn Einstein summation notation, this is\n\tdL / dx_j = (dL / dout_i) * w_ij\n\tAs the sum is over i implicitly.\n\n\tConverting this to einsum means taking the subscripts, and moving them\n\tInto the first argument:\n\tnp.einsum(i,ij-\u003ej, (dL / dout), w)\n\n\n\n"}, {"color": {"background": "#212129", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\software\\Hydra/", "id": "Hydra", "label": "Hydra", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Hydra\n\n\nCan do sweeps"}, {"color": "#35208d", "font": {"color": "white"}, "id": "ML Systems", "label": "ML Systems", "mass": 2.25, "shape": "dot", "size": 15.0, "title": ""}, {"color": {"background": "#35208d", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Mixed-Precision/", "id": "Mixed Precision", "label": "Mixed Precision", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Mixed Precision\n- Weights are float32, but during forward pass, we use fp16 for everything.\n- bf16 is like fp16, but has better dynamic range\n- 1.5x - 3x speedup (for both, in my observation)\n- You can increase batch size as well.\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#35208d", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Large-Scale-Deep-Learning/", "id": "Large Scale Deep Learning", "label": "Large Scale Deep Learning", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": "# Large Scale Deep Learning\n\n- Distributed Storage Systems, network communications\n- Use rotational drives instead of NVMe cheaper\n- node-to-node communications become an issue.\n- Storage is on distributed systems, this is only reachable over network.\n- Weight Sync is done using pytorch\n- Need Fast I/O - this is the hardest part.\n- Read from Disk -\u003e Memory -\u003e Network card -\u003e compute node network card -\u003e memory -\u003e GPU\n- Some parts are scalable, other\u0027s aren\u0027t. Can\u0027t just double memory bus\u0027s bandwidth.\n- optimal = 100% utilization of most expensive part (GPU)\n- GPUs consume 200-1000 MB per second.\n- nvidia-smi shows gpu utilization. \n- smaller datasets can fit on NVMe drive, which transfers at 3 GB/second, easily keeping 4 desktop GPUs busy.\n- Example: 100 TB, 16 GPUs, 200 MB/s/GPU\n    - NVMe:\n        - $35,000 alone for storage\n        - 300 GB/s\n        - cant fit in one machine\n    - Rotational\n        - 8 TB, 16, $320\n        - 3.2 GB/s bandwidth\n        - can fit into 1-2 machines\n## Key Principles\n- Sequential I/O\n- Pipelining\n- Sharding\n\n### Sequential IO\n- PyTorch, file-based - access samples in random order.\n    - random access requires moving the disk head, loading, moving again.\n    - 20 MB per second (on rotational)\n- Sequential IO\n    - Make a .tar archive of all the examples in the dataset\n    - reads are sequential.\n    - IterableDatasets\n    - 200 MB/s read speeds (on rotational)\n    - in-memory shuffle buffer:\n        - while True:\n            - sample = next(dataset)\n            - index = randint(0, len(buffer)-1)\n             - sample, buffer[index] = buffer[index], sample\n        - basically, the buffer does all the shuffling for us. We read examples sequentially into a buffer,\n        and then randomly sample from that buffer. Each time we feed a sample from the buffer to the model,\n        its spot in the buffer getsw replaced by the next thing on disk. All reads from disk are sequential. \n        - better for network storage\n    - TFRecord/tf.Example, Google GFS, Hadoop, FORTRAN\n### Pipelining\n- make one request, many examples flow\n- possible with sequential storage.\n- Caching? Doesn\u0027t help in deep learning, since we iterate through the entire epoch\n\n### Sharding\n- 100 TB file, read sequentially. 200 MB/s IO\n- All reading is sequential.\n- Sharding - 100 TB dataset, split it up into 10,000 shards of 10 TB each.\n- access shards randomly - read shards in random order, read each shard sequentially with in memory buffer for shuffling.\n![alt text](sharding.png)\n\n### Three Tiers\n- Dataservers send data to CPU nodes, which do preprocessing. \nThese are then sent to the GPU.\n- RDMA based transfers from CPU to GPU. stops PCI bus as bottleneck.\n- combine the middle tier into the storage system (AIStore, NVIDIA).\n- HDFS can be difficult to configure, it was designed for big data.\n\nLast Reviewed: 5/1/25"}, {"color": {"background": "#35208d", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\software\\Webdataset/", "id": "Webdataset", "label": "Webdataset", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Webdataset\n\n- .tar files store things in continuous chunks\n- HDP for distributed storage protocol - use any webserver.\n- works for local files.\n- implements IterableDataset\n- all datasets are POSIX tar archives\n    - create using tar, or the tar writer class in the library\n    - all I/O is sequential.\n- refer to webdatasets by using a filepattern.\n- consists of many shards.\n- url = ...\n- dataset = wds.Dataset(url) \n- you might want to do decompression\n- iterating:\n    - key is the file extension\n    - value = binary content\n    - __key__ is the sample itself, common basename that is for the sample.\n    - decodings are standard. use .decode() to turn into dictionary of all files comprised in a sample, which can be turned into a tuple, then apply transforms.\n    - Then you can shuffle.\n    dataset = (\n        wds.Dataset(url, length=10000)\n        .decode(\"pil\")\n        .to_tuple(\"ppm\", \"cls\")\n        .shuffle(10000)\n    )\n\n## Creating a Webdataset:\n- .tar file\n- files with a common basename make up a sample.\n- basename = same samples\n- speaker1/sample1.txt\n- speaker1/sample1.wav  \n- can just make a tar archive.\n- basename is ALL extensions removeL sample.input.png -\u003e sample\n    - the key is \u0027sample\u0027, the dictionary entries are input.png/ \n- use tar\n- for labels can look up metadata.\n\n## avoiding extra storage\n- instead of storing a ton of files,\n- create a recipe then use \"tarp create\"\n- filename/source table. \n\n\n## ShardWriter\n- ensures each file is less than a certain size and has less than a given number of samples.\n\n\n## More practical stuff\n- Use resample = True, which will mean that if a worker runs out of data on their shard, they\nsimply find (\"sample\") a new shard\n- batch in the webdataset\n- see fmdiffae code\nLast Reviewed: 5/1/25"}, {"color": {"background": "#2f2259", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\software\\Wandb/", "id": "Wandb", "label": "Wandb", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Wandb\n"}, {"color": {"background": "#2f2259", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\software\\PyTorch/", "id": "PyTorch", "label": "PyTorch", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": "# PyTorch\nDatasets:\n---need __len__ and __get_item__\n\nDataloaders\n---collate_fn defines how the different data examples should be turned into a batch\n\nBackwards:\n---fills \"grad\" field of every tensor that requires it\n\nZero_grad\n---turns \"grad\" field of every tensor that requires it to 0\n\noptimizer.step()\n---the optimizer has a bunch of parameters stored in it, and it looks at the gradient of all the parameters\nand then does a backward step\n\n\nUse register_buffer to add a desired tensor to the model, so it gets moved to the right device.\npersistent=False will make it not part of the state_dict.\n\n## Memory\n- Pre-allocating tensors saves memory instead of appending them to a list then concatenating. This is because torch.cat requires a sudden allocation of a lot of contiguous memory, AND lots of appending, while pre-allocating just requires the former.\n\n## Learning Rate Schedulers\nLearning rate schedulers are sometimes recursive, like cosineLR.\n\n## In Place Operations\n- Be careful with in-place operations, for instance\nx = x + relu(x, inplace=True)\n- Will zero out the negative parts of both terms, since that happens before addition.\n\n## Useful Operations\n- torch.split splits tensors to the specified size.\n- torch.chunk splits tensors into a desired number of chunks\n- unbid removes a tensor dimension and returns a tuple of slices\n- To generate a boolean mask for an operation that varies based on batches, we can create a tensor of scores then use \u003e= and \u003c= to unmask certain elements.\n- torch.expand - you can actually expand to a larger number of dimensions, with new dimensions being added to the front.\n\nFor instance, you can do:\n    A = torch.arange(80).reshape(2, 2, 2, 10, 1, 1)\n\n    A = A.expand(69, 49, 27, -1, -1, -1, -1, -1, -1)\n\nBE CAREFUL - if you overwrite part of the expanded vector, you overwrite everything it was expanded to.\n\n\n## No Grad\n- Inference mode is faster, but can\u0027t mutate tensors\n- can also use with torch.grad_enabled()\n\n\n## Datasets\n- Map dataset load everything\n- Iterables define a way to iterate through the dataset.\n- convention is return a tuple of things per batch\n\n## Dataloaders\n- Pin memory only allocates when the dataloader is called, apparently\n- persistent workers keeps dataloader workers around, use for train, not valid\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#2f2259", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Autograd/", "id": "Autograd", "label": "Autograd", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Autograd\n\nAutograd defines gradients for every point, although these may be just left or right gradients.\n\n\nLast Reviewed: 10/6/25"}, {"color": {"background": "#2f2259", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\software\\Lightning/", "id": "Lightning", "label": "Lightning", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Lightning\n\n## 1. Lightning Module\n- No need to loop over epochs/dataset, eval/train, enabling/disabling gradients\n\n## 2. DataModule\n- Hook\n\n## 3. Trainer\n- Gradient Clipping\n- DDP\n- min_epochs = minimum number of epochs (default 1)\n- max_epochs = 1000 (default 1000)\n- min_steps, max_steps (takes precedence over epochs)\n- check_val_every_n_epochs (default 1, maybe want 10, 100)\n- val_check_interval (in case epoch = a few days, integer after n steps, or float for percentage of epoch)\n- num_sanity_val_steps (sanity, default 2, 0 to turn off, -1 for full valid loop)\n- limit_train_batches, limit_val_batches, limit_test_batches, (10-20 epochs for an action, shorten length of train/valid loops)\n- limit_val_batches = 0.1 = 10% of valid batches, int = a number of batches\n- gpus = 8 (use 8 GPUs), or pass in a list of indices according to PCI ordering, -1 for ALL gpus\n- auto_select_gpus = True -\u003e pick the right number of GPUs\n- log_gpu_memory=\u0027all\u0027, \u0027min_max\u0027 -\u003e log memory usage for GPU, but may slow training, it uses nvidia-smi\n    - recommended to prevent memory leaks\n- benchmark=True -\u003e results in speedups, but if the inputs change in size, not good.\n- deterministic=True -\u003e reproducable, but slowdown.\n- num_nodes - number of compute nodes. \n- \"ddp\" - pytorch - syncs gradients.\n- batch_size = num_nodes * num_gpus * num_nodes\n- need to set the seed, since otherwise the model weights will all be different.\n- can\u0027t use DDP in notebook/colab, or if you do fit multiple times. Then you need ddp_spawn, but that pickles everything, and you can\u0027t have num_works \u003e 0, and model on the original process will not be updatd.\n- DDP not supported on windows.\n- DataParrallel - splits data between batches, transfers across data a lot\n- DDP2 - examples with negative samples/contrastive training.\n- ddp_cpu - useful for debugging.\n\n\n## GPU training\n- delete all .cuda(), .to(device) calls\n- initialize tensors with device=self.device, and use register_buffer\n- z.type_as(x, device=self.device)\n\n\n## Mixed Precision\n- Lightning also casts buffers\n\n\n## Order things are called:\n- see https://lightning.ai/docs/pytorch/latest/common/lightning_module.html#hooks\n\nLast Reviewed: 4/28/25"}, {"color": {"background": "#35208d", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Distributed-Training/", "id": "Distributed Training", "label": "Distributed Training", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Distributed Training\n\n- DDP Syncs Gradients.\n- Then each optimizer updates their copy of the weights\n- \n\nLast Reviewed: 4/28/25"}, {"color": "#808080", "font": {"color": "white"}, "id": "Reinforcement Learning", "label": "Reinforcement Learning", "mass": 3.25, "shape": "dot", "size": 18.027756377319946, "title": ""}, {"color": {"background": "#808080", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\DPO/", "id": "DPO", "label": "DPO", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Direct Preference Optimization\n\nAbstract:\n- RLHF is complex and often unstable\n- requires fitting a reward model first\n- fine-tune LM with RL, without drifting too far from pre-training.\n\nDPO - new parametrization of reward model in RLHF, allows optimal policy in closed form, train with classification loss.\n\n- Stable, performant, lightweight computationally.\n- Don\u0027t need to sample from the LM during fine-tuning.\n\n- DPO \u003e PPO in sentiment control, equal or better in response quality, single-turn dialogue\n\n\n## LMs\n- LMs trained on all sorts of garbage data, e.g. bad code - want to bias towards the rare \u0027good code\u0027 examples. Or, misconceptions that 50% of people believe - bias toward truth.\n- select responses and behavior from knowledge and abilities.\n- RL objectives simplify to BCE.\n![alt text](image.png)\n\n- RL typically does better than SFT on human demonstrations\n- RLHF requires training another LM as reward model\n    - contains loss to prevent drifting too much from original model"}, {"color": "#808080", "font": {"color": "white"}, "id": "ReaLChords", "label": "ReaLChords", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#808080", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\RLHF/", "id": "RLHF", "label": "RLHF", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# RLHF\n\n- Train a reward model\n\n- to collect more data, find trajectories for which the ensemble of reward models have high variance, and disagree more.\n"}, {"color": {"background": "#808080", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\RLOO/", "id": "RLOO", "label": "RLOO", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# RLOO\n- Taking RL out of RLHF - preference training\n- RLHF - supervised fine tuning\n- Reward model, trained as a binary classifier\n- RL step - maximize subject to current distribution, with KL penalty, to prevent reward hacking.\n- PPO - unnecessarily complicated\n    - clipped loss prevents catastrophic gradient updates\n    - difficult to tune\n- REINFORCE - estimator, 1990s, provides update rule to maximize reward under a policy.\n    - unbiased baseline - expectation doesn\u0027t move when trying to optimize\n    - actor critic reduces variance.\n- no need to have a parametrized baseline - could have moving average - moving average of all rewards throughout training.\n    - not super strong.\n\n## RLOO\n- RLOO - leave one out - use additional samples to create a parameter free baseline.\n    - generate additional samples.\n- PPO \n    - Generally, GAE is the nob that controls bias-variance in PPO.\n    - lamba = 0.95, turning up all the way to 1, you get value function as return.\n- Generally, don\u0027t introduce bias to reduce variance. just vary lamba - \n    - smaller lamba = the worse optimization is\n- Clipping is not necessary\n    - large ratios are rare and not needed in RLHF.\n    - importance sampled ratio has high variance, large ratios are not something we see in RLHF.\n    - 1-3% this clipping is activated in RLHF.\n\n- Sequence as action - reward is only attributed to the EOS token, but all other tokens carry a KL penalty\n- does this really make sense?\n- Entire sequence is an action, instead of each token.\n- In LLM, initial policy is unusually strong.\n- All the probability mass is contained in top 32 tokens.\n- Not thta many actoins that are probabe."}, {"color": {"background": "#808080", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Adaptive-Preference-Aggregation/", "id": "Adaptive Preference Aggregation", "label": "Adaptive Preference Aggregation", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Adaptive Preference Aggregation\n\nClassical Bradley-Terry model - humans have underlying score, which is a logit with some noise.\n\n- Adapt social theory for aggregating diverse human preferences\n\n- Does not cover the more-than-two-choice case\n\n- Annotators have different agreement\n\n- Assume label is coming from one-person - annotators have consistency\n\n- Circular agreement - A \u003e B, B \u003e C, C \u003e A.\n\n- Tied performance, inconsistent preferences.\n\n\nWhat they do:\n \n- solve circular agreement, use a tool from game theory - crowd preference/voting.\n- put N copies in candidate, each time sample, replace.\n- randomly\n- random nash equilibrium - choose 1/3. \n- very highly subjective/noisy samples.\n\n"}, {"color": {"background": "#808080", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Few-Shot-Preference-Based-RL/", "id": "Few-Shot Preference-Based RL", "label": "Few-Shot Preference-Based RL", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Few-Shot Preference-Based RL\n- Start with an existing reward function, every k-steps, they ask for 10 new responses from their expert, who specifies new preferences based on their observation.\n\n- Update the reward model:\n    - at each fine-tuning step, start from reward model, and fine-tune based on ALL human annotations collected during the course of policy training\n\n\n- reduces amount of data by 20x.\n- Meta-World - what are the human preference? If the task is backflip, the human is asked if the robot has a good backflip or not.\n- what about training the policy using expert-annotated rewards? Much more data inefficient, need more than 10 annotations."}, {"color": {"background": "#808080", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\AlphaGo/", "id": "AlphaGo", "label": "AlphaGo", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# AlphaGo\n\nUses CNN on board state\n\n\n12-Layer ConvNet -\u003e 40 Layer ResNet, by increasing network depth, huge boost in terms of ELO rating.\n\n40 -\u003e 80 = another huge boost.\n\nLast Reviewed: 10/25/2025"}, {"color": "#808080", "font": {"color": "white"}, "id": "Imitation Learning", "label": "Imitation Learning", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": {"background": "#808080", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Policy-Gradient/", "id": "Policy Gradient", "label": "Policy Gradient", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": "# Policy Gradient\n\nTo do - transfer\n\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#808080", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Actor-Critic/", "id": "Actor Critic", "label": "Actor Critic", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": "# Actor Critic\n\nTo do - transfer\n\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#808080", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Generalized-Advantage-Estimation/", "id": "Generalized Advantage Estimation", "label": "Generalized Advantage Estimation", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Generalized Advantage Estimation\nDefine this:\n![alt text](GAE-2.png)\nThe first term is actor critic, next state minus current state\nThe last term is pure policy gradient, with value function as baseline.\n\nGAE takes a weighted sum between all these terms:\n![alt text](GAE-1.png)\n\n\n$\\lambda$ thus determines the bias-variance tradeoff. Policy gradient has high variance. One step actor critic has high bias due to reliance on value function, but low variance since we are only considering values between $t$ and $t+1$.\n\n"}, {"color": "#7a61ab", "font": {"color": "white"}, "id": "CS 285", "label": "CS 285", "mass": 0.25, "shape": "dot", "size": 5.0, "title": ""}, {"color": "#FDE7B1", "font": {"color": "white"}, "id": "Soft Skills", "label": "Soft Skills", "mass": 1.2500000000000002, "shape": "dot", "size": 11.180339887498949, "title": ""}, {"color": {"background": "#fde7b1", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\soft-skills\\Julius-Advice/", "id": "Julius Advice", "label": "Julius Advice", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Julius Advice\n\nTaken from his website.\n\n## Julius Story\n\n- Rice, student + musician, technical foundation in DSP\n- ESL Job, DSP at sea, played in bands for fun\n- MS/EE (while working?)\n    - CCRMA, gave hardware + software assistance for access to a computer\n    - 300-level classes in EE\n    - from gigs to computer music\n- Hertz fellowship\n- first two years, taking courses, pure interest, and also applicability\n    - Kailath + Goodman\n- master a branch of the lit: digital filter design / system\n- sometimes, a promising idea remains after reading all the lit\n- started publishing in 3rd year of PhD\n\n- job at SCT shortly before graduating, adaptive signal processing, dept of systems control tech - not music, but kept publishing in music venues\n    - kind of a postdoc\n\n- CCRMA research associate, continue developing waveguide synthesis\n    - half time at SCT\n    \n- couple years later - Steve jobs calls, wants music processing software, very few EE PhDs in music tech - 25th employee and NeXT, replcated SCT, moved down to 1 day per week at CCRMA\n\n- 1989 - full time Prof at CCRMA, CCRMA money came from FM synthesis patent\n\nLesson: opportunities are unpredictable\nBut, the big picture is simple. \n- Choose courses according to \u0027likely relevance\u0027, get best education. \n- work best job to develop skills. \n- publish some things (annually).\n- pursue music in own way\n\n\npursue relationships with the best institutes in the field - brand recognition is important, since there is so much going on.\n\nToday, one needs to tool up to produce on many fronts.\n\n## Choosing your next move\nbase this off interests, talents, past experience.\n    - observe past/present self to see\n    - what are your interests really? recurring themes from the past - spontaneous interests that recur reliably.\n    - you can decide on what you\u0027re interested in, and be wrong\n    - major should be emergent after taking all sequences we want - courses reinforce a theme. julius\u0027s idea for courses is \"achievement unlocked\" - take any classes, graduate whenever.\n    - graduate courses, depth outweights breadth, important to squeeze these in\n\n## Short-Term Research\n\nDaily: Read Paper, replicate results\nWeekly: summarize work from past week, describe plan for next week, present papers.\n\n\n## Prioritizing Ideas:\n\nPriority = Significance / (Difficulty * Number of People Working on it)\n\n\n## Ideal Research Paper Outline\n\n1. Abstract - Terse Summary of rest, and conclusions\n2. Intro - setting, problem statement, motivation: application area, problem addressed, why it is important\n    - Prior Work - Prior Efforts, why they fall short, references to/cogent summaries of literature\n    - Summary of contributions - how paper goes beyond prior efforts\n    - paper summary for rest of paper\n3. The Problem\n4. Our new approach\n5. Results\n6. Discussions + Interpretation of results, thorough\n7. Conclusions - summarize results, future work\n8. Appendicies  (for proof, e.g.)\n9. Bibliography\n\n\n## Research Ideas\n- ideas from papers in recent lit. - good coverage, community will accept, not re-inventing\n- harder - find idea great in isolation, then apply, may have a hard time - better to be connected with community\n\n## Prioritizing Time\n- Give each day a theme, try to stick to it\n- to do list:\n    - stack: new items go to top, not bottom. There is permafrost at bottom.\n    - stuff that gets pushed down might re-insert backup if actually important\n    - to do shrinks at home, grows on campus, Julius prefers to WFH\n    - email adds to to do\n    - Julius uses emacs to email him the calendar.\n\n\n## Best Practices\n- paper to arkiv, code to github, readme with instructions to replicate results\n- webpage with sound examples, paper, code, videos, etc.\n\n## Conference Proceedings\n - Fast, quick dissemination, high acceptance rates\n - HTML page of rweb traffic\n - Journals useful for tenure, but don\u0027t usually publish here\n - Link to a video on making posters\n\n## Tips on Writing\n - Read good writing to get in the voice\n\n## How it realy works\n - top positions are based on \u0027like\u0027 factor - rec lettters\n - don\u0027t let work distract from likeability, likeability also better for well-being\n\nLast Reviewed: 8/5/2025\n\n"}, {"color": "#fde7b1", "font": {"color": "white"}, "id": "Paper Writing", "label": "Paper Writing", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": {"background": "#fde7b1", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\soft-skills\\How-to-Write-a-Good-Paper/", "id": "How to Write a Good Paper", "label": "How to Write a Good Paper", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# How to write a good paper\nTalk by Bill Freeman, CVPR 2020, workshop on how to write a good review\n\n## Good papers are the only ones that matter\n\nonly creative/original/good papers count, and bad papers actually hurt\n\ngraph of paper quality vs impact on career\n\noutliers/unique papers/unusual papers are great.\n\n\n\n## Research is a crowded marketplace\nit\u0027s not a bunch of scholars poring over your manuscript - everyone is trying to get attention\n\n\n\n## Structuring a paper (not the only way)\n\n1. what is the problem we\u0027re addressing\n - why should the audience care? sometimes you must tell them\n\n2. what are the other solutions, and why are they not satisfactory?\n3. explain your ownn solution, compare with others, why is yours better\n4. Related work, similar solutions applied to another problem\n\nWait to write a paper until you have something important to say\n\nExample format:\n1. Introduction\n2. Related Work\n3. Main Idea (e.g. image  model)\n4. algorithm\n    - e.g. estimating bur kernel\n            - multiscale approach\n            - user supervision\n5. experiments\n    - e.g., large blur, small blur, images with significant saturation\n6. discussion\n7. conclusion\n\n\n\n## Introduction\n\nyou must make the paper easy to read - for people to tell what the paper is about, problem, why problem is interesting, what\u0027s new, what\u0027s not, what\u0027s neat\n\n\n## Main Idea\n\ncan include a toy example\ne.g. low, mid, high spatial bands look very different when you shift a wavelet\n\n## Experiments \nexperiments on examples people care about are required - need quantitative comparison against other algorithms\nif it\u0027s a new problem, find a workaround - how might a reasonable person modify another algorithm, or use a disabled version of your algorithm as baseline\n\n## Conclusions\nWhy is the world a better place, what can you do now\n\nfuture work: bad to end off with \"here\u0027s all the things we didn\u0027t do\". There\u0027s no partial credit. It\u0027s also a list of ideas for people to steal.\nsay where the work will lead, in general directions.\n\n\n## Writing Tips\nDoing these tips takes another 2-3 days\n\n### Keep the reader in mind\n- what does the reader expect/know so far?\n- reader is a guest in your house - anticipate their needs (oh, you\u0027re probably thirsty/oh you\u0027re probably wondering this)\n- what are they curious about/what do they understand/what do they need to know next\n- gie a talk before to your lab, gives it structure, see where people are confused/what didn\u0027t work\n\n\n### omit needless works\n- sentence has no needless works, paragraphs no needeless sentences\n- can be long, but every word tells\n- there\u0027s a list of phrases that can be shortened\n- most authors are too wordy (too many words)\n- concise writing is easier to glance at and get meaning from\n- writing example\n- benefits:\n    - more espace for other things (figure, experiment, more detail onto something - papers are always a squeeze)\n    - shorter = easier to understand\n- work through the first draft and make it concise\n\n### Readership\n- Most only glance at title, some skim abstract/figures, some read every word\n- readers who read every word are the most important, but the paper should also work for people who only look at figures/abstract\n- should be possible to read in a hurry and get the main points\n- want figures to be self-contained, including captions\n    - captions should tell the reader what to notice about the figure\n\n- Equations are mostly skimmed over, except the most basic (knuth) - paper should make sense when all but the simplest equations are replaced with grunts, and be smooth\n    - identify equations with a phrase, so people won\u0027t have to memorize numbers\n\n### Tone\n- be kind and gracious to baselines, security, not competition - \"we\u0027re all good\", we\u0027re standing among greats, not that every other paper is bad\n- don\u0027t oversell, hide drawbacks, and disparage\n- \"because the author was ___ I could trust the results\" - best paper prize, 2020\n- Be positive - more pleasurable to read than a paper that implies it is the only good paper around.\n- convey the right impression of performance: be honest\n\n## Titles\n- Shiftable multiscale transforms should be \"what\u0027s wrong with wavelets\"\n\n\n## The job of ACs is to reject 80% of papers.\n- they\u0027re always looking for easy reasons to reject, since they need to reject that many papers, and it is basically their job to reject papers\n- easy reasons:\n    - promises undelivered\n    - missing important references\n    - too incremental\n    - results not believable\n    - poorly written\n    - incorrect statementns\n- 1/3 are obvious rejects, and 1-2 are ORALs that really stand out and are EASY to select\n- borderline papers:\n    - cockroach - not exciting, boring, well-written, incremental, ok reviews. 2/3 are accepted\n    - puppy with 6 toes - easy to point flaws, but delightful, 2/3 rejected, but maybe Oral next time\n\n\n## Other notes\n- Good writing is rewriting\n- for negative results - you have to be thorough in provivg \"the answer is not here\". A bad negative result paper could have negative results that just depend on the choices made in the paper, and are not comprehensive. still these papers are hard to get accepted\n- talk to others, ask about if they think experiments are enough.\n- Faculty candidates for MIT: instead of counting papers, count the good papers. don\u0027t start writing too late, require an outlier first (richard szeliski)\n- author list - since only great papers matter, it\u0027s better to be one of many on a great paper than one of few on a mid paper. do whatever it takes to make the paper pbetter.\n- ask them if they feel they should be an author, if they say yes, put them on.\n- other references\n    - how to get your SIGGRAPH paper rejected (1993)\n    - ted adelson\u0027s informal guidelines\n    - notes on technical writing (knuth)\n    - what\u0027s wrong with these equations\n    - fredo durand\u0027s notes on writing\n    - ten simple rules for mathematical writing\n\n\nLast Reviewed: 7/30/2025"}, {"color": {"background": "#fde7b1", "border": "white", "borderWidth": 1}, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\soft-skills\\ICML-Best-Practices/", "id": "ICML Best Practices", "label": "ICML Best Practices", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# ICML Best Practices\n\n- claims and problems: clearly state the problems addressed and claims made.\n- soundness: clearly explain how the results substantiate the claims.\n- honesty: identify technical assumptions/limitations\n- significance: if it\u0027s a new problem, why is the problem important?\n- self-containment: expert reader has a good chance of understanding the paper\n- context: paper discusses prior work, how it is related to the contributions made, making it easier for future readers to understand why the problems being addressed are being targeted\n- background: for readers not familiar, provides some background\n\n\n## Writing\n- title expresses main contribution\n- abstract gives short, objective summary of contributions\n- paper separates problem description from contributions\n- paper has conceptual outline/psuedocode description of algorithms/methods\n- random access: you can read the problem description, skim notation, then look at claims and justification\n- proofread\n\n\n## Theoretical Contributions\n- assumptions are stated in a formal, clear way\n- novel claims are formally stated\n- self-contained: theorem statements should not require all previous context to understand\n- non-trivial statements are proved\n- intuitive arguments for why statements hold\n- why is the new algorithm better than previous state of the art? what changed?\n- the parts of proofs that are not new are generously attributed\n- definitions for words that are non-standard\n- all external results cited, with page number/theorem number if needed\n\n\n## Computational Experiments\n- explain why we do each experiment - what questions are answered?\n- separate\n    - design of experiment\n    - description of experiment\n    - results\n    - interpretation of results\n- experiments with randomness can be replicated, by setting seeds\n- which algorithms are used to compute each metric, along with references for the metric, and number of runs\n- instead of average/median, include distributional info (e.g. std if data is close to normally distributed)\n- list all hyperparameters\n- number/range of hyperparameter values\n- source code required is in supp or appendix, made publicly available\n- compute requirements, GPU/CPU models, OS, names/versions of relevant software/frameworks\n\n\n## Datasets\n- all novel benchmarks are included in appendix/supp\n- will be made publicly available\n- existing benchmarks are cited/publicly available\n- ones that aren\u0027t public are described in detail (how benchmark was designed/how data was obtained/descriptive stats)\n\n\nLast Reviewed 7/30/2025"}]);
                  edges = new vis.DataSet([{"arrows": "to", "color": "#C41E3A", "from": "Functions", "to": "Math", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Group Theory", "to": "Math", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Real Analysis", "to": "Math", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Linear Algebra", "to": "Math", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Eigenvalues", "to": "Linear Algebra", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Calculus", "to": "Math", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Infinitesimals", "to": "Calculus", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Gradients", "to": "Calculus", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Gradients", "to": "Linear Algebra", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Gradients - UDL", "to": "Gradients", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Chain Rule", "to": "Gradients", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Information Theory", "to": "Math", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "A Brief Introduction To Information", "to": "Information Theory", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Deep Learning Chapter 3", "to": "Information Theory", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Mutual Information", "to": "Information Theory", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Rate Distortion Theory", "to": "Information Theory", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "KL Divergence", "to": "Information Theory", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Six Interpretations of KL Divergence", "to": "KL Divergence", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Entropy", "to": "Information Theory", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Cross Entropy", "to": "Information Theory", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Statistics", "to": "Math", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Biased vs Unbiased Estimates", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Uniform Width Sampling", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Random Variables and Probability Distributions", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Bayes", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Conditional Independence", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Covariance", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "Running Covariance Algorithms", "to": "Covariance", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Optimization", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "Optimization - UDL", "to": "Optimization", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "Momentum, RMSProp, Adam", "to": "Optimization", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Machine Learning", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "PCA", "to": "Machine Learning", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "Linear Classifiers", "to": "Machine Learning", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "SVM", "to": "Linear Classifiers", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "K-Nearest Neighbors", "to": "Machine Learning", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "K-Means", "to": "Machine Learning", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "Approximate Nearest Neighbors", "to": "K-Nearest Neighbors", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "Approximate Nearest Neighbors", "to": "K-Means", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "DDSP", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "PQMF", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Downsampling and Stretching", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Convolution", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Transpose Convolution", "to": "Convolution", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Fourier Transform", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Discrete Fourier Transform", "to": "Fourier Transform", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Fourier Dualities", "to": "Fourier Transform", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Pink Frequency Profiles", "to": "Fourier Transform", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "1f Noise in Music and Speech", "to": "Pink Frequency Profiles", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Initialization - UDL", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Measuring Performance - UDL", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Precision and Recall", "to": "Measuring Performance - UDL", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Interpretability", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Concept Activation Vectors", "to": "Interpretability", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Deep Learning Theory", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Geometric Deep Learning", "to": "Deep Learning Theory", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Double Descent", "to": "Deep Learning Theory", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Generalization", "to": "Deep Learning Theory", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Bias-Variance Tradeoff", "to": "Deep Learning Theory", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Neural Scaling Laws", "to": "Deep Learning Theory", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "PAC-Bayes", "to": "Deep Learning Theory", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Training", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "A Recipe for Training Neural Networks", "to": "Training", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Evolution Strategies", "to": "Training", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Pretraining", "to": "Training", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Backpropagation", "to": "Training", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Backpropagation", "to": "Chain Rule", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Loss Functions", "to": "Training", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Loss Functions - UDL", "to": "Loss Functions", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Fine Tuning", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Transfer Learning", "to": "Fine Tuning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "LoRA", "to": "Fine Tuning", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Architecture", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Positional Encodings", "to": "Architecture", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Autoencoders", "to": "Architecture", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "MLPs", "to": "Architecture", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "MLP Interpretation - UDL", "to": "MLPs", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Approximation Theorem", "to": "MLPs", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "CNNs", "to": "Architecture", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "CNNs", "to": "Convolution", "width": 4}, {"arrows": "to", "color": "#8a16b5", "from": "UNet", "to": "CNNs", "width": 4}, {"arrows": "to", "color": "#8a16b5", "from": "Visualizing CNNs", "to": "CNNs", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Sequence Models", "to": "Architecture", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "LSTM", "to": "Sequence Models", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "RNNs", "to": "Sequence Models", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Transformers", "to": "Architecture", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Free Transformer", "to": "Transformers", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Encoder Transformers", "to": "Transformers", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Decoder Transformers", "to": "Transformers", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "KV-Caching", "to": "Decoder Transformers", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Encoder Decoder Transformers", "to": "Decoder Transformers", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Encoder Decoder Transformers", "to": "Encoder Transformers", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Activation Functions", "to": "Architecture", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Pocketed Activations", "to": "Activation Functions", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Gated Activations", "to": "Activation Functions", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Normalization", "to": "Architecture", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Batchnorm", "to": "Normalization", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "GroupNorm", "to": "Normalization", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "InstanceNorm", "to": "Normalization", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "LayerNorm", "to": "Normalization", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "RMSNorm", "to": "Normalization", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Generative Modeling", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#FFD900", "from": "Energy Based Generative Models", "to": "Generative Modeling", "width": 4}, {"arrows": "to", "color": "#FFD900", "from": "Next-Scale Prediction", "to": "Generative Modeling", "width": 4}, {"arrows": "to", "color": "#FFD900", "from": "GANs", "to": "Generative Modeling", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "CLIP Plus GAN", "to": "GANs", "width": 4}, {"arrows": "to", "color": "#FFD900", "from": "VAEs - UDL", "to": "Generative Modeling", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "ELBO", "to": "VAEs - UDL", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Jensens Inequality", "to": "ELBO", "width": 4}, {"arrows": "to", "color": "#FFD900", "from": "Diffusion Models", "to": "Generative Modeling", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Diffusion Best Practices", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Denoising Autoencoder", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Classifier Free Guidance", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Elucidated Diffusion Models", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "DiT", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "DDIM", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Diffusion Beats GANs", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Edify Image", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Multi-Diffusion", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "SDEdit", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Diffusion Forcing", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "History Guidance", "to": "Diffusion Forcing", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Understanding Diffusion Models: A Unified Perspective", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Score-Based Generative Models", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Generative Modeling Using SDEs", "to": "Score-Based Generative Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Wiener Process", "to": "Generative Modeling Using SDEs", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "DDPM - UDL", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "DDPM - Math", "to": "DDPM - UDL", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "DDPM - Reparametrization", "to": "DDPM - UDL", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "DDPM - Noise Schedules", "to": "DDPM - UDL", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Representation Learning", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Reconstruction-Based Learning", "to": "Representation Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Similarity-Based Learning", "to": "Representation Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Contrastive Learning", "to": "Similarity-Based Learning", "width": 4}, {"arrows": "to", "color": "#3FFF57", "from": "DAC", "to": "Audio", "width": 4}, {"arrows": "to", "color": "#3FFF57", "from": "Re-Bottleneck", "to": "Audio", "width": 4}, {"arrows": "to", "color": "#3FFF57", "from": "Audio Generation", "to": "Audio", "width": 4}, {"arrows": "to", "color": "#3fff57", "from": "Audio Diffusion", "to": "Audio Generation", "width": 4}, {"arrows": "to", "color": "#3fff57", "from": "DiffWave", "to": "Audio Diffusion", "width": 4}, {"arrows": "to", "color": "#3fff57", "from": "Stable Audio", "to": "Audio Diffusion", "width": 4}, {"arrows": "to", "color": "#3fff57", "from": "Multi-Source Diffusion", "to": "Audio Diffusion", "width": 4}, {"arrows": "to", "color": "#3fff57", "from": "Stemphonic", "to": "Audio Diffusion", "width": 4}, {"arrows": "to", "color": "#3fff57", "from": "Token-Based Audio Generation", "to": "Audio Generation", "width": 4}, {"arrows": "to", "color": "#3fff57", "from": "VampNet", "to": "Token-Based Audio Generation", "width": 4}, {"arrows": "to", "color": "#3fff57", "from": "Next-Scale Audio Prediction", "to": "Token-Based Audio Generation", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Next-Scale Audio Prediction", "to": "Next-Scale Prediction", "width": 4}, {"arrows": "to", "color": "#3fff57", "from": "Autoregressive", "to": "Token-Based Audio Generation", "width": 4}, {"arrows": "to", "color": "#3fff57", "from": "AudioLM", "to": "Autoregressive", "width": 4}, {"arrows": "to", "color": "#3fff57", "from": "MusicGen", "to": "Autoregressive", "width": 4}, {"arrows": "to", "color": "#3FFF57", "from": "Speech", "to": "Audio", "width": 4}, {"arrows": "to", "color": "#3fff57", "from": "Speech to Speech Models", "to": "Speech", "width": 4}, {"arrows": "to", "color": "#3fff57", "from": "W2V-Bert", "to": "Speech", "width": 4}, {"arrows": "to", "color": "#3FFF57", "from": "Music", "to": "Audio", "width": 4}, {"arrows": "to", "color": "#3fff57", "from": "MIDI-DDSP", "to": "Music", "width": 4}, {"arrows": "to", "color": "#3fff57", "from": "Universal Audio Synthesizer Control with Normalizing Flows", "to": "Music", "width": 4}, {"arrows": "to", "color": "#79443B", "from": "PixelVAE", "to": "Vision", "width": 4}, {"arrows": "to", "color": "#79443B", "from": "VAR", "to": "Vision", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "VAR", "to": "Next-Scale Prediction", "width": 4}, {"arrows": "to", "color": "#79443B", "from": "Masked Image Modeling", "to": "Vision", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Masked Image Modeling", "to": "Reconstruction-Based Learning", "width": 4}, {"arrows": "to", "color": "#79443B", "from": "Segmentation", "to": "Vision", "width": 4}, {"arrows": "to", "color": "#79443b", "from": "ODISE", "to": "Segmentation", "width": 4}, {"arrows": "to", "color": "#79443b", "from": "RCNN", "to": "Segmentation", "width": 4}, {"arrows": "to", "color": "#79443b", "from": "FastRCNN", "to": "RCNN", "width": 4}, {"arrows": "to", "color": "#79443b", "from": "Mask-RCNN", "to": "RCNN", "width": 4}, {"arrows": "to", "color": "#79443B", "from": "3D Reconstruction", "to": "Vision", "width": 4}, {"arrows": "to", "color": "#79443b", "from": "Gaussian Splatting", "to": "3D Reconstruction", "width": 4}, {"arrows": "to", "color": "#79443b", "from": "NeRF", "to": "3D Reconstruction", "width": 4}, {"arrows": "to", "color": "#79443B", "from": "Advances In Computer Vision", "to": "Vision", "width": 4}, {"arrows": "to", "color": "#79443b", "from": "Image Formation", "to": "Advances In Computer Vision", "width": 4}, {"arrows": "to", "color": "#79443b", "from": "Linear Image Processing", "to": "Advances In Computer Vision", "width": 4}, {"arrows": "to", "color": "#00FF00", "from": "Language Modeling from Scratch", "to": "Language", "width": 4}, {"arrows": "to", "color": "#00FF00", "from": "LLMs", "to": "Language", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "LLMs", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#7d93a6", "from": "BERT", "to": "LLMs", "width": 4}, {"arrows": "to", "color": "#7d93a6", "from": "ColBERT", "to": "BERT", "width": 4}, {"arrows": "to", "color": "#00FF00", "from": "Tokenization", "to": "Language", "width": 4}, {"arrows": "to", "color": "#00ff00", "from": "Byte-Pair Encoding", "to": "Tokenization", "width": 4}, {"arrows": "to", "color": "#00ff00", "from": "WordPiece", "to": "Tokenization", "width": 4}, {"arrows": "to", "color": "#00ff00", "from": "SentencePiece", "to": "Tokenization", "width": 4}, {"arrows": "to", "color": "#3FFF57", "from": "Audio-Language Models", "to": "Audio", "width": 4}, {"arrows": "to", "color": "#00FF00", "from": "Audio-Language Models", "to": "Language", "width": 4}, {"arrows": "to", "color": "#2dff3b", "from": "CLAP", "to": "Audio-Language Models", "width": 4}, {"arrows": "to", "color": "#2dff3b", "from": "FLAM", "to": "Audio-Language Models", "width": 4}, {"arrows": "to", "color": "#79443B", "from": "Vision-Language Models", "to": "Vision", "width": 4}, {"arrows": "to", "color": "#00FF00", "from": "Vision-Language Models", "to": "Language", "width": 4}, {"arrows": "to", "color": "#75a136", "from": "CLIP", "to": "Vision-Language Models", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "CLIP", "to": "Contrastive Learning", "width": 4}, {"arrows": "to", "color": "#75a136", "from": "SigLIP", "to": "Vision-Language Models", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "SigLIP", "to": "Contrastive Learning", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Linked Lists", "to": "Computer Science", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Minhash", "to": "Computer Science", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Software", "to": "Computer Science", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Git", "to": "Software", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Filesystems", "to": "Software", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Slurm", "to": "Software", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Python", "to": "Software", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Vector Operations", "to": "Python", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Einsum", "to": "Vector Operations", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Hydra", "to": "Python", "width": 4}, {"arrows": "to", "color": "#212129", "from": "ML Systems", "to": "Computer Science", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "ML Systems", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#35208d", "from": "Mixed Precision", "to": "ML Systems", "width": 4}, {"arrows": "to", "color": "#35208d", "from": "Large Scale Deep Learning", "to": "ML Systems", "width": 4}, {"arrows": "to", "color": "#35208d", "from": "Webdataset", "to": "Large Scale Deep Learning", "width": 4}, {"arrows": "to", "color": "#35208d", "from": "Wandb", "to": "ML Systems", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Wandb", "to": "Software", "width": 4}, {"arrows": "to", "color": "#35208d", "from": "PyTorch", "to": "ML Systems", "width": 4}, {"arrows": "to", "color": "#212129", "from": "PyTorch", "to": "Python", "width": 4}, {"arrows": "to", "color": "#2f2259", "from": "Autograd", "to": "PyTorch", "width": 4}, {"arrows": "to", "color": "#2f2259", "from": "Lightning", "to": "PyTorch", "width": 4}, {"arrows": "to", "color": "#35208d", "from": "Distributed Training", "to": "Large Scale Deep Learning", "width": 4}, {"arrows": "to", "color": "#808080", "from": "DPO", "to": "Reinforcement Learning", "width": 4}, {"arrows": "to", "color": "#808080", "from": "ReaLChords", "to": "Reinforcement Learning", "width": 4}, {"arrows": "to", "color": "#808080", "from": "RLHF", "to": "Reinforcement Learning", "width": 4}, {"arrows": "to", "color": "#808080", "from": "RLOO", "to": "Reinforcement Learning", "width": 4}, {"arrows": "to", "color": "#808080", "from": "Adaptive Preference Aggregation", "to": "Reinforcement Learning", "width": 4}, {"arrows": "to", "color": "#808080", "from": "Few-Shot Preference-Based RL", "to": "Reinforcement Learning", "width": 4}, {"arrows": "to", "color": "#808080", "from": "AlphaGo", "to": "Reinforcement Learning", "width": 4}, {"arrows": "to", "color": "#808080", "from": "Imitation Learning", "to": "Reinforcement Learning", "width": 4}, {"arrows": "to", "color": "#808080", "from": "Policy Gradient", "to": "Reinforcement Learning", "width": 4}, {"arrows": "to", "color": "#808080", "from": "Actor Critic", "to": "Reinforcement Learning", "width": 4}, {"arrows": "to", "color": "#808080", "from": "Generalized Advantage Estimation", "to": "Actor Critic", "width": 4}, {"arrows": "to", "color": "#808080", "from": "CS 285", "to": "Actor Critic", "width": 4}, {"arrows": "to", "color": "#808080", "from": "CS 285", "to": "Policy Gradient", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "CS 285", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#FDE7B1", "from": "Julius Advice", "to": "Soft Skills", "width": 4}, {"arrows": "to", "color": "#FDE7B1", "from": "Paper Writing", "to": "Soft Skills", "width": 4}, {"arrows": "to", "color": "#fde7b1", "from": "How to Write a Good Paper", "to": "Paper Writing", "width": 4}, {"arrows": "to", "color": "#fde7b1", "from": "ICML Best Practices", "to": "Paper Writing", "width": 4}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {"layout": {"clusterThreshold": 1000000}, "nodes": {"borderWidth": 1, "borderWidthSelected": 3, "chosen": true, "shape": "dot", "font": {"size": 20, "color": "white"}}, "edges": {"smooth": false}, "physics": {"enabled": true, "solver": "hierarchicalRepulsion", "hierarchicalRepulsion": {"nodeDistance": 120, "centralGravity": 0.01, "springLength": 120, "springConstant": 0.0007, "damping": 0.5, "avoidOverlap": 1.0}, "stabilization": {"enabled": true, "iterations": 2000, "fit": true}}, "interaction": {"zoomView": true, "dragView": true, "zoomSpeed": 0.5, "mouseWheel": true}};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    
    <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        network.on('click', function(params) {
            if (params.nodes.length > 0) {
                var nodeId = params.nodes[0];
                var nodeData = network.body.data.nodes.get(nodeId);
                if (nodeData.href) {
                    window.open(nodeData.href, '_blank');
                }
            }
        });
    });
    </script>
    </body>
    
</html>