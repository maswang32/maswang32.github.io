<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 100vh;
                 background-color: #000000;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 100%;
                 height: 100vh;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#C41E3A", "fixed": false, "font": {"color": "white"}, "id": "Math", "label": "Math", "mass": 11.46, "shape": "dot", "size": 33.85262175962151, "title": ""}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Group-Theory/", "id": "Group Theory", "label": "Group Theory", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Group Theory\nA group is a set with an associative binary operation, such that every element in the group has an inverse under that operation, and there is an identity element.\n\nLast Reviewed 4/30/25"}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "Linear Algebra", "label": "Linear Algebra", "mass": 4.0, "shape": "dot", "size": 20.0, "title": ""}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "Calculus", "label": "Calculus", "mass": 4.500000000000001, "shape": "dot", "size": 21.213203435596427, "title": ""}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Gradients/", "id": "Gradients", "label": "Gradients", "mass": 4.0, "shape": "dot", "size": 20.0, "title": "# Gradients\n\nGradient indicates direction of highest increase\nGradient specifies linearization (plane) of the function up to an offset (derivative gets rid of +C)\nGradient direction specifies plane orientation\nGradient magnitude specifies plane slope\nPlane tells you all directional derivatives\nLast Reviewed: 10/27/24\n"}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Chain-Rule/", "id": "Chain Rule", "label": "Chain Rule", "mass": 3.0000000000000004, "shape": "dot", "size": 17.320508075688775, "title": "# Chain Rule\n\nReference Page: #1\nUnivariate Chain Rule - \u0027speeding up\u0027 interperation.\n\u0027boosting\u0027 at a point\n\nAll derivatives are evaluated at the same point, just in different input domains\n\nMultivariate chain rule, dx,dy can be separated due to linearization.\nIncreases accumulate across dx, and dy.\n\nextending to multi-in, multi-out\nviewing things in terms of unit changes after linearization.\n\nKey Idea: we can think of moving dx in x, and then moving dy in y,\nand seeing how much f changes. This will be the same as moving in the directional derivative,\nsince for linear functions, the slope is the same everywhere.\n\nKey Idea: to compute df/ds, linearize everything, move one unit in s, and see how much that affects f.\n\nThe linearity assumption is the assumes that changes in variables will affect the output independently.\n\nwherever a function has a derivative, it is locally linear\n\nLast Reviewed: 10/27/24\nReference Page: #1"}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Infinitesimals/", "id": "Infinitesimals", "label": "Infinitesimals", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Infinitesimals\n{% raw %}\n\nSuppose we have a rectangular approximation to a definite integral with limits $a$ and $b$. We take $N$ evenly spaced points $x_1, \\ldots, x_N$, where $x_1 = a$ and $x_N = b$. This corresponds to $N-1$ rectangles. The area under the curve is approximated as:\n\n$$\n\\sum_{i=1}^N f(x) (x_{i+1} - x_{i}) =\n\\sum_{i=1}^N f(x_i) \\Delta x\n$$\n\nWhere $\\Delta x$ is $x_{i+1} - x_{i}$. \n\nAs we take $\\Delta x \\rightarrow 0^+$:\n\n$$\n\\sum_{i=1}^N f(x_i) \\Delta x \\rightarrow \\int_a^b f(x) dx\n$$\n\nThe $dx$ represents an infinitely small change in $x$.\n\nIt is also why\n\n\n$$\n\\int_a^b dx = b - a\n$$\nSince\n\n$$\n\\lim_{\\Delta x \\rightarrow 0} \\left[ \\sum_{i=1}^N f(x_i) \\Delta x \\right] = b - a\n$$\n\nLast Reviewed: 2/4/25\n{% endraw %}\n"}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Functions/", "id": "Functions", "label": "Functions", "mass": 0.01, "shape": "dot", "size": 5, "title": "# Functions\n\npolynomials are a linear combination of x, x**2, x**3, as functions\nLast Reviewed: 12/1/24\n"}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "Information Theory", "label": "Information Theory", "mass": 1.54, "shape": "dot", "size": 12.409673645990857, "title": ""}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\posts\\A-Brief-Introduction-To-Information/", "id": "A Brief Introduction To Information", "label": "A Brief Introduction To Information", "mass": 0.04, "shape": "dot", "size": 5, "title": "# A Brief Introduction To Information\n\n\n- All information is communication - it requires a method of decoding, must be interpreted.\n- All digital info = bits\n- Setup: communicate a sequence of random events\n- The streaming setting (where bits are decoded as they come) requires disambiguous prefixes - I proved this.\n\n[Original Blog Post](https://calvinyluo.com/2019/03/19/a-brief-introduction-to-information.html)\n\nLast Reviewed: 11/9/24\n"}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "Deep Learning Chapter 3", "label": "Deep Learning Chapter 3", "mass": 1.0, "shape": "dot", "size": 10.0, "title": ""}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\KL-Divergence/", "id": "KL Divergence", "label": "KL Divergence", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": "# KL-Divergence\nAsymmetric\nDKL(P||Q) - symbols are drawn from P, but if we encode assuming drawn from Q, \nhow many extra bits on expectation are used\nEx~p[log(P(x))-log(Q(x))]\nAsymmetric b/c depends on which distribution you\u0027re sampling from - two examples\n(Should read more)\n\nhow much more suprised you\u0027d be seeing P while expecting Q\nLast Reviewed: 10/27/24\nReference Page: #2\n"}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\posts\\Six-Interpretations-of-KL-Divergence/", "id": "Six Interpretations of KL Divergence", "label": "Six Interpretations of KL Divergence", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# An Interpretation of KL Divergence\nImagine a lottery game. Let $X$ be a random variable describing the outcome of the lottery game, and $x$ be a realization of that random variable.\n\n- For a bet of $c$ on an outcome $x$, the house pays you $\\frac{c}{q(x)}$ where $q(x)$ is the probability they assign to the outcome $x$.\n- This is the optimal way for *them* to make money, if they believe the true distribution is $q(x)$.\n\nNow, let\u0027s talk about what you do as a player:\n- Suppose you know the true distribution of outcomes $p(x)$.\n- To maximize your winnings, you should bet proportional to $p(x)$.\n- Suppose you bet 1 dollar total.\n- Then you optimally bet $p(x)$ dollars for each outcome.\n\nYour expected log-winnings are:\n\n$$\n\\sum_x\\left[ p(x) \\log \\left(\\frac{p(x)}{q(x)} \\right)\\right]\n$$\n\nThis is actually the formula for KL-divergence.\n\nIn other words, $D_{\\text{KL}(p,q)}$ is the maximum amount of log-money that can be made off one dollar, when the payoffs are assigned by the distribution $q$, but the real distribution is $p$.\n\n\n\u003cspan style=\"color:blue\"\u003eTo Do: review other interpretations\u003c/span\u003e.\n\n[Source](https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence)\n\nLast Reviewed: 1/20/25\n\n\n\n\n"}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Entropy/", "id": "Entropy", "label": "Entropy", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Entropy\nExpected information in a distribution\nmeasures uncertainty in a probability distribution\nBernoulli Example\nLast Reviewed: 10/27/24\nReference Sheet #3."}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Cross-Entropy/", "id": "Cross Entropy", "label": "Cross Entropy", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Cross Entropy\n\n-\u00e2\u02c6\u00ab(p(x) log(q(x) dx))\n\nEntropy is:\n-\u00e2\u02c6\u00ab(p(x) log(p(x) dx))\nCall this Ent(p, p)\n\nCross Entropy is:\n-\u00e2\u02c6\u00ab(p(x) log(q(x) dx))\nCall this Ent(p, q)\n\nKL Divergence is:\n- \u00e2\u02c6\u00ab(p(x) log(q(x) dx)) - (-\u00e2\u02c6\u00ab(p(x) log(p(x) dx)))\nOr\n\u00e2\u02c6\u00ab(p(x) log(q(x) dx))  + \u00e2\u02c6\u00ab(p(x) log(p(x) dx))\n\nThis is Ent(p, q) - Ent(p,p)\n\nWhen we add KL divergence and entropy, we get cross entropy\nCross entropy = number of bits it takes to encode samples from P using an encoding trained on Q\nEntropy = number of bits it takes to encode samples from P using an encoding trained on P\nKL Divergence = number of extra bits it takes to encode samples from P using an encoding trained on Q.\nOr, KL divergence is cross entropy minus entropy.\nLast Reviewed: 1/20/25\n"}, {"color": {"background": "#c41e3a", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Info-Theory-Basics/", "id": "Info Theory Basics", "label": "Info Theory Basics", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Info Theory Basics\n\nInformation is -logP(x) for an event\nIndependent events have additive infomration\nLess likely events have higher Information\nknowing outcome of an event with 50% prob has 1 bit of information\nMeasured in nats or bits (recall logs of all bases are proportional)\n0 information if certain\n\nsetup: a bitstream encodes a sequence of random vairables. Prefix requirements impose a cost of 2^l\n\nLast Reviewed: 10/27/24\nReference Sheet #3, 3.1"}, {"color": "#FF6F20", "fixed": false, "font": {"color": "white"}, "id": "Statistics", "label": "Statistics", "mass": 6.410000000000001, "shape": "dot", "size": 25.317977802344327, "title": ""}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Biased-vs-Unbiased-Estimates/", "id": "Biased vs Unbiased Estimates", "label": "Biased vs Unbiased Estimates", "mass": 0.09, "shape": "dot", "size": 5, "title": "# Biased vs Unbiased Estimates\nIf we take the sample variance, it is actually an underestimate of the true variance.\nThat\u0027s because we\u0027re using the sample mean, which will be closer to the true mean.\nMeaning we also have to account for the variance in the sample mean."}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Uniform-Width-Sampling/", "id": "Uniform Width Sampling", "label": "Uniform Width Sampling", "mass": 0.01, "shape": "dot", "size": 5, "title": "# Uniform Width Sampling\n\nSuppose you would like to sample k out of N classes randomly.\n\nInstead of doing two stage sampling, you can generate N scores, then threshold those scores by k/N, where k is the desired number of classes.\n\nLast Reviewed 4/30/25"}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Random-Variables-and-Probability-Distributions/", "id": "Random-Variables-and-Probability-Distributions", "label": "Random-Variables-and-Probability-Distributions", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "# Random Variables and Probability Distributions\n\n## Definition of Random Variable\n\n\u003c!-- ### Practical Definition\nIn machine learning, it suffices to think of a random variable simply as a \u0027variable\u0027, or a placeholder for a number or vector. There are notions of probability and randomness associated with it, but these can be associated with other constructs, like probability distributions.\n\nFor instance,\n\n$$\ny = 2z\n$$\nMeans \u0027take the value that $$z$$ takes, and multiply it by $$2$$ to get $$y$$. There is no concept of \u0027randomness\u0027 yet introduced. --\u003e\n\n### Precise Definition\n\nA random variable $$X$$ is a function from a sample space $$\\Omega$$ to a set of outcomes $$E$$.\n\nFor instance, we can assign a random variable to the result of a dice roll, and call it $$X$$. In this case, the sample space is:\n$$\\Omega = \\{1,2,3,4,5,6\\}$$\n\nThe random variable $$X$$ is a function such that $$X(\\omega) = \\omega$$, for $$\\omega \\in \\Omega$$.\n\n\u003c!-- It might seem redundant to define a random variable as a function, but it is conceptually useful because functions can take on multiple values depending on their input. If we think of $$X$$ as a function, it is easier to cope with the possibility that $$X$$ may take on more than one value. --\u003e\n\n### Operators on Random Variables\nIf we assign\n$$\nY = 2X\n$$\n\nWe can think of multiplication by $$2$$ as an operator on the function $$X$$ (An operator takes in a function and provides another function).\n\nWe are really saying that $$Y$$ is a new random variable (a new function), $$y \\mapsto Y(\\omega)$$, such that\n\n$$\nY(\\omega) = 2X(\\omega) \\quad \\forall \\omega.\n$$\n\nIf $$X$$ is a random variable representing the value of a dice roll, $$Y$$ is a random variable representing twice the value of a dice roll.\n\n### More notes on notation\n#### Capital vs. Lowercase\nWhile random variables are represented with capital letters, we typically use the corresponding lower case letter to denote a realization of that random variable. In math terms, we use $$x$$ to denote $$X(\\omega)$$. \n\n$$\nx = X(\\omega)\n$$\n\nWhile $$X$$ is a function, $$x$$ is a value.\n\nThis is similar to how in mathematicals more generally, if $$f$$ refers to a function, then $$f(x)$$ refers to a value, namely, the output of $$f$$ when its input is $$x$$, although this distinction is [blurred frequently](https://en.wikipedia.org/wiki/Abuse_of_notation#Function_notation).\n\n#### Bold vs unbolded\nBolded random variables and their values (realizations) simply indicate that the random variable is vector-valued.\n\n\n\n### Sample Spaces in Machine Learning\nThe sample space is not usually referenced in machine learning. For instance, we might have a latent variable $$\\mathbf{Z}$$ in a latent variable model. If $$\\mathbf{z} = \\mathbf{Z}(\\mathbf{w}) \\in \\mathbb{R}^d$$, the sample space $$\\Omega$$ is $$\\mathbb{R}^d$$, and we think of $$\\mathbf{Z} : \\Omega \\rightarrow \\mathbb{R}^d$$ as $$\\mathbf{Z}(\\omega) = \\omega$$.\n\n\n## Probability Distributions\nA probability distribution is a maps random variable ***values*** to densities. Formally, a random variable $$\\mathbf{X}$$ is a function with an output domain (often $$\\mathbb{R}^d$$), and the probability density function maps the output domain of $$\\mathbf{X}$$ to density values in $$\\mathbb{R}$$.\n$$\np(\\mathbf{x}) : \\mathbb{R}^d \\rightarrow \\mathbb{R}.\n$$\n\nAs an exercise in notation, this means we should also be able to write:\n$$\np\\left(\\mathbf{X}(\\mathbf{\\omega})\\right) : \\mathbb{R}^d \\rightarrow \\mathbb{R}.\n$$\n\nWe abbreviate \"probability density function\" as \"PDF\".\n\n\n#### Note\nIt is not the case that a random variable has a single probability distribution, although we often reference a \"true\" probability distribution. Rather, a probability distribution is simply a mapping from a random variable\u0027s value to a density value.\n\n### Notes on Notation\n#### $$Pr(x)$$ vs $$p(x)$$\nTypically, $$Pr(A)$$ refers to the probability of event $$A$$, while $$p(x)$$ refers to the value of the PDF at a data point $$x$$. Not everyone uses this notation, though.\n\n#### Resolving function vs. value dilemma\nWe often use $$f(x)$$ to refer to the function $$f$$, instead of the value of $$f$$ at $$x$$. This is also true in statistics. We commonly use\n$$\np(\\mathbf{x})\n$$\n\nTo refer to the probability distribution $$p$$, even though it should denote the density value of the probability distiribution at the point $$\\mathbf{x}$$. This is used ubiquitiously, and perhaps the inclusion of $$\\mathbf{x}$$ helps specify that the distribution\u0027s input are realizations of the random variable $$X$$. Sometimes, this is important because $$p$$ may represent a *family* of distributions, not just a single probability distribution.\n\n\n#### Families of Probability Distributions\nIn machine learning, when we write something like $$p_\\theta$$, we are typically referring to a *family* of probability distributions, not just a single distribution.\n\nFor instance, if we draw $$\\mathbf{x_1}, \\ldots , \\mathbf{x}_N$$ independently from a data distribution, we can write\n\n$$\np_\\theta(\\mathbf{x_1}, \\ldots , \\mathbf{x}_N) = \\prod_{i=1}^N p_\\theta(\\mathbf{x_i}).\n$$\n\nIn this case, the left hand side refers to the probability of observing $$\\mathbf{x}_1, \\ldots , \\mathbf{x}_N$$ according to the joint distribution given by the parameters $$\\theta$$. We use $$p_\\theta$$ to refer to both the joint *and* marginal distributions. They are intertwined by the rules of probability, e.g., the chain rule.\n\n\n\n\n### Distributions \u0027over\u0027 random variables.\nIf we write\n$$\n\\mathcal{N}_x(0,I)\n$$\nThe $$x$$ in the subscript indicates that the distribution is \"over\" the random variable realization $$x$$, as opposed to another random variable. More precisely, $$\\mathcal{N}_x(0,I)$$ is function mapping $$x = X(\\omega)$$ to $$\\mathbb{R}$$. The $$x$$ denotes what we use as the input to this function.\n\nThis is useful to disambiguate the input to the PDF when we have several PDFs.\n\n#### Example\n\nSuppose we have \n\n$$\np(\\mathbf{x}) = \\int \\mathcal{N}_\\mathbf{x}(f(\\mathbf{z}), I) \\cdot N_\\mathbf{z}(0,I) d\\mathbf{z}.\n$$\nIn this case,\n\n\n- $$p(\\mathbf{x})$$ is a function mapping $$\\mathbf{x}$$ to probability values.\n- For a given input $$\\mathbf{x}$$, we would substitute that value of $$\\mathbf{x}$$ into $$\\mathcal{N}_\\mathbf{x}(f(\\mathbf{z}), I)$$ to get a density value.\n- The value of $$\\mathbf{z}$$ we would use is determined by the integrand.\n\n\nTo evaluate $$p(\\mathbf{x})$$ for a specific $$\\mathbf{x}$$, we would:\n1. Iterate through all values of $$\\mathbf{z}$$\n2. Plug in $$f(\\mathbf{z})$$ for the these values to get the parameters (mean and variance) for the distribution $$\\mathcal{N}_\\mathbf{x}$$.\n3. Plug in $$\\mathbf{x}$$ into this distribution to get a probability density value.\n4. Plug in $$\\mathbf{z}$$ into $$\\mathcal{N}_\\mathbf{z}(0,I)$$ to get a probability density value\n5. Multiply the two density values together (the outputs of the two normal distributions) and accumulate it over all $$\\mathbf{z}$$ to evaluate the integral.\n\nThe parameters $$f(\\mathbf{z}), I$$ are how we describe the function that is the probability distribution.\n\nLast Reviewed: 1/31/25"}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Bayes/", "id": "Bayes", "label": "Bayes", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "# Bayes\nWe can ignore terms that are constant with regard to the distribution we are computing.\nFor instance, for a fixed x*,\n\np(z | x*)  = (p(x* | z) p(z)) / p(x*)\n\nbut we can ignore p(x*) since we are interested in a distribution with respect to z.\nTo get this distribution, we can evaluate p(x* | z) p(z) at all z and ensure it integrates to 1\nby rescaling it by C\nignoring the need for p(x*) term (which is 1/C).\n\nTO DO: Find notes about \u0027evidence\u0027 in Bayes\n\nLast Reviewed: 1/25/25\n\n\n"}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Conditional-Independence/", "id": "Conditional Independence", "label": "Conditional Independence", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Conditional Independence\nLast Reviewed: 1/25/25\n\ne.g., in diffusion models, we have\n\nq(z_2 | z_1, x) = q(z_2 | z_1)\n\nsince z_1 provides all information needed to compute z_2, thus given z_1 as information,\nz_2 is independent from x, or x provides no \u0027additional information\u0027."}, {"color": "#ff6f20", "fixed": false, "font": {"color": "white"}, "id": "Optimization", "label": "Optimization", "mass": 3.32, "shape": "dot", "size": 18.2208671582886, "title": ""}, {"color": {"background": "#ff6f20", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Momentum,-RMSProp,-Adam/", "id": "Momentum, RMSProp, Adam", "label": "Momentum, RMSProp, Adam", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "# Momentum, RMSProp, Adam\n\nNotes from \"A visual explanation\"\n\nMomentum in Physics - F = ma, a force will cause a constant change in velocity.\nSame as momentum in ML - momentum = velocity, \nforces = decay (friction), and the additional gradient\nderivative = applying a force for one time frame, leading to an acceleration (change in velocity)\nmomentum helps with plateaus and local minima\n\n\nAdaGrad - history of squared gradients for a direction accumulate, updates in that direction are divided by this\n= encourages exploration in directions where not many changes have happened\n- escapes saddle points better by going diagonally\n- regular GD optimizes steeper features first\n- slow b/c squared gradient accumuates\n\n\nRMSProp - squared gradients decay, squared gradients have momentum\n\nAdam - gradients have momentum, so do squared gradients.\n- momentum allows for escaping local minima\n- sum of squares = explore new directions\n\n\n\nNotes from Andrew NG:\nMomentum cancels oscillations\nCorrections are usually applied to Adam so things get rolling earlier\nLast Reviewed: 11/9/24\n\n\n\n\n\n"}, {"color": "#e24b31", "fixed": false, "font": {"color": "white"}, "id": "Machine Learning", "label": "Machine Learning", "mass": 0.49, "shape": "dot", "size": 7.0, "title": ""}, {"color": {"background": "#e24b31", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Linear-Classifiers/", "id": "Linear Classifiers", "label": "Linear Classifiers", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# Linear Classifiers\nFrom CS 231N\nA linear classifier works like this:\n- A neural network (a single linear layer) produces a score, and a loss function maps this score to an \u0027agreement\u0027 value with the class label.\n- We minimize the loss with respect to the parameters of the score function\n- If you have $ W\\mathbf{x} + b$, that  is like evaluating $K$ classifiers separately one for each class. Each classifier is a row of $W$.\n    - This is like template matching using the dot product, or \u0027one template\u0027 KNN - where there is one image per class, and distance is the dot product, not L2 or L1 distance.\n    - It is much faster than KNN since you do not have to compute distances to all the training set points.\n\n- The weights indicate the directionality of the relationship, for each pixel - like a positive weight on blue means we want that pixel to be blue.\n\n- Each row of $W$ is a hyperplane, with a normal vector indicating the direction of increase, and \"on the plane\" meaning 0.\n    - (Is the template in the same direction as the average image?)\n\n- if you have multiple layers, maybe earlier layers detect specific cars (e.g. green, blue) and the NN is a weighted sum of individual car detectors.\n- zero mean centering is more important than scaling (why)?\n\n## SVM\n\n- Wants corrrect class to have a bigger score than all the incorrect classes by some margin $\\Delta$.\n$$ L_i = \\sum_{j\\neq y_i} \\max (0, s_j - s_i, \\delta) $$\n- There is a loss on all the scores that are not class $i$, and the loss is how much bigger they are than class $i$\u0027s score minus $\\Delta$.\n- If the score for the incorrect class is less than the correct class by $\\Delta$, there is no loss here!\n- There is also quadratic hinge loss, where each term in the sum is squared\n\n### Regularization\n- Weight magnitude is underdetermined, if you class all correctly, weight can be any scasle\n- Normalize with respect to L2 Norm\n- $L = \\frac{1}{N} \\sum_i L_i + \\lambda R(W) $\n- Don\u0027t regularize biases\n- Improves generalization by requiring dependency on all inpus\n\nWe can set $\\Delta$ to $1.0$ and only tune $\\lambda$, due to the weight magnitude thing.\n\n## Softmax\n\n\n$$\nL_i = - \\log \\frac{e^{s_{y_i}}}{\\sum_k e^{s_k}}\n$$\n$$\n= - s_{y_i} + \\log \\sum_k e^{s_k}\n$$\n\n- Scores are interpreted as unnormalized log probabilities\n- shift by max\n- Regularization is a Gaussian Prior on the weight matrix.\n- Large $\\lambda$ means more diffuse probabilities\n- Perfomance difference between SVM and softmax is small.\n- SVM is more local - it stops trying onces scores are good enough, only care about scores near the margin.\n- A car classifier shouldn\u0027t focus on lower the probability of classifying ducks as cars even more, it should focus on distinguishing between cars and trucks.\n- this can be an arg in favor of SVM."}, {"color": "#0000FF", "fixed": false, "font": {"color": "white"}, "id": "Deep Learning", "label": "Deep Learning", "mass": 32.83, "shape": "dot", "size": 57.29746940310715, "title": ""}, {"color": {"background": "#a4009a", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Backpropagation/", "id": "Backpropagation", "label": "Backpropagation", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "# Backpropagation\n\nCS 231 Notes\n\n## Notes on Derivatives\nDerivative = sensitivity to an input.\n$\\frac{df}{dx} = 3$ means changes in $x$ will change $f$ by 3 times that  amount.\n\n$x$ increasing by $h$ results in change $\\frac{df}{dx} * h$.\n\n$f(x+h) \\approx f(x) + h \\frac{df}{dx}$\n\n$f(x,y) = \\max(x,y), \\frac{df}{dx} = 1[x \\geq y], \\frac{df}{dy} = 1[y \\geq x]$\n\nonly the higher value matters, gradient is zero on the lower value.\n\n\n## Notation\n\n- Always assume $dq$ implies $\\frac{df}{dq}$ or $\\frac{dL}{dq}$.\n- the computation is a bunch of gates - \n    - computes inputs to outputs\n    - computes $dout$ to derivatives with respect to inputs\n    - local graient means gradient with respect to gate\u0027s output, or $\\frac{dout}{din}$.\n    - the \"upstream gradient\" is $\\frac{dL}{dout}$.\n![\n](image-3.png)\n\n\nIn this graph, green is activations, red is derivatives. To increase $f$ by 1, we must decrease $q$ with a force of $-4$.\n\nThis multiplies $\\frac{dx}{dq}$ by $-4$.\n\nGates are determined by convenience.\n\nBackpropagation is gates communicating how to increase the output.\n\n\n\n## Backpropogation\n- just think of the one-input, one-output gradient, using the total derivative. then broadcast this operation. don\u0027t need to think of it as \tmatrix multiplies, as this gets harder when the dimensionality increases.\n\t-there are local gradients, and upstream gradients (dout/din, dL/dout). you always do a sum reduction across this dimension.\n\t-grouping operations in a single gate for simplicity\n\t-this combines with the branching rule.\n\t- need to cache forward pass variables, if you do things one step at a time\n\t- plus gates always route gradients to its inputs equally, max gates routes the gradient to the bigger value, and multiply gates take input activations, multiply them by the gradient of the multiply gate\u0027s output, and swaps them.\n\t-the multiply gate assigns the bigger gradient to the tiny input - so, if you multiply the input values by 1000x in linear classification, the weight has a very large affect on the output, and you will need to lower the learning rate. that is why preprocessing is important.\n\n## Vector Matrix Derivatives\nVector Matrix derivative: https://cs231n.stanford.edu/vecDerivs.pdf\n\t-taking derivatives of multiple things simultaneously, applying the chain rule, and taking derivatives when there is a summation - split these things up\n\t-write the formula for a single element of the output in terms of scalars\n\t-remove summation notation\n"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Normalization/", "id": "Normalization", "label": "Normalization", "mass": 0.5800000000000001, "shape": "dot", "size": 7.615773105863909, "title": "# Normalization\n\n## LayerNorm (Computer Vision)\n- You can think of this as a datapoint normalization, that puts all the datapoints \non the same playing field.\n- Not really used in computer vision, but useful to understand Group Norm\n- Statistics are computed along the C, H, W dimensions\n- Each example in the batch has shape (H, W)\n- Each example in the batch has mean 0, std 1. (before affine transformation)\n- The weight has a shape of C, H, W. This is confusing\n\n## Instance Norm\n- You can think of this as a channel normalization, that puts all the same channels on the same playing field.\n- Stastics are computed along H and W.\n- Each channel in each image has shape (H, W).\n- Each channel in each image has mean 0, std 1. (if no affine transformtation)\n- If affine transform, then there is a weight of shape (C), a bias of shape (C)\n- This helps us magnify different channels that previously were all nerfed to be mean 0, std 1.\n\n## Group Norm\n- Statisics are computed for a group of channels (GroupSize, H, W)\n- Each group of channels in each image has shape (H, W)\n- Each group of channels in each image then will have mean 0, std 1.\n- GroupNorm has per-channel weights (C), instead of per group weights.\n- It would also make sense to have per-group weights, since all the groups were nerfed to be on the same level, and we can help distinguish them.\n- However, per-channel weights can do the same thing.\n\n\n## Weight Norm\n- Splits up a weight vector into magnitude and norm."}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Batchnorm/", "id": "Batchnorm", "label": "Batchnorm", "mass": 0.09, "shape": "dot", "size": 5, "title": "## Batchnorm\n\t-input data should have zero mean, uncorrelated features, unit variance\n\t-activations at later layers should also have zero mean and unit variance\n\t-distributions of features shift during training\n\t-running averages are only used at test time\n\n\tNormal Batch Norm:\n\t-The mean is computed along the batch axis, meaning the batch axis is the only thing being reduced.\n\t-every DIMENSION in (D,) has its own mean and a variance.\n\t-gamma and beta (scale and shift params) are of size (D,) since these are PER-DIMENSION scale and shift parameters\n\n\tConvolutional batch norm\n\t-you don\u0027t just average over the batch axis, you also average over height and width.\n\t-that means every CHANNEL has a mean and variance, but it\u0027s not like every individual pixel does.\n\t-at the same time, gamma and beta (scale and shift) parameters are of size (C,), since these are CHANNELWISE scale and shift parameters."}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Positional-Encodings/", "id": "Positional Encodings", "label": "Positional Encodings", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# Positional Encodings\n\n\n\n## Transformer\nThe positional embedding is a series of sinusoids.\nThe sinusoids decrease in frequency logarithmically, such that there are much more low frequencies than high ones.\nTypically the \u0027minimum\u0027 frequency achieves one radian at the \u0027maximum value\u0027 that we encode.\n\n\n## Random Fourier Features\nPick random frequencies\nevaluate sines and cosines at those frequencies\nSeems worse than Positional Embedding"}, {"color": "#0000ff", "fixed": false, "font": {"color": "white"}, "id": "Interpretability", "label": "Interpretability", "mass": 0.49, "shape": "dot", "size": 7.0, "title": ""}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Concept-Activation-Vectors/", "id": "Concept Activation Vectors", "label": "Concept Activation Vectors", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# Concept Activation Vectors\nSpecify a concept by collecting examples\ntrain a classifier on these examples wrt random examples or another group (e.g. stripes vs dots)\ntake the orthogonal vector to this classifier (CAV)\ncompute the directional derivative of a class label (e.g. zebra) wrt CAV\ncan use to tell which concepts inform classifier decision\nother use cases (see notes)\n\nLast Reviewed: 10/27/24\nReference # 1"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Fine-Tuning/", "id": "Fine Tuning", "label": "Fine Tuning", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Fine Tuning\nLoRA\nControlNet\nSEGA\nLast Reviewed: 10/26/24\n"}, {"color": "#0000ff", "fixed": false, "font": {"color": "white"}, "id": "Activation Functions", "label": "Activation Functions", "mass": 0.08000000000000002, "shape": "dot", "size": 5, "title": ""}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Pocketed-Activations/", "id": "Pocketed Activations", "label": "Pocketed Activations", "mass": 0.04, "shape": "dot", "size": 5, "title": "# Pocketed Activation\nLast Reviewed: 1/3/25\n\nDead ReLU problem - activation ranges get super negative\nPocketed Activation (Swish, Mish) - activations get stuck in pocket, since it\u0027s a local minima\nEnough examples can remove from pocket\n\nGeLU is the same as setting the dropout probabilty to the CDF of the neuron value, and taking the expectation\n\nThink about this more"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Gated-Activations/", "id": "Gated Activations", "label": "Gated Activations", "mass": 0.04, "shape": "dot", "size": 5, "title": "# Gated Activations\n\nGLU = (Ax + b)*sigma(Cx + D)\nSwiGLU = (Ax + b)*swish(Cx + D)\nSwiGLU has this squared part (derivative vanishes near zero)\nReLU^2 also does well, perhaps due to this square part\nSnake has an x^2 term in its expansion\n\nLast Reviewed: 1/17/25\n"}, {"color": "#0000ff", "fixed": false, "font": {"color": "white"}, "id": "Understanding Deep Learning", "label": "Understanding Deep Learning", "mass": 8.809999999999999, "shape": "dot", "size": 29.68164415931166, "title": ""}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\chapters\\MLP-Interpretation-UDL/", "id": "MLP Interpretation - UDL", "label": "MLP Interpretation - UDL", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "# MLP Interpretation - UDL\n- Shallow MLPs clip linear functions, rescale, and combine.\n- $D$ hidden units means $D+1$ Linear Regions\n- Multivariate outputs are all clipped at the same joints\n- There\u0027s a Multivariate Input Visualization in the book\n- All ReLU MLPs split input space into Linear Regions\n- \"Folding\" interpretation\n- Adding a Layer is clipping Each Linear Region, and recombining\n- Bottlenecks are restricting weights to outer product\n- Depth efficiency is exponential compared to width efficiency\n- Depth generalizes and trains better\n- Swishes solve Dying ReLU\n- Weights can be rescaled as long as biases are too\n- Depth approximation theorem\n\nLast Reviewed: 11/1/24\n"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\chapters\\Loss-Functions-UDL/", "id": "Loss Functions - UDL", "label": "Loss Functions - UDL", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "# Loss Functions - UDL\n\nBelow are some brief notes on loss functions from Understanding Deep Learning.\n\n- Most losses are some form of negative log likelihood.\n- There is a \u0027formula\u0027 for writing loss functions:\n    - Model predicts parameters of a distribution, on which the probability of data is evaluated.\n    - Maximize probability of data, or minimize negative log probability of data.\n- Assume data is independent \n  - Assume the value of one datapoint does not affect the value of another (after the model is optimized)\n  - The probability of observing all your datapoints is the product of the probabilities of observing each of your individual datapoints.\n\n## MSE Loss\n- MSE results from assuming y is sampled from gaussians with means determined by x\n- In the heterodastic MSE, the variance of the output varies with the input\n\n## BCE Loss\n- BCE loss comes from assuming the distribution $p(y \\mid x)$ is Bernoulli (there\u0027s a visualization)\n- Multiclass cross entropy loss is discussed here as well\n- There is a table of distributions, and their usage in different tasks.\n\n## Other Notes\n- In multi-output situations, assume different outputs are conditionally independent given the input.\n- NLL minimization is the same as minimizing cross entropy between (possibly conditional on input) data distributions. This is really cool!\n\nReference Sheet: UDL Chapter 5\nLast Reviewed: 11/1/24"}, {"color": {"background": "#c63598", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\chapters\\Optimization-UDL/", "id": "Optimization - UDL", "label": "Optimization - UDL", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "# Optimization - UDL\n\n\u003cspan style=\"color:blue\"\u003eTo Do: Fill this out\u003c/span\u003e.\n\nLast Reviewed: 11/9/24\nReference Sheet: UDL Chapter 6\n"}, {"color": {"background": "#c82d78", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\chapters\\Gradients-and-Initialization-UDL/", "id": "Gradients and Initialization - UDL", "label": "Gradients and Initialization - UDL", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "# Gradients and Initialization - UDL\nForwards and backwards passes should preserve variance.\nNormalization helps with this.\nSee Written Notes\n\nLast Reviewed: 4/26/25\n"}, {"color": "#FFD900", "fixed": false, "font": {"color": "white"}, "id": "Generative Modeling", "label": "Generative Modeling", "mass": 17.01, "shape": "dot", "size": 41.24318125460256, "title": ""}, {"color": {"background": "#c876a3", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\chapters\\VAEs-UDL/", "id": "VAEs - UDL", "label": "VAEs - UDL", "mass": 1.32, "shape": "dot", "size": 11.489125293076057, "title": "# VAEs - UDL\n\n\u003cspan style=\"color:blue\"\u003eTo Do: Organize\u003c/span\u003e.\n\n\u003cspan style=\"color:blue\"\u003eTo Do: Insert ELBO Math\u003c/span\u003e.\n\n## Introduction\n- VAEs do not let you evaluate p(x)\n- MLE training is not trivial, but can define a lower bound\n- model an \u0027unobserved\u0027 latent variable, the thing that \u0027gives rise\u0027 to the image/sound\n- p(x) = integral(P(x,z)dz) = integral(p(x|z)p(z)dz)\nExample - Mixture of Gaussians, z describes which gaussian.\n\nVAE - P(z) is N(O,I), and P(x|z) is N(f(z), s^2I)\nIn other words, z maps to a gaussian\u0027s mean, and the distribution is a marginalization of all these gaussians over z.\n\nSee image where one distribution is made up of as a \u0027marginalization\u0027 (sum) of gaussians\n\nGeneration: sample from P(z) then P(x|z)\n\nEvaluating/maximizing p(x) slash sum(log(p(x)))is intractable.\n\nA model that maximizes it could try to assign big probabilities to your data, without being restricted to \nintegrating to 1.\n\nTrying to restrict it to integrate to 1 is intractable.\n\n\nNote:\np(z|x) is the \u0027posterior.\u0027 What could the latent variable be after observing x?\np(z) is the \u0027prior\u0027 on the latent variable.\np(x|z) is the \u0027likelihood\u0027. This helps us evaluate the Posterior, since we want to see, for each value of z,\nwhat is the probability we could have gotten that x? it\u0027s detective work.\nTo evaluate the posterior, Bayes Rule would say:\n\np(z|x) = p(x|z)p(z)/p(x).\nBut really, we only need the numerator, since we can make it integrate to 1, and p(x) does not determine the\nrelative probabilites of the z\u0027s.\n1---Compute p(x|z) for each value of z\n2---multiply by p(z)\n3---normalize so the posterior p(z|x) sums to 1\nThere\u0027s a diagram of this in UDL.\n\np(x) is called the evidence\nLast Reviewed: 1/19/25\n"}, {"color": {"background": "#e87267", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\ELBO/", "id": "ELBO", "label": "ELBO", "mass": 0.32000000000000006, "shape": "dot", "size": 5.656854249492381, "title": "# Downsampling and Stretching\nLots of math here. You can reprove this by hand from Jensen\u0027s inequality or look at your notes\n\nBasically, start with log(p(x)).\n---Then express it using a latent variable model decomposition\n---choose an arbitrary q(z) as your \u0027weighting\u0027\nint(  log(    q(z) * p(x,z)/q(z)         )  dz)\n---apply Jensen\u0027s inequality\nwhen you get to p(x,z) split it up into p(z|x) and p(x)\nthat will let you take out p(x), and also give you a KL term\n\nELBO = log(p(x)) - KL(q(z), p(z|x))) (this KL is assuming q is \u0027ground truth\u0027)\n\nMaximizing p(x) with respect to the parameters for q(z) and p(z|x) involves expectation maximization, this means\n---can improve ELBO\u0027s lower bound by changing p(z|x) slash p(x|z)\u0027s parametrization\nOR\n---can make ELBO bound more tight by changing q(z)\u0027s parametrization\nLast Reviewed: 1/19/25"}, {"color": {"background": "#e87267", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Jensens-Inequality/", "id": "Jensens Inequality", "label": "Jensens Inequality", "mass": 0.16, "shape": "dot", "size": 5, "title": "# Jensens Inequality\n\nImagine a bunch of datapoints lying on log(y)\n\nImagine a point (E[y], E[log[y]])\n\nThis is the midpoint of all the datapoints\nThis will lie under the log(y) curve by concavity:\nf((1-a)x + a*y) \u003e= (1-a)f(x) + a*f(y)\n\nThe midpoint will lie under the log curve\nIt is thus lower than\n\n(E[y], log(E[y])) which is on the curve.\n\nTo Do: Prove Jensen\u0027s Inequality\nLast Reviewed: 1/19/25\n\n\n\n"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Energy-Based-Generative-Models/", "id": "Energy Based Generative Models", "label": "Energy Based Generative Models", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# Energy-Based Generative Model\nInstead of modeling the gradient of the log of the probability, try to model the log probability directly."}, {"color": "#ffd900", "fixed": false, "font": {"color": "white"}, "id": "Diffusion Models", "label": "Diffusion Models", "mass": 14.220000000000002, "shape": "dot", "size": 37.70941526992961, "title": ""}, {"color": {"background": "#c876a3", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\chapters\\DDPM-UDL/", "id": "DDPM - UDL", "label": "DDPM - UDL", "mass": 3.4900000000000007, "shape": "dot", "size": 18.681541692269406, "title": "# Diffusion Models - UDL Notes\n{% raw %}\n\n## Introduction\n\u003cspan style=\"color:blue\"\u003eTo Do: Write Introduction\u003c/span\u003e\n\n\nDiffusion models can be interpreted as Hierarchical VAEs. The encoder in this case has no learnable parameters, and gradually adds noise to data. The decoder attempts to reverse this process.\n\n## DDPM - Noise Schedule\nLet $$\\mathbf{X}$$ the random variable representing a data sample from the intended distribution, and $$\\mathbf{x}$$ be a realization of it. Also, define\n\n$$\\mathbf{Z_0} = \\mathbf{X}$$\n\nWe have latent variables $$\\mathbf{z_1}, \\ldots, \\mathbf{z_T} $$ correponding to noise levels $$1, \\ldots, T$$. These latent variables are given by:\n\n$$\n\\mathbf{z_t} = \\sqrt{1 - \\beta_t }\\mathbf{z_{t-1}} + \\sqrt{\\beta_t}\\mathbf{\\epsilon}_t\n$$\n\nwhere $$ \\mathbf{\\epsilon}_t\\sim \\mathcal{N}(0,I) $$.\n\n### Marginal Distributions\nComputing $$\\mathbf{q}(\\mathbf{x}_t \\mid \\mathbf{x})$$ is like adding a noise level corresponding to $$t$$ to our data. The formulas above allow us to generate these by iteratively adding noise. However, we can actually directly compute the marginal distributions in closed form:\n\n$$\n\\mathbf{q}(\\mathbf{x}_t \\mid \\mathbf{x})\n$$\n\n(where we are marginalizing over $$\\mathbf{z}_1,\\ldots, \\mathbf{z}_{t-1}$$).\n\nWe can do this by working inductively.\n\n\u003cspan style=\"color:blue\"\u003eTo Do: Insert Inductive Proof\u003c/span\u003e.\n\nTo summarize, we define\n\n$$\n\\alpha_t = \\prod_{i=1}^t (1 - \\beta_t)\n$$\n\nAnd discover that\n\n$$\n\\mathbf{q}(\\mathbf{x}_t \\mid \\mathbf{x}) = \\mathcal{N}(\\sqrt{\\alpha_t} \\mathbf{x}, (1-\\alpha_t) \\mathbf{I})\n$$\n\n\n## DDPM - Derivation of Objective\nWe would like to maximize the log probability of our data under our generative model. We show we can express an ELBO of our objective as a weighted sum of MSE losses from a sample of $$\\mathbf{z}_{t-1} \\sim p(\\mathbf{z}_{t-1} \\mid \\mathbf{x})$$ and a prediction of that sample obtained from a learned function on  $$\\mathbf{z}_{t}$$. In other words, the objective is to predict $$\\mathbf{z}_{t-1}$$ from $$\\mathbf{z}_{t}$$, or to denoise the data one step.\n\nThis [DDPM Math](DDPM-Math.md) is shown here.\n\nIn practice, we express the function $$\\mathbf{f}$$ as as a linear combination of $$\\mathbf{z}_{t-1}$$ and a noise term $$\\mathbf{\\epsilon}_t \\sim \\mathcal{N}(0,\\mathbf{I})$$.\n\nWe use a neural network $$\\mathbf{g_\\theta}$$ to predict the noise term $$\\mathbf{\\epsilon}_t \\sim \\mathcal{N}(0,\\mathbf{I})$$. This leads to a [reparametrization, and a method of training and inference for DDPMs](DDPM-Reparametrization.md).\n\n\n## Why Does Diffusion Work?\n\n\n### Modeling Multi-modal distributions\nGenerative modeling is about turning simple distributions (noise) into more complex ones (data). \n- The data distribution is very complex and multimodal. But we can assume each denoising step $$ p( \\mathbf{z}_{t-1} \\mid \\mathbf{z}_t) $$ is approximately normal.\n- VAEs attempt to map $$\\mathcal{N}(0,I)$$ to a data distribution with potentially many modes. This is easier to do when each timestep increases the number of modes.\n- We sample from a normal distribution at **each reverse time-step**, which eventually allows us to be in a particular \u0027modes\u0027 of the distirbution.\n- This eventually allows us to model a multimodal distribution.\n- Metaphorically, each sampling step allows you to guides you towards a specific path or another.\n- Eventually, you\u0027ll fall into one of the modes of the distribution, but due to stochasticity, we hope to see *all* of them\n\n### Plinko\nThe game [Plinko](https://spribe.co/games/plinko) is a good analogy.\n\n- Higher layers in Plinko are the higher noise levels.\n- Instead of each stick outputting \u0027left\u0027 or \u0027right\u0027 with 50/50 probability, the stick steers it in a certain direction.\n\nWhen we extend this analogy to diffusion when the number of timesteps is infinite, we get something like Brownian Motion.\n\n- If we instead imagine an infinite number of sticks at an infinite number of heights, we have a simulation of Brownian motion or a [Wiener Process](https://masonlwang.com/knowledgemap/notes/concepts/Wiener-Process/).\n- The ball moves randomly at *every* time\n- The ball become a \u0027particle\u0027 randomly colliding with other particles (but moving down due to gravity, which is like moving forward in time.)\n\n\n## Notes on Optimization\n- We would like our model to predict $$q(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t})$$, where $$q$$ is the probability distribution given by the deterministic encoder.\n- However, we don\u0027t have supervision for this.\n- We can compute $$q(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_t, \\mathbf{x})$$.\n    - This is a Gaussian whose mean is near $$\\mathbf{z}_t$$, but a little further from 0 due to the drift term $$\\sqrt{1-\\beta_t}$$ applied to the mean of the data during the forward process.\n\n- This guides $$p_\\theta(\\mathbf{z}_{t-1}, \\mathbf{z}_t)$$ toward $$\\mathbb{E}_{\\mathbf{x} \\sim p(\\mathbf{x})} q(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_t, \\mathbf{x}) = q(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_t, \\mathbf{x})$$\n- It is possible to approximate the expectation term above using Monte-Carlo sampling.\n- In other words, we are guiding $$p_\\theta(\\mathbf{z}_{t-1})$$ with $$q(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_t, \\mathbf{x})$$, where samples $$\\mathbf{x}$$ are drawn from the data, but since the model cannot see $$\\mathbf{x}$$, it will fit $$q(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t})$$.\n\n\n## More Observations\n- The diffusion model\u0027s predictions are always \u0027white noise\u0027. When generating audio examples, this probably lends itself to a whole different set of artifacts than say, transpose convolution or a CNN.\n- While VAEs model output pixels as conditionally independent given $$z$$, this is NOT true of diffusion models when there are multiple steps. This is because the output pixel at one step has some dependency on the output pixels at intermediate steps.\n\nSee math in \"DiffusionMath\"\n\nLast Reviewed: 1/23/25\n\n\nMore Resources I should look at:\n[Sander](https://sander.ai/2024/09/02/spectral-autoregression.html)\n[Sander2](https://sander.ai/2023/07/20/perspectives.html)\n\n{% endraw %}\n"}, {"color": {"background": "#c876a3", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\chapters\\DDPM-Math/", "id": "DDPM - Math", "label": "DDPM - Math", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "# DDPM Math\n{% raw %}\n\n### Main Expression\n$$\n\\log p_\\theta(\\mathbf{x}) = \\log \\int_{\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T}} p_\\theta(\\mathbf{x},\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T} )d\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T}\n$$\n\n$$\n= \\log \\int_{\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T}} \\frac{p_\\theta(\\mathbf{x},\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T} )}{q(\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T}|\\mathbf{x})} q(\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T}|\\mathbf{x})d\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T}\n$$\n\n$$\n\\geq \\int_{\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T}} \\log \\left[ \\frac{p_\\theta(\\mathbf{x},\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T} )}{q(\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T}|\\mathbf{x})} \\right] q(\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T}|\\mathbf{x})d\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T}\n$$\n\n### Focusing on the Numerator in the log\n\nNow,\n\n$$\np_\\theta(\\mathbf{x},\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T} )=p_\\theta(\\mathbf{x}|\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T})p_\\theta(\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T})\n$$\n\nWe have parametrized the decoder such that $$p_\\theta( \\mathbf{x} \n\\mid \\mathbf{z}_1)$$ is conditionally independent from $$\\mathbf{z}_2,\u00e2\u20ac\u00a6.,\\mathbf{z}_T$$. We call this the Markov property of $$p$$. This is:\n\n$$\n= p_\\theta(\\mathbf{x}|\\mathbf{z}_1)p_\\theta(\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T})\n$$\n\nExpanding by using the Chain Rule:\n\n$$\np_\\theta(\\mathbf{x}|\\mathbf{z}_1 )p_\\theta(\\mathbf{z}_1|\\mathbf{z}_{2,\u00e2\u20ac\u00a6,T})p_\\theta(\\mathbf{z}_{2,\u00e2\u20ac\u00a6,T})\n$$\n\nOnce again, using the Markov property of $$p$$:\n\n$$\np_\\theta(\\mathbf{x}|\\mathbf{z}_1 )p_\\theta(\\mathbf{z}_1|\\mathbf{z}_2 )p_\\theta(\\mathbf{z}_{2,\u00e2\u20ac\u00a6,T})\n$$\n\nContinuing, we get \n\n$$\np_\\theta(\\mathbf{x}|\\mathbf{z}_1 )p_\\theta(\\mathbf{z}_1|\\mathbf{z}_2 )p_\\theta(\\mathbf{z}_2|\\mathbf{z}_3 )\\dots p_\\theta(\\mathbf{z}_{T-1}|\\mathbf{z}_T )p_\\theta(\\mathbf{z}_T )\n$$\n\nThus, the probability of taking a particular path from $$\\mathbf{z}_T$$ to $$\\mathbf{x}$$ is given by the above expression. Which is typical, this is just the chain rule applied to Markov chain.\n\n#### One way to put this:\nThe integrals iterate over all possible values of $$ \\mathbf{z}_1, \\dots, \\mathbf{z}_T$$. They are then plugged in for the expression for the joint distribution $$ p_\\theta(\\mathbf{x}, \\mathbf{z}_{1,\\dots,T}) $$ to get a probability value, which accumulates across the loop. $$ p_\\theta(\\mathbf{x}, \\mathbf{z}_{1,\\dots,T}) $$ maps a tuple of values $$ (\\mathbf{x}, \\mathbf{z}_{1,\\dots,T}) $$ to a density value based on the joint distribution.\n\nHowever, given this same tuple, $$ (\\mathbf{x}, \\mathbf{z}_{1,\\dots,T}) $$, we can evaluate the density by evaluating the probability density of the \u00e2\u20ac\u02dcpath\u00e2\u20ac\u2122 that goes from $$ \\mathbf{z}_T $$ to $$ \\mathbf{x} $$, which is this expression:\n\n$$\np_\\theta(\\mathbf{x} | \\mathbf{z}_1) p_\\theta(\\mathbf{z}_1 | \\mathbf{z}_2) p_\\theta(\\mathbf{z}_2 | \\mathbf{z}_3) \\dots p_\\theta(\\mathbf{z}_{T-1} | \\mathbf{z}_T) p_\\theta(\\mathbf{z}_T)\n$$\n\nWhich is the probability that the decoder takes that \u00e2\u20ac\u02dcpath\u00e2\u20ac\u2122 from $$ \\mathbf{z}_T $$ to $$\\mathbf{x}$$.\n\n\n### Focusing on the Denominator in the log\n\nNow consider:\n\n$$\nq(\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T}|\\mathbf{x})\n$$\n\n$$\n= q(\\mathbf{z}_T |\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T-1},\\mathbf{x})q(\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T-1} |\\mathbf{x})\n$$\n\nSince the forward process is a Markov Chain:\n\n$$\nq(\\mathbf{z}_T |\\mathbf{z}_{T-1})q(\\mathbf{z}_{1,\u00e2\u20ac\u00a6,T-1} |\\mathbf{x})\n$$\n\nContinuing:\n\n$$\nq(\\mathbf{z}_T |\\mathbf{z}_{T-1})q(\\mathbf{z}_{T-1}|\\mathbf{z}_{T-2}) \\dots q(\\mathbf{z}_1|\\mathbf{x})\n$$\n\nThus, evaluating $$ q( \\mathbf{z}_{1,\u00e2\u20ac\u00a6,T} \\mid \\mathbf{x})$$ involves starting with $$\\mathbf{x}$$, and evaluating the probability fo the chain of events leading from $$\\mathbf{x}$$ to $$\\mathbf{z}_T$$.\n\n#### Using Bayes\u0027 rule:\n\n$$\nq(\\mathbf{z}_t|\\mathbf{z}_{t-1}) = \n$$\n\n$$\nq(\\mathbf{z}_t | \\mathbf{z}_{t-1},\\mathbf{x}) = \\frac{q(\\mathbf{z}_{t-1} | \\mathbf{z}_t)q(\\mathbf{z}_t | \\mathbf{x})}{q(\\mathbf{z}_{t-1}|\\mathbf{x})}\n$$\n\n\n$$\nq(\\mathbf{z}_t| \\mathbf{z}_{t-1})=q\\left(\\mathbf{z}_t| \\mathbf{z}_{t-1},\\mathbf{x}\\right)=\\frac{q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)q\\left(\\mathbf{z}_t| \\mathbf{x}\\right)}{q\\left(\\mathbf{z}_{t-1}| \\mathbf{x}\\right)}$$\n\nThe first step seems like a hack \u00e2\u20ac\u201c since $$\\mathbf{z}_t$$ conditioned on $$\\mathbf{z}_{t-1}$$ is independent from $$\\mathbf{x}$$, we can add in the extra condition on $$\\mathbf{x}$$ without worrying.\n\nThus,\n\n$$\nq\\left(\\mathbf{z}_T\\middle| \\mathbf{z}_{T-1}\\right)q\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_{T-2}\\right)\\cdots q\\left(\\mathbf{z}_2| \\mathbf{z}_1\\right)q\\left(\\mathbf{z}_1| \\mathbf{x}\\right)\n$$\n\n$$=\\frac{q\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)q\\left(\\mathbf{z}_T| \\mathbf{x}\\right)}{q\\left(\\mathbf{z}_{T-1}| \\mathbf{x}\\right)}\\cdots\\frac{q\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)q\\left(\\mathbf{z}_2| \\mathbf{x}\\right)}{q\\left(\\mathbf{z}_1| \\mathbf{x}\\right)}q\\left(\\mathbf{z}_1| \\mathbf{x}\\right)\n$$\n\n$$\n=q\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)\\cdots q\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)\\cdot\\frac{\\left[q\\left(\\mathbf{z}_T| \\mathbf{x}\\right)\\ q\\left(\\mathbf{z}_{T-1\\ }| \\mathbf{x}\\right)\\cdots q\\left(\\mathbf{z}_2| \\mathbf{x}\\right)q\\left(\\mathbf{z}_1| \\mathbf{x}\\right)\\right]}{q\\left(\\mathbf{z}_{T-1}| \\mathbf{x}\\right)\\cdots q\\left(\\mathbf{z}_1| \\mathbf{x}\\right)}\n$$\n\n$$\n=q\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)\\cdots\\ q\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)\\cdot\\frac{\\left[q\\left(\\mathbf{z}_T| \\mathbf{x}\\right)\\ q\\left(\\mathbf{z}_{T-1\\ }| \\mathbf{x}\\right)\\cdots q\\left(\\mathbf{z}_2| \\mathbf{x}\\right)q\\left(\\mathbf{z}_1| \\mathbf{x}\\right)\\right]}{q\\left(\\mathbf{z}_{T-1}| \\mathbf{x}\\right)\\cdots q\\left(\\mathbf{z}_1| \\mathbf{x}\\right)}\n$$\n\nThings cancel in the fraction:\n\n$$\n=q\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)\\cdots\\ q\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)\\cdot q\\left(\\mathbf{z}_T| \\mathbf{x}\\right)\n$$\n\n### Focusing on the log part\nPutting this together, we have\n\n$$\n\\log{\\left[\\frac{p_\\theta\\left(\\mathbf{x},\\ \\mathbf{z}_{1,\\ldots,T}\\right)}{q\\left(\\mathbf{z}_{1,\\ldots,T} | \\mathbf{x}\\right)}\\right]}=\\ \\log{\\left[\\frac{p_\\theta\\left(\\mathbf{x}| \\mathbf{z}_1\\right)p_\\theta\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)p_\\theta\\left(\\mathbf{z}_2| \\mathbf{z}_3\\right)\\cdots\\ p_\\theta\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)p_\\theta\\left(\\mathbf{z}_T\\right)}{q\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)\\cdots\\ q\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)\\cdot q\\left(\\mathbf{z}_T| \\mathbf{x}\\right)}\\right]}\n$$\n\n$$\n=\\log{\\left[\\frac{p_\\theta\\left(\\mathbf{x}| \\mathbf{z}_1\\right)p_\\theta\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)p_\\theta\\left(\\mathbf{z}_2| \\mathbf{z}_3\\right)\\cdots\\ p_\\theta\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)p_\\theta\\left(\\mathbf{z}_T\\right)}{q\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)\\cdots\\ q\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)\\cdot q\\left(\\mathbf{z}_T| \\mathbf{x}\\right)}\\right]}\n$$\n\n$$\n=\\log{p_\\theta\\left(\\mathbf{x}| \\mathbf{z}_1\\right)}+\\log{\\frac{p_\\theta\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)}{q\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)}+\\cdots}\\ \\log{\\frac{p_\\theta\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)}{q\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)}}+\\log{\\frac{p_\\theta\\left(\\mathbf{z}_T\\right)}{q\\left(\\mathbf{z}_T| \\mathbf{x}\\right)}}\n$$\n\nAssume the last term goes to zero, since the distribution after all the forward diffusion steps should be very similar to $$p_\\theta\\left(\\mathbf{z}_T\\right)=\\mathcal{N}\\left(0,\\mathbf{I}\\right)$$.\n\nWhy is the distribution for $$p_\\theta\\left(\\mathbf{z}_T\\right)=N\\left(0,\\mathbf{I}\\right)?$$ Well, we can choose it to be that way, by making the \u00e2\u20ac\u02dcdecoder\u00e2\u20ac\u2122 evaluate it as such. Or, we can think of p as attempting to fit the \u00e2\u20ac\u02dctrue\u00e2\u20ac\u2122 distribution of the data, which is done approximately in this case by being $$\\mathcal{N}\\left(0,\\mathbf{I}\\right)$$.\n\n$$\\approx\\log{p_\\theta\\left(\\mathbf{x}| \\mathbf{z}_1\\right)}+\\log{\\frac{p_\\theta\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)}{q\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)}+\\cdots}\\ \\log{\\frac{p_\\theta\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)}{q\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)}}$$\n\nLet us put this in the integral:\n\n### Back to Main Expression\n\n$$\n\\int_{\\mathbf{z}_{1,\\ldots,T}}{\\left[\\log{p_\\theta\\left(\\mathbf{x}| \\mathbf{z}_1\\right)}+\\log{\\frac{p_\\theta\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)}{q\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)}+\\cdots}\\ \\log{\\frac{p_\\theta\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)}{q\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)}}\\right]q\\left(\\mathbf{z}_{1,\\ldots,T}| \\mathbf{x}\\right)d\\mathbf{z}_{1,\\ldots,T}}\n$$\n\n$$\n\\int_{\\mathbf{z}_{1,\\ldots,T}}\\log{\\left[p_\\theta\\left(\\mathbf{x}| \\mathbf{z}_1\\right)\\right]}q\\left(\\mathbf{z}_{1,\\ldots,T}| \\mathbf{x}\\right)d\\mathbf{z}_{1,\\ldots,T} + \n$$\n\n$$\n\\int_{\\mathbf{z}_{1,\\ldots,T}}{\\log{\\left[\\frac{p_\\theta\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)}{q\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)}\\right]}q\\left(\\mathbf{z}_{1,\\ldots,T}| \\mathbf{x}\\right)}d\\mathbf{z}_{1,\\ldots,T} + \n$$\n\n$$\n\\cdots+\\int_{\\mathbf{z}_{1,\\ldots,T}}{\\log{\\left[\\frac{p_\\theta\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)}{q\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)}\\right]}q\\left(\\mathbf{z}_{1,\\ldots,T}| \\mathbf{x}\\right)}d\\mathbf{z}_{1,\\ldots,T}\n$$\n\nWe can marginalize out a lot of stuff. Here is an example:\n\n### Single Term\n\n$$\n\\int_{\\mathbf{z}_{1,\\ldots,T}}{\\log{\\left[\\frac{p_\\theta\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)}{q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)}\\right]}q\\left(\\mathbf{z}_{1,\\ldots,T}| \\mathbf{x}\\right)d\\mathbf{z}_{1,\\ldots,T}} \n$$\n\n$$\n= \\int_{\\mathbf{z}_{1,\\ldots,T}}{\\log{\\left[\\frac{p_\\theta\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)}{q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)}\\right]}q\\left(\\mathbf{z}_{t-1},\\mathbf{z}_t| \\mathbf{x}\\right)q\\left(\\mathbf{z}_{1,\\ldots t-2,\\ t+1,T}| \\mathbf{x},\\ \\mathbf{z}_{t-1},\\mathbf{z}_t\\right)}d\\mathbf{z}_{1,\\ldots,T}\n$$\n\n$$\n=\\int_{\\mathbf{z}_{1,\\ldots,T}}{\\log{\\left[\\frac{p_\\theta\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)}{q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)}\\right]}q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t,\\ \\mathbf{x}\\right)q\\left(\\mathbf{z}_t| \\mathbf{x}\\right)q\\left(\\mathbf{z}_{1,\\ldots t-2,\\ t+1,T}| \\mathbf{x},\\ \\mathbf{z}_{t-1},\\mathbf{z}_t\\right)}d\\mathbf{z}_{1,\\ldots,T}\n$$\n\n$$\n=\\int_{\\mathbf{z}_t}\\int_{\\mathbf{z}_{t-1}}\\int_{\\mathbf{z}_{1,\\ldots,t-2,t+1,\\ldots,T}}{\\log{\\left[\\frac{p_\\theta\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)}{q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)}\\right]}q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t,\\ \\mathbf{x}\\right)q\\left(\\mathbf{z}_t| \\mathbf{x}\\right)q\\left(\\mathbf{z}_{1,\\ldots t-2,\\ t+1,T}| \\mathbf{x},\\ \\mathbf{z}_{t-1},\\mathbf{z}_t\\right)}d\\mathbf{z}_{1,\\ldots t-2,\\ t+1,T}d\\mathbf{z}_{t-1}{dz}_t\n$$\n\n$$\n=\\int_{\\mathbf{z}_t}\\int_{\\mathbf{z}_{t-1}}{\\log{\\left[\\frac{p_\\theta\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)}{q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)}\\right]}q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t,\\ \\mathbf{x}\\right)q\\left(\\mathbf{z}_t| \\mathbf{x}\\right)\\left\\{\\int_{\\mathbf{z}_{1,\\ldots,t-2,t+1,\\ldots,T}} q\\left(\\mathbf{z}_{1,\\ldots t-2,\\ t+1,T}| \\mathbf{x},\\ \\mathbf{z}_{t-1},\\mathbf{z}_t\\right)d\\mathbf{z}_{1,\\ldots,t-2,t+1,\\ldots,T}\\right\\}}d\\mathbf{z}_{t-1}{dz}_t\n$$\n\nThe stuff in the brackets goes to 1, since all conditional distributions are still distributions, they integrate to 1 over their input variable(s).\n\n$$\n=\\int_{\\mathbf{z}_t}\\int_{\\mathbf{z}_{t-1}}{\\log{\\left[\\frac{p_\\theta\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)}{q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)}\\right]}q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t,\\ \\mathbf{x}\\right)q\\left(\\mathbf{z}_t| \\mathbf{x}\\right)}d\\mathbf{z}_{t-1}{dz}_t\n$$\n\n$$\n=\\int_{\\mathbf{z}_t}{\\left\\{\\int_{\\mathbf{z}_{t-1}}{\\log{\\left[\\frac{p_\\theta\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)}{q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)}\\right]}q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t,\\ \\mathbf{x}\\right)d\\mathbf{z}_{t-1}}\\right\\}q\\left(\\mathbf{z}_t| \\mathbf{x}\\right)}{dz}_t$$\nThe term in the middle is a KL expression:\n$$=-\\int_{\\mathbf{z}_t}{D_{KL}\\left[q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)\\ || \\ p_\\theta\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)\\right]q\\left(\\mathbf{z}_t| \\mathbf{x}\\right)}{dz}_t\n$$\n\n$$\n= -E_{\\mathbf{z}_t \\sim  q\\left(\\mathbf{z}_t| \\mathbf{x}\\right)}\\left[D_{KL}\\left(q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right) || p_\\theta\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)\\right)\\ \\right]\n$$\n\n### Back to Main Expression\n\n$$\n\\int_{\\mathbf{z}_{1,\\ldots,T}}\\log{\\left[p_\\theta\\left(\\mathbf{x}| \\mathbf{z}_1\\right)\\right]}q\\left(\\mathbf{z}_{1,\\ldots,T}| \\mathbf{x}\\right)d\\mathbf{z}_{1,\\ldots,T}\n$$\n\n$$\n-E_{\\mathbf{z}_2\\sim q\\left(\\mathbf{z}_2| \\mathbf{x}\\right)}\\left[D_{KL}\\left(q\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right) || p_\\theta\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)\\right)\\ \\right] -  \n$$\n\n$$\n\\cdots-E_{\\mathbf{z}_T\\sim q\\left(\\mathbf{z}_T| \\mathbf{x}\\right)}\\left[D_{KL}\\left(q\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right) ||\\ \\ p_\\theta\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)\\right)\\ \\right]\n$$\n\nMarginalizing the first term (won\u00e2\u20ac\u2122t show the whole thing this time):\n\n$$\n\\int_{\\mathbf{z}_1}{\\log{\\left[p_\\theta\\left(\\mathbf{x}| \\mathbf{z}_1\\right)\\right]}q\\left(\\mathbf{z}_1| \\mathbf{x}\\right)d\\mathbf{z}_1}\n$$\n\n$$\n-E_{\\mathbf{z}_2\\sim q\\left(\\mathbf{z}_2| \\mathbf{x}\\right)}\\left[D_{KL}\\left(q\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right) ||\\ \\ p_\\theta\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)\\right)\\ \\right] - \\cdots -\n$$\n\n$$\nE_{\\mathbf{z}_T\\sim q\\left(\\mathbf{z}_T| \\mathbf{x}\\right)}\\left[D_{KL}\\left(q\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right) ||\\ \\ p_\\theta\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)\\right)\\ \\right]\n$$\n\nTurning the first term into an expectation:\n\n$$\n=E_{\\mathbf{z}_1\\sim q\\left(\\mathbf{z}_1| \\mathbf{x}\\right)}\\left[\\log p_\\theta \\left(\\mathbf{x}| \\mathbf{z}_1\\right)\\right] - \n$$\n\n$$\nE_{\\mathbf{z}_2\\sim q\\left(\\mathbf{z}_2| \\mathbf{x}\\right)}\\left[D_{KL}\\left(q\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right) ||\\ \\ p_\\theta\\left(\\mathbf{z}_1| \\mathbf{z}_2\\right)\\right)\\ \\right] - \n$$\n\n$$\n\\cdots - E_{\\mathbf{z}_T\\sim q\\left(\\mathbf{z}_T| \\mathbf{x}\\right)}\\left[D_{KL}\\left(q\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right) ||\\ \\ p_\\theta\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)\\right)\\ \\right]\n$$\n\n### Focusing on the Later KL Terms\nAccording to our parametrization, we have\n\n$$\np_\\theta\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)=N\\left(g\\left(\\mathbf{z}_t\\right),\\sigma_t^2I\\right)\n$$\n\nAnd we have from other computations that:\n\n$$\nq\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t,\\mathbf{x}\\right)=\nN_{\\mathbf{z}_{t-1}}\\left(\\frac{\\left(1-\\alpha_{t-1}\\right)}{1-\\alpha_t}\\sqrt{1-\\beta_t}\\ \\mathbf{z}_t+\\frac{\\sqrt{\\alpha_{t-1}}\\beta_t}{1-\\alpha_t}\\mathbf{x},\\frac{\\beta_t\\left(1-\\alpha_{t-1}\\right)}{1-\\alpha_t}I\\right)\n$$\n\nThus, since we are computing the KL between two Gaussian distributions, we can actually compute the KL divergence here in closed form:\n\n$$\nD_{KL}\\left(q\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right) ||\\ p_\\theta\\left(\\mathbf{z}_{T-1}| \\mathbf{z}_T\\right)\\right)=\n\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\frac{\\left(1-\\alpha_{t-1}\\right)}{1-\\alpha_t}\\sqrt{1-\\beta_t}\\ \\mathbf{z}_t+\\frac{\\sqrt{\\alpha_{t-1}}\\beta_t}{1-\\alpha_t}\\mathbf{x}- \\mathbf{f}_\\theta\\left(\\mathbf{z}_t\\right)\\right|\\right|^2+C\n$$\n\nWhich is proportional to the squared difference between the means.\n\n### First Term:\n\n$$\nE_{\\mathbf{z}_1\\sim q\\left(\\mathbf{z}_1| \\mathbf{x}\\right)}\\left[\\log p_\\theta(\\mathbf{x}| \\mathbf{z}_1)\\right]=E_{\\mathbf{z}_1\\sim q\\left(\\mathbf{z}_1| \\mathbf{x}\\right)}\\left[-\\frac{\\left(\\mathbf{x}- \\mathbf{f}_\\theta\\left(\\mathbf{z}_1\\right)\\right)^2}{2\\sigma}_1^2\\right]\n$$\n\n### Back to Main Expression\n\n$$\n= E_{\\mathbf{z}_1\\sim q\\left(\\mathbf{z}_1| \\mathbf{x}\\right)}\\left[\\log p_\\theta(\\mathbf{x}| \\mathbf{z}_1)\\right]-\\sum_{t=2}^{t=T}{E_{\\mathbf{z}_t\\sim q\\left(\\mathbf{z}_t| \\mathbf{x}\\right)}\\left[D_{KL}\\left(q\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right) ||\\ \\ p_\\theta\\left(\\mathbf{z}_{t-1}| \\mathbf{z}_t\\right)\\right)\\ \\right]}\n$$\n\n$$\n=E_{\\mathbf{z}_1\\sim q\\left(\\mathbf{z}_1| \\mathbf{x}\\right)}\\left[-\\frac{\\left(\\mathbf{x}- \\mathbf{f}_\\theta\\left(\\mathbf{z}_1\\right)\\right)^2}{{2\\sigma}_1^2}\\right]-\\ \\sum_{t=2}^{t=T}{E_{\\mathbf{z}_t\\sim q\\left(\\mathbf{z}_t| \\mathbf{x}\\right)}\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\frac{\\left(1-\\alpha_{t-1}\\right)}{1-\\alpha_t}\\sqrt{1-\\beta_t}\\ \\mathbf{z}_t+\\frac{\\sqrt{\\alpha_{t-1}}\\beta_t}{1-\\alpha_t}\\mathbf{x}- \\mathbf{f}_\\theta\\left(\\mathbf{z}_t\\right)\\right|\\right|^2}\n$$\n\nMaximizing this means minimizing this:\n\n$$\n=E_{\\mathbf{z}_1\\sim q\\left(\\mathbf{z}_1| \\mathbf{x}\\right)}\\left[\\frac{\\left(\\mathbf{x}- \\mathbf{f}_\\theta\\left(\\mathbf{z}_1\\right)\\right)^2}{{2\\sigma}_1^2}\\right]+\\ \\sum_{t=2}^{t=T}{E_{\\mathbf{z}_t\\sim q\\left(\\mathbf{z}_t| \\mathbf{x}\\right)}\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\frac{\\left(1-\\alpha_{t-1}\\right)}{1-\\alpha_t}\\sqrt{1-\\beta_t}\\ \\mathbf{z}_t+\\frac{\\sqrt{\\alpha_{t-1}}\\beta_t}{1-\\alpha_t}\\mathbf{x}- \\mathbf{f}_\\theta\\left(\\mathbf{z}_t\\right)\\right|\\right|^2}\n$$\n\nWe can imagine minimizing this term regarding a specific $$\\mathbf{x}$$ by sampling $$\\mathbf{z}_1,\\ldots,\\mathbf{z}_{T}$$ from $$\\mathbf{x}$$, computing the expression, and changing the parameters of $$\\mathbf{f}$$. We can see this more clearly by adding stuff to the expectations.\n\n$$\n=E_{\\mathbf{z}_{1,\\ldots,T}\\sim q\\left(\\mathbf{z}_{1,\\ldots,T}| \\mathbf{x}\\right)}\\left[\\frac{\\left(\\mathbf{x}- \\mathbf{f}_\\theta\\left(\\mathbf{z}_1\\right)\\right)^2}{{2\\sigma}_1^2}\\right]+\\ \\sum_{t=2}^{t=T}{E_{\\mathbf{z}_{1,\\ldots,T}\\sim q\\left(\\mathbf{z}_{1,\\ldots,T}| \\mathbf{x}\\right)}\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\frac{\\left(1-\\alpha_{t-1}\\right)}{1-\\alpha_t}\\sqrt{1-\\beta_t}\\ \\mathbf{z}_t+\\frac{\\sqrt{\\alpha_{t-1}}\\beta_t}{1-\\alpha_t}\\mathbf{x}- \\mathbf{f}_\\theta\\left(\\mathbf{z}_t\\right)\\right|\\right|^2}\n$$\n\n$$\n=E_{\\mathbf{z}_{1,\\ldots,T}\\sim q\\left(\\mathbf{z}_{1,\\ldots,T}| \\mathbf{x}\\right)}\\left[\\frac{\\left(\\mathbf{x}- \\mathbf{f}_\\theta\\left(\\mathbf{z}_1\\right)\\right)^2}{{2\\sigma}_1^2}\\right]+\\ E_{\\mathbf{z}_{1,\\ldots,T}\\sim q\\left(\\mathbf{z}_{1,\\ldots,T}| \\mathbf{x}\\right)}\\left[\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\frac{\\left(1-\\alpha_{t-1}\\right)}{1-\\alpha_t}\\sqrt{1-\\beta_t}\\ \\mathbf{z}_t+\\frac{\\sqrt{\\alpha_{t-1}}\\beta_t}{1-\\alpha_t}\\mathbf{x}- \\mathbf{f}_\\theta\\left(\\mathbf{z}_t\\right)\\right|\\right|^2}\\right]\n$$\n\n$$\n=\\ E_{\\mathbf{z}_{1,\\ldots,T}\\sim q\\left(\\mathbf{z}_{1,\\ldots,T}| \\mathbf{x}\\right)}\\left[\\frac{\\left(\\mathbf{x}- \\mathbf{f}_\\theta\\left(\\mathbf{z}_1\\right)\\right)^2}{{2\\sigma}_1^2}\\right]+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\frac{\\left(1-\\alpha_{t-1}\\right)}{1-\\alpha_t}\\sqrt{1-\\beta_t}\\ \\mathbf{z}_t+\\frac{\\sqrt{\\alpha_{t-1}}\\beta_t}{1-\\alpha_t}\\mathbf{x}- \\mathbf{f}_\\theta\\left(\\mathbf{z}_t\\right)\\right|\\right|^2}\n$$\n\nNow using a Monte Carlo estimate:\n\n$$\n=\\ \\sum_{i=1}^{N}\\left[\\frac{\\left(\\mathbf{x}^{\\left(i\\right)}- \\mathbf{f}_\\theta\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\frac{\\left(1-\\alpha_{t-1}\\right)}{1-\\alpha_t}\\sqrt{1-\\beta_t}\\ \\mathbf{z}_t^{\\left(i\\right)}+\\frac{\\sqrt{\\alpha_{t-1}}\\beta_t}{1-\\alpha_t}\\mathbf{x}- \\mathbf{f}_\\theta\\left(\\mathbf{z}_t^{\\left(i\\right)}\\right)\\right|\\right|^2}\\right]\n$$\n\nThe loss function minimizes the difference between the estimated mean $$\\mathbf{f}(\\mathbf{z}_t)$$ of $$\\mathbf{z}_{t-1}$$, and the most likely value (mean) it took, given $$\\mathbf{z}_t$$ and $$\\mathbf{x}$$.\n{% endraw %}"}, {"color": {"background": "#c876a3", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\chapters\\DDPM-Reparametrization/", "id": "DDPM - Reparametrization", "label": "DDPM - Reparametrization", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "# DDPM - Reparametrization\n{% raw %}\n\n### Expressing $\\mathbf{z}_t$ in terms of noise\n\nNote that\n$$\n\\mathbf{z}_t=\\sqrt{\\alpha_t }\\ \\mathbf{x}+\\sqrt{1-\\alpha_t}\\ \\mathbf{\\epsilon}\n$$\nWe can rearrange to write $\\mathbf{x}$ as:\n$$\n\\mathbf{x}=\\frac{\\mathbf{z}_t}{\\sqrt{\\alpha_t}}-\\frac{\\sqrt{1-\\alpha_t}}{\\sqrt{\\alpha_t}}\\mathbf{\\epsilon}\n$$\nAnd we substitute this into our objective.\n\nFirst, we denote $\\mathbf{\\epsilon}_t^{(i)}$ as the noise added to data sample $i$ at time step $t$, to get $\\mathbf{z}_t$ (when we are using the diffusion kernel, which is going straight from $\\mathbf{x}$ to $\\mathbf{z}_t$)\n\n$$\n\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{\\mathbf{z}_1^{\\left(i\\right)}}{\\sqrt{\\alpha_1}}-\\frac{\\sqrt{1-\\alpha_1}}{\\sqrt{\\alpha_1}}\\mathbf{\\epsilon}_1^{(i)}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\frac{\\left(1-\\alpha_{t-1}\\right)}{1-\\alpha_t}\\sqrt{1-\\beta_t}\\ \\mathbf{z}_t^{\\left(i\\right)}+\\frac{\\sqrt{\\alpha_{t-1}}\\beta_t}{1-\\alpha_t}\\left(\\frac{\\mathbf{z}_t^{(i)}}{\\sqrt{\\alpha_t}}-\\frac{\\sqrt{1-\\alpha_t}\\mathbf{\\epsilon}_t^{(i)}}{\\sqrt{\\alpha_t}}\\right)-f\\left(\\mathbf{z}_t^{\\left(i\\right)}\\right)\\right|\\right|^2}\\right]\n$$\n\n$$\n\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{\\alpha_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{1-\\alpha_1}}{\\sqrt{\\alpha_1}}\\mathbf{\\epsilon}_1^{(i)}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\frac{1}{(1-\\alpha_t)}\\left(\\left(1-\\alpha_{t-1}\\right){\\sqrt{1-\\beta_t}}_\\ +\\frac{\\sqrt{\\alpha_{t-1}}\\beta_t}{\\sqrt{\\alpha_t}}\\ \\right)\\mathbf{z}_t^{\\left(i\\right)}-\\frac{\\sqrt{\\alpha_{t-1}}\\beta_t\\sqrt{1-\\alpha_t}}{(1-\\alpha_t)\\sqrt{\\alpha_t}}\\mathbf{\\epsilon}_t^{(i)}-f\\left(\\mathbf{z}_t^{\\left(i\\right)}\\right)\\right|\\right|^2}\\right]\n$$\n\nNote that $\\alpha_t=\\alpha_{t-1}\\left(1-\\beta_t\\right)$, so $\\frac{\\alpha_{t-1}}{\\alpha_t}=\\frac{1}{(1-\\beta_t)}$\n\n$$\n=\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{\\alpha_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{1-\\alpha_1}}{\\sqrt{\\alpha_1}}\\mathbf{\\epsilon}_1^{(i)}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\frac{1}{(1-\\alpha_t)}\\left(\\left(1-\\alpha_{t-1}\\right){\\sqrt{1-\\beta_t}}_\\ +\\frac{\\beta_t}{\\sqrt{1-\\beta_t}}\\ \\right)\\mathbf{z}_t^{\\left(i\\right)}-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}\\mathbf{\\epsilon}_t^{(i)}-f\\left(\\mathbf{z}_t^{\\left(i\\right)}\\right)\\right|\\right|^2}\\right]\n$$\n\n$$=\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{\\alpha_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{1-\\alpha_1}}{\\sqrt{\\alpha_1}}\\mathbf{\\epsilon}_1^{(i)}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\left(\\frac{\\left(1-\\alpha_{t-1}\\right)}{(1-\\alpha_t)}{\\sqrt{1-\\beta_t}}_\\ +\\frac{\\beta_t}{(1-\\alpha_t)\\sqrt{1-\\beta_t}}\\ \\right)\\mathbf{z}_t^{\\left(i\\right)}-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}\\mathbf{\\epsilon}_t^{(i)}-f\\left(\\mathbf{z}_t^{\\left(i\\right)}\\right)\\right|\\right|^2}\\right]\n$$\n\n$$\n=\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{\\alpha_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{1-\\alpha_1}}{\\sqrt{\\alpha_1}}\\mathbf{\\epsilon}_1^{(i)}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\left(\\frac{\\left(1-\\alpha_{t-1}\\right)\\sqrt{1-\\beta_t}}{(1-\\alpha_t)\\sqrt{1-\\beta_t}}{\\sqrt{1-\\beta_t}}_\\ +\\frac{\\beta_t}{(1-\\alpha_t)\\sqrt{1-\\beta_t}}\\ \\right)\\mathbf{z}_t^{\\left(i\\right)}-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}\\mathbf{\\epsilon}_t^{(i)}-f\\left(\\mathbf{z}_t^{\\left(i\\right)}\\right)\\right|\\right|^2}\\right]\n$$\n\n$$\n=\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{\\alpha_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{1-\\alpha_1}}{\\sqrt{\\alpha_1}}\\mathbf{\\epsilon}_1^{(i)}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\left(\\frac{\\left(1-\\alpha_{t-1}\\right)(1-\\beta_t)}{(1-\\alpha_t)\\sqrt{1-\\beta_t}}+\\frac{\\beta_t}{(1-\\alpha_t)\\sqrt{1-\\beta_t}}\\ \\right)\\mathbf{z}_t^{\\left(i\\right)}-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}\\mathbf{\\epsilon}_t^{(i)}-f\\left(\\mathbf{z}_t^{\\left(i\\right)}\\right)\\right|\\right|^2}\\right]\n$$\n\n$$\n=\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{\\alpha_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{1-\\alpha_1}}{\\sqrt{\\alpha_1}}\\mathbf{\\epsilon}_1^{(i)}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\left(\\frac{\\left(1-\\alpha_{t-1}\\right)\\left(1-\\beta_t\\right)+\\beta_t}{(1-\\alpha_t)\\sqrt{1-\\beta_t}}\\ \\right)\\mathbf{z}_t^{\\left(i\\right)}-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}\\mathbf{\\epsilon}_t^{(i)}-f\\left(\\mathbf{z}_t^{\\left(i\\right)}\\right)\\right|\\right|^2}\\right]\n$$\n\n$$\n=\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{\\alpha_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{1-\\alpha_1}}{\\sqrt{\\alpha_1}}\\mathbf{\\epsilon}_1^{(i)}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\left(\\frac{\\left(1-\\alpha_{t-1}+\\alpha_{t-1}\\beta_t\\right)}{(1-\\alpha_t)\\sqrt{1-\\beta_t}}\\ \\right)\\mathbf{z}_t^{\\left(i\\right)}-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}\\mathbf{\\epsilon}_t^{(i)}-f\\left(\\mathbf{z}_t^{\\left(i\\right)}\\right)\\right|\\right|^2}\\right]\n$$\n\n$$\n=\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{\\alpha_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{1-\\alpha_1}}{\\sqrt{\\alpha_1}}\\mathbf{\\epsilon}_1^{(i)}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\left(\\frac{\\left(1-\\alpha_{t-1}\\right)\\left(1-\\beta_t\\right)}{(1-\\alpha_t)\\sqrt{1-\\beta_t}}\\ \\right)\\mathbf{z}_t^{\\left(i\\right)}-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}\\mathbf{\\epsilon}_t^{(i)}-f\\left(\\mathbf{z}_t^{\\left(i\\right)}\\right)\\right|\\right|^2}\\right]\n$$\n\n$$\n=\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{\\alpha_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{1-\\alpha_1}}{\\sqrt{\\alpha_1}}\\mathbf{\\epsilon}_1^{(i)}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\left(\\frac{\\left(1-\\alpha_t\\right)}{(1-\\alpha_t)\\sqrt{1-\\beta_t}}\\ \\right)\\mathbf{z}_t^{\\left(i\\right)}-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}\\mathbf{\\epsilon}_t^{(i)}-f\\left(\\mathbf{z}_t^{\\left(i\\right)}\\right)\\right|\\right|^2}\\right]\n$$\n\n$$\n=\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{\\alpha_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{1-\\alpha_1}}{\\sqrt{\\alpha_1}}\\mathbf{\\epsilon}_1^{(i)}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\left(\\frac{1}{\\sqrt{1-\\beta_t}}\\ \\right)\\mathbf{z}_t^{\\left(i\\right)}-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}\\mathbf{\\epsilon}_t^{(i)}-f\\left(\\mathbf{z}_t^{\\left(i\\right)}\\right)\\right|\\right|^2}\\right]\n$$\n\n$$\n=\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{1-\\beta_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{{\\beta}_1}}{\\sqrt{1-{\\beta}_1}}\\mathbf{\\epsilon}_1^{(i)}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\left(\\frac{1}{\\sqrt{1-\\beta_t}}\\ \\right)\\mathbf{z}_t^{\\left(i\\right)}-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}\\mathbf{\\epsilon}_t^{(i)}-f\\left(\\mathbf{z}_t^{\\left(i\\right)}\\right)\\right|\\right|^2}\\right]\n$$\n\nObserve that if we multiply the numerator and denominator by $\\sqrt{\\beta_1}$ in coefficient on $\\mathbf{\\epsilon}_1$, the form of the first half of the sum matches the second half.\n\n### Defining $\\mathbf{g}$, our neural network\nNow $\\mathbf{f}$, is our model, so we parametrize it however we want. It\u00e2\u20ac\u2122s just a formula we use with some parameters, with $\\mathbf{z}_t^{(i)}$ as input. So, let \n\n$$\n\\mathbf{f}\\left(\\mathbf{z}_t^{\\left(i\\right)}\\right)=\\ \\left(\\frac{1}{\\sqrt{1-\\beta_t}}\\ \\right)\\mathbf{z}_t^{\\left(i\\right)}-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}g(\\mathbf{z}_t^{\\left(i\\right)})\n$$\n\nWhich we are totally allowed to do, since it\u00e2\u20ac\u2122s just another function of $\\mathbf{z}_t^{(i)}$.\nThen we get\n\n$$\n\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{1-\\beta_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{{\\beta}_1}}{\\sqrt{1-{\\beta}_1}}\\mathbf{\\epsilon}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|\\left(\\frac{1}{\\sqrt{1-\\beta_t}}\\ \\right)\\mathbf{z}_t^{\\left(i\\right)}-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}\\mathbf{\\epsilon}_t^{(i)}-\\ \\left(\\frac{1}{\\sqrt{1-\\beta_t}}\\ \\right)\\mathbf{z}_t^{\\left(i\\right)}+\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}g(\\mathbf{z}_t^{\\left(i\\right)})\\right|\\right|^2}\\right]\n$$\n\n$$\n\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{1-\\beta_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{{\\beta}_1}}{\\sqrt{1-{\\beta}_1}}\\mathbf{\\epsilon}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}\\mathbf{\\epsilon}_t^{(i)}+\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}g(\\mathbf{z}_t^{\\left(i\\right)})\\right|\\right|^2}\\right]\n$$\n\n$$\n\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{1-\\beta_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{{\\beta}_1}}{\\sqrt{1-{\\beta}_1}}\\mathbf{\\epsilon}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{1}{2\\sigma_t^2}\\ \\left|\\left|-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}\\left(\\mathbf{\\epsilon}_t^{(i)}-g\\left(\\mathbf{z}_t^{\\left(i\\right)}\\right)\\right)\\right|\\right|^2}\\right]\n$$\n\n$$\n\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{1-\\beta_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{{\\beta}_1}}{\\sqrt{1-{\\beta}_1}}\\mathbf{\\epsilon}-f\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{\\beta_t^2}{2\\sigma_t^2(1-\\alpha_t)(1-\\beta_t)}\\ \\left|\\left|\\mathbf{\\epsilon}_t^{(i)}-g(\\mathbf{z}_t^{\\left(i\\right)})\\right|\\right|^2}\\right]\n$$\n\n$$\n\\sum_{i=1}^{N}\\left[\\frac{\\left(\\frac{1}{\\sqrt{1-\\beta_1}}\\mathbf{z}_1^{\\left(i\\right)}-\\frac{\\sqrt{{\\beta}_1}}{\\sqrt{1-{\\beta}_1}}\\mathbf{\\epsilon}-\\left(\\frac{1}{\\sqrt{1-\\beta_1}}\\ \\right)\\mathbf{z}_t^{\\left(i\\right)}+\\frac{\\beta_1}{\\sqrt{1-\\alpha_1}\\sqrt{1-\\beta_1}}g(\\mathbf{z}_1^{\\left(i\\right)})\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{\\beta_t^2}{2\\sigma_t^2(1-\\alpha_t)(1-\\beta_t)}\\ \\left|\\left|\\mathbf{\\epsilon}_t^{(i)}-g(\\mathbf{z}_t^{\\left(i\\right)})\\right|\\right|^2}\\right]\n$$\n\n$$\n\\sum_{i=1}^{N}\\left[\\frac{\\left(-\\frac{\\sqrt{{\\beta}_1}}{\\sqrt{1-{\\beta}_1}}\\mathbf{\\epsilon}+\\frac{\\beta_1}{\\sqrt{1-\\alpha_1}\\sqrt{1-\\beta_1}}g(\\mathbf{z}_1^{\\left(i\\right)})\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{\\beta_t^2}{2\\sigma_t^2(1-\\alpha_t)(1-\\beta_t)}\\ \\left|\\left|\\mathbf{\\epsilon}_t^{(i)}-g(\\mathbf{z}_t^{\\left(i\\right)})\\right|\\right|^2}\\right]\n$$\n\n$$\n\\sum_{i=1}^{N}\\left[\\frac{\\left(-\\frac{{\\beta}_1}{\\sqrt{{\\beta}_1}\\sqrt{1-{\\beta}_1}}\\mathbf{\\epsilon}+\\frac{\\beta_1}{\\sqrt{\\beta_1}\\sqrt{1-\\beta_1}}g(\\mathbf{z}_1^{\\left(i\\right)})\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{\\beta_t^2}{2\\sigma_t^2(1-\\alpha_t)(1-\\beta_t)}\\ \\left|\\left|\\mathbf{\\epsilon}_t^{(i)}-g(\\mathbf{z}_t^{\\left(i\\right)})\\right|\\right|^2}\\right]\n$$\n\n$$\n\\sum_{i=1}^{N}\\left[\\frac{\\left(-\\frac{{\\beta}_1}{\\sqrt{{\\beta}_1}\\sqrt{1-{\\beta}_1}}\\left[\\mathbf{\\epsilon}-g\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right]\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{\\beta_t^2}{2\\sigma_t^2(1-\\alpha_t)(1-\\beta_t)}\\ \\left|\\left|\\mathbf{\\epsilon}_t^{(i)}-g(\\mathbf{z}_t^{\\left(i\\right)})\\right|\\right|^2}\\right]\n$$\n\n$$\n\\sum_{i=1}^{N}\\left[\\frac{\\frac{{\\beta}_1^2}{{\\beta}_1(1-{\\beta}_1)}\\left(\\left[\\mathbf{\\epsilon}-g\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right]\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{\\beta_t^2}{2\\sigma_t^2(1-\\alpha_t)(1-\\beta_t)}\\ \\left|\\left|\\mathbf{\\epsilon}_t^{(i)}-g(\\mathbf{z}_t^{\\left(i\\right)})\\right|\\right|^2}\\right]\n$$\n\n$$\n\\sum_{i=1}^{N}\\left[\\frac{\\frac{{\\beta}_1^2}{(1-{\\alpha}_1)(1-{\\beta}_1)}\\left(\\left[\\mathbf{\\epsilon}-g\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right]\\right)^2}{{2\\sigma}_1^2}+\\sum_{t=2}^{t=T}{\\frac{\\beta_t^2}{2\\sigma_t^2(1-\\alpha_t)(1-\\beta_t)}\\ \\left|\\left|\\mathbf{\\epsilon}_t^{(i)}-g(\\mathbf{z}_t^{\\left(i\\right)})\\right|\\right|^2}\\right]\n$$\n\n$$\n\\sum_{i=1}^{N}\\left[\\frac{{\\beta}_1^2}{{2\\sigma}_1^2(1-{\\alpha}_1)(1-{\\beta}_1)}\\left(\\left[\\mathbf{\\epsilon}-g\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right]\\right)^2+\\sum_{t=2}^{t=T}{\\frac{\\beta_t^2}{2\\sigma_t^2(1-\\alpha_t)(1-\\beta_t)}\\ \\left|\\left|\\mathbf{\\epsilon}_t^{(i)}-g(\\mathbf{z}_t^{\\left(i\\right)})\\right|\\right|^2}\\right]\n$$\n\n$$\n=\\sum_{i=1}^{N}\\left[\\frac{{\\beta}_1^2}{{2\\sigma}_1^2(1-{\\alpha}_1)(1-{\\beta}_1)}\\left(\\left[\\mathbf{\\epsilon}-g\\left(\\mathbf{z}_1^{\\left(i\\right)}\\right)\\right]\\right)^2+\\sum_{t=2}^{t=T}{\\frac{\\beta_t^2}{2\\sigma_t^2(1-\\alpha_t)(1-\\beta_t)}\\ \\left|\\left|\\mathbf{\\epsilon}_t^{(i)}-g(\\mathbf{z}_t^{\\left(i\\right)})\\right|\\right|^2}\\right]\n$$\n\n$$\n=\\sum_{i=1}^{N}\\left[\\sum_{t=1}^{t=T}{\\frac{\\beta_t^2}{2\\sigma_t^2(1-\\alpha_t)(1-\\beta_t)}\\ \\left|\\left|\\mathbf{\\epsilon}_t^{(i)}-g(\\mathbf{z}_t^{\\left(i\\right)})\\right|\\right|^2}\\right]\n$$\n\nThis is our objective.\nThe constants out front can be used to reweight the objective based on the time-step. In NCSN and DDPM, we have the same weighting for all time steps. Thus, we ignore the constants out front and simply it further:\n\n$$\n\\sum_{i=1}^{N}\\left[\\sum_{t=1}^{t=T}{\\ \\left|\\left|\\mathbf{\\epsilon}_t^{(i)}-g(\\mathbf{z}_t^{\\left(i\\right)})\\right|\\right|^2}\\right]\n$$\n\nSo we are just predicting the (unscaled) noise added. Another way to write it:\n\n$$\n\\sum_{{i}=1}^{N}\\left[\\sum_{{t}=1}^{{t}={T}}{\\ \\left|\\left|{\\mathbf{\\epsilon}}_{t}^{({i})}-{g}({\\mathbf{\\mathbf{x}}}_{i}\\sqrt{{\\alpha}_{t}}+{\\mathbf{\\epsilon}}_{t}^{\\left({i}\\right)}\\sqrt{1-{\\alpha}_{t}})\\right|\\right|^2}\\right]\n$$\n\n### Thus, to train a diffusion model:\n- For all data:\n -  For all time steps:\n\t- Generating a sample according to the diffusion kernel\n\t- Try to predict epsilon (the noise added pre-normalization), using MSE loss.\n\nOr, stochastically:\n\n- For a batch of data:\n\t- Generate random time step $t$\n\t- Generate noise $\\mathbf{\\epsilon} \\sim \\mathcal{N}(0,I)$\n\t- Optimize  $\\left\\|\\mathbf{\\epsilon}-\\ \\mathbf{g}(\\mathbf{x}_i\\sqrt{\\alpha_t}+\\mathbf{\\epsilon}\\sqrt{1-\\alpha_t}) \\right\\|^2$\n\nFor inference:\n- Sample from $\\mathcal{N}(0,I)$ to get $\\mathbf{z}_T$\n- Compute f:\n $f\\left(\\mathbf{z}_t\\right)=\\ \\left(\\frac{1}{\\sqrt{1-\\beta_t}}\\ \\right)\\mathbf{z}_t-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}\\sqrt{1-\\beta_t}}g(\\mathbf{z}_t)$\n- Sample $\\mathbf{z}_{t-1}$ from\n\t\t$\\mathcal{N}\\left(f\\left(\\mathbf{z}_t\\right),\\ \\sigma_t\\right)$\n\tEventually, we compute $\\mathbf{f}(\\mathbf{z}_1)$, which is our data sample.\n\tSigmas here are predetermined. In practice, I use the standard deviation of \n\t$$\n\tq(\\mathbf{z_{t-1} \\mid \\mathbf{z}_t, x})\n\t$$\n\twhich is \n\t$$\n\t\\sqrt{\\beta_t\\frac{1 - \\alpha_{t-1}}{1 - \\alpha_t}}\n\t$$\n\n{% endraw %}"}, {"color": {"background": "#c876a3", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\DDPM-Noise-Schedules/", "id": "DDPM - Noise Schedules", "label": "DDPM - Noise Schedules", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# DDPM Noise Schedules\n\nNoise schedules often are such that the coefficient on the noise squared plus the coefficient on the data squared equals one.\n\nThis is sort of \u0027variance preserving\u0027.\n\nLinear schedules have linearly increasing betas, and concentrate a lot on high noise levels.\n\nPolynomial have betas proportional to some polynomial function of $t$.\n\nCosine schedule hvae betas whose increase looks like a 1 - cosine function in the first quadrature.\n\nNichol and Dhariwal (Diffusion beats GANs) used another fancy cosine schedule that increases the noise level more linear-like\n\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Diffusion-Best-Practices/", "id": "Diffusion Best Practices", "label": "Diffusion Best Practices", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Diffusion Best Practices\n\n\n- Training loss is a hard metric\n- Normalize data to [-1, 1]\n- small UNet = 10s of millions, large = 100s of millions\n- use spatial attention\n- Monitor:\n    - Training loss\n    - Valid loss\n    - sample quality\n    - gradient norm\n- Maintain an EMA of weights\n- Use warmup"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Diffusion-Forcing/", "id": "Diffusion Forcing", "label": "Diffusion Forcing", "mass": 0.98, "shape": "dot", "size": 9.899494936611665, "title": "# Diffusion Forcing\n\nSee notes on paper\n"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Diffusion-Beats-GANs/", "id": "Diffusion Beats GANs", "label": "Diffusion Beats GANs", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# Diffusion Models Beat GANs on Image Synthesis\n\n## Imagenet 64 x 64\n- 296M params\n- Dataset: 1.2 million images\n- 192 base channels\n- 1, 2, 3, 4 channel multiplication\n- 64 channels per head\n- 32, 16, 8 attention resolutions\n\n\n- 540k iterations\n- batch size 2048\n- 3e-4 learning rate\n\n\n## Imagenet 256\n- 554M params\n- Dataset: 540k images \n- 256 base channels\n- 1,1,2,2,4,4 channel multiplication\n- 64 channels per head\n- 32, 16, 8 attention resolutions\n\n- 1980K iterations, or 1.9 M iterations\n- batch size 256.\n- 1e-4 learning rate"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\History-Guidance/", "id": "History Guidance", "label": "History Guidance", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# History Guidance\n\nVideo Diffusion\nWant to condition basd on history\n\nhard to condition on varaible length input\n\nan order of magnitude less compute than industry\n\nTemporal History Guidance - combines scores from different history windows\n\nFractional History Guidance - corrupts history windows with noise\n\nCombine HG-t and HG-f for history guidance across time and frequency\n\nCompose score from multiple conditioning"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\DiT/", "id": "DiT", "label": "DiT", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# DiT\n\n"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Edify-Image/", "id": "Edify Image", "label": "Edify Image", "mass": 0.04, "shape": "dot", "size": 5, "title": "# Edify - Image\n"}, {"color": "#ffd900", "fixed": false, "font": {"color": "white"}, "id": "Understanding Diffusion Models: A Unified Perspective", "label": "Understanding Diffusion Models: A Unified Perspective", "mass": 4.0, "shape": "dot", "size": 20.0, "title": ""}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Score-Based-Generative-Models/", "id": "Score Based Generative Models", "label": "Score Based Generative Models", "mass": 2.25, "shape": "dot", "size": 15.0, "title": "# Score-Based Generative Models\n{% raw %}\n\n## Motivation\n\nGiven datapoints $\\mathbf{x_1}, \\ldots , \\mathbf{x}_N$ independently[^1] drawn from an underlying distribution $p(\\mathbf{x})$, we would like to model the data distribution $p(\\mathbf{x})$. \n\n[^1]: Note that datapoints are assumed to be independent, i.e., the generation of a datapoint $\\mathbf{x_i}$ does not influence the generation of another datapoint $\\mathbf{x_j}$.\n\n\nWe can parametrize the disribution as $p_\\theta(\\mathbf{x})$, and try maximizing the probability of observing all data points, which is\n\n$$\np_\\theta(\\mathbf{x_1}, \\ldots , \\mathbf{x}_N) = \\prod_{i=1}^N p_\\theta(\\mathbf{x_i}).\n$$\n\n\nThis is equivalent to maximizing the log probability:\n\n$$\n\\log p_\\theta(\\mathbf{x_1}, \\ldots , \\mathbf{x}_N) = \\sum_{i=1}^N \\log(p_\\theta(\\mathbf{x_i}))\n$$\n\nIf we do not require $p_\\theta(\\mathbf{x})$ to be a valid probabilty density function (that integrates to $1$), the values of $p_\\theta(\\mathbf{x})$ will become arbitrarily large (and can also do so even if we impose the constraint that it integrates to $1$, depending on the parametrization). However, ensuring this density function integrates to one is typically not tractable.\n\n\n## Definition\n\nThe score function is the gradient of the log of the probability density function, or\n\n$$\n\\nabla_\\mathbf{x} \\log(p(\\mathbf{x}))\n$$\n\n![Score function of a mixture of 2 Gaussians](images/MOG_Score.png)\n\n*Score function of a mixture of 2 Gaussians*\n\n\n\n## Avoiding the Normalizing Constant\nModeling the score function does not require knowing the normalizing constant that results in $p(\\mathbf{x})$ integrating to 1.\n\nFor instance, we can model any probability distribution as \n$$\n\\frac{e^{-f_\\theta(\\mathbf{x})}}{Z_\\theta}\n$$\nWhere $Z_\\theta$ is a normalizing constant, which depends on $\\theta$.\n\nTaking the log of this PDF results in\n\n$$\n\\log{e^{-f_\\theta(\\mathbf{x})}} - \\log{Z_\\theta} = \n-f_\\theta(\\mathbf{x})- \\log{Z_\\theta}\n$$\n\nAnd taking the gradient of this results in the second term vanishing:\n\n$$\n-\\nabla_\\mathbf{x} f_\\theta(\\mathbf{x}) - \\nabla_\\mathbf{x} \\log{Z_\\theta} =\n-\\nabla_\\mathbf{x} f_\\theta(\\mathbf{x}) = s_\\theta (\\mathbf{x})\n$$\n\nThe normalizing constant imposes restrictive architectural choices (like invertible layers in normalizing flows) or discretization (like outputting a softmax distribution). However, fitting $s_\\theta$ does not require considering the normalizing constant, which allows for more flexibility. In fact, $s_\\theta(\\mathbf{x})$ only has the constraint that it should have correct input and output dimensionality, namely, both should equal the dimensionality of $\\mathbf{x}$.\n\n## Optimization\nWe can attempt to fit the score function by minimizing \n\n$$\nE_{p(\\mathbf{x})}\\left[ \\lVert \\nabla_\\mathbf{x} \\log(p_\\theta(\\mathbf{x})) - s_\\theta (\\mathbf{x}) \\rVert^2 _2 \\right]\n$$\n\nHow do we know the ground truth score function? There is a technique called **score matching**.\n\n## Langevin Sampling\nLangevin sampling allows you to sample from a distribution given its score function. First, we draw from an arbitrary distribution:\n\n$$\nx_0 \\sim \\pi(\\mathbf{x})\n$$\n\nThen we iteratively perform \"noisy gradient ascent\":\n\n$$\nx_{i+1} = x_{i} + \\epsilon \\nabla_x \\log (p(\\mathbf{x})) + \\sqrt{2 \\epsilon} z_i, \\quad i = 0, \\ldots, K\n$$\n\nWhere $z_i \\sim \\mathcal{N}(0,I)$.\n\nAs the step size $\\epsilon \\rightarrow 0$, and the number of steps $K \\rightarrow \\infty$, this will result in a sample from $p(\\mathbf{x})$.\n\n![Using Langevin Dynamics to sample from a mixture of 2 Gaussians](images/langevin_sampling.gif)\n\n*Using Langevin Dynamics to sample from a mixture of 2 Gaussians.*\n\nInstead of using the ground-truth score function, we can plug in our estimate for the score function in order to generate samples.\n\n### Additional Thoughts\nUsing the log allows for *larger* absolute scores near the edges of the Gaussian, since the numbers are smaller there, so the logarithm is large. Thus, scores are larger the further away we are from the mean of the Gaussian, which is what we want, since we want to jump further.\n\n## Adding Noise\n### Problems with Naive Langevin Sampling\n\nModeling the score function naively is difficult, because our estimates of the score function are inaccurate in low-density regions.\n\nNote that\n\n$$\n\\mathbb{E}_{p(\\mathbf{x})}\\left[ \\lVert \\nabla_\\mathbf{x} \\log(p_\\theta(\\mathbf{x})) - s_\\theta (\\mathbf{x}) \\rVert^2 _2 \\right] =\n\\int p(\\mathbf{x}) \\lVert \\nabla_\\mathbf{x} \\log(p_\\theta(\\mathbf{x})) - s_\\theta (\\mathbf{x}) \\rVert^2 _2  d \\mathbf{x}\n$$\n\nSo the distance between the score and our estimate are downweighted in regions where $p(\\mathbf{x})$ is small (low density regions). (In practice, the expectation is computed by sampling real data, so the score estimate is inaccurate where we have little data).\n\nThis complicates Langevin sampling, since $\\mathbf{x}_0$ (the first sampling step) usually starts in a low-density region according to the data distribution (it is usually just noise).\n\n\n### Additional Thoughts\n\n#### Adding two random variables\n1. Recall from [Brad Osgood\u0027s course](https://see.stanford.edu/course/ee261) that adding two random variables $Z = X + Y$ results in a distribution that is a *convolution* of the distributions of $X$ and $Y$.\n\n2. By adding noise to our data, we are essentially *smoothing* the data distribution with a *gaussian kernel*.\n\n3. This smoothing can be thought of as *increasing the support* of the distribution. Think about it - if we add lots of noise, almost anything can arise with some probability.\n\n4. In addition, we can represent our data distribution as a set of discrete samples, or impulse functions at each datapoint. The sifting theorem means that the noisy data distribution can be represented as a **mixture of Gaussians**.\n\n#### Score function of a Gaussian\n\n1. Note that the score function of a Gaussian is linear:\n\n$$\n\\nabla \\log \\left[\\exp\\frac{-(x-\\mu)^2 }{\\sigma^2} \\right] = \\frac{-2(x - \\mu)}{\\sigma^2}\n$$\n\n2. In fact, if we consider the Gaussian distribution as originating from a data point at $\\mu$ with added noise $n \\sim \\mathcal{N}(0,\\sigma^2)$, then we have that a sample $x$ from the Gaussian is:\n\n$$\nx = \\mu + \\sigma n\n$$\n\nAnd the score function is \n\n$$\n\\frac{-2(x - \\mu)}{\\sigma^2}\n = \\frac{-2(\\mu + \\sigma n - \\mu)}{\\sigma^2}\n = \\frac{-2n}{\\sigma}\n$$\n\nThat means that the score function is proportional to the noise added!\n\n#### Score function of Mixture of Gaussians\n1. The score function of a *mixture of gaussians* is approximately piecewise linear. It would be exactly piecewise linear if our smoothed distribution was piece-wise Gaussian. In a mixture of Gaussians, there is some spillover from other Gaussians everywhere.\n\n2. Since ReLU neural networks output piecewise linear functions, they are great for modeling score functions. (This was from Mert\u0027s class)\n\n## Improved Score Matching\nAdding lots of noise corrupts the data distribution substantially, while adding low levels of noise may not result in enough smoothing or coverage. We can choose $L$ levels of noise:\n\n$$\n\\sigma_1 \u003c \\sigma_2 \u003c \\cdots \u003c\\sigma_L.\n$$\n\nWe can compute the noisy distributions:\n\n$$\np_{\\sigma_i}(\\mathbf{x}) = \\int p(\\mathbf{y}) N_\\mathbf{x}(\\mathbf{y}, \\sigma_i^2 I) d \\mathbf{y}\n$$\n\n\n### Additional Thoughts\n#### Interpreting the Integral\nWe can view computing this integral like this:\n1. Iterate through all possible data examples $\\mathbf{y}$\n2. Compute the probability that $\\mathbf{x}$ occurs under a normal distribution centered at $\\mathbf{y}$.\n3. Sum across all possible values of $\\mathbf{y}$, weighted by the data distribution $p(\\mathbf{y})$.\n\n#### Expectation\nThis can also be viewed as an expectation:\n\n$$\n\\int p(\\mathbf{y}) N_\\mathbf{x}(\\mathbf{y}, \\sigma_i I) d \\mathbf{y} = \\mathbb{E}_{y \\sim p(\\mathbf{y})}[N_\\mathbf{x}(\\mathbf{y}, \\sigma_i^2 I)]\n$$\n\nOr, when we sample $\\mathbf{y}$ from our data distribution $p(\\mathbf{y})$, what is the expected density of a normal distribution centered at $\\mathbf{y}$?\n\n#### Convolution\nNote that this can be seen as the convolution between two probability distributions:\n\n$$\np_{\\sigma_i}(\\mathbf{x}) = \\int p(\\mathbf{y}) q(\\mathbf{x - y}) d \\mathbf{y}\n$$\n\nWhere \n\n$$\nq(\\mathbf{z}) = N_\\mathbf{z}(0, \\sigma_i I)\n$$\n\n\n#### Flipped Convolution\nSince convolution is commutative, there are two ways to express a convolution, and we can also express it as \n\n$$\np_{\\sigma_i}(\\mathbf{x}) = \\int p(\\mathbf{y}) q(\\mathbf{x - y}) d \\mathbf{y} = \\int q(\\mathbf{y}) p(\\mathbf{x - y}) d \\mathbf{y}\n$$\n\n\nIn this case, we integrate over all possible values of the normal distribution $q$, and evaluate $ \\mathbf{x - y}$ (or equivalently $\\mathbf{x + y}$, since adding noise subtracting noise do the same thing) according to the data distribution.\n\nBasically, there\u0027s a different way to \"add up\" to $\\mathbf{x + y}$, one for each value of $\\mathbf{y}$ sampled from a normal distribution. We\u0027re essentially accumulating across all these possible ways to \"add up\" to $\\mathbf{x}$. We can also express this as an expectation:\n\n$$\n\\mathbb{E_{\\mathbf{y} \\sim q(\\mathbf{y})}}[p(\\mathbf{x - y})]\n$$\n\nWhich is saying, draw a sample (noise) from the normal distribution, and evaluate the probability of the data distribution at the point where example that results from *subtracting* that noise.\n\nIn other words, $\\mathbf{y}$ is the noise added to the data example $\\mathbf{x}$, and we are evaluating $p(\\mathbf{x})$ according to the data distribuition, but we are aggretating over all possible noises that could have been added, which is $\\mathbf{y}$.\n\nWe can also think of this as fixing the noise vector, and asking, \"What is the probability density if the noise is this?\" Then we accumulate over all possible noise vectors.\n\n#### End Additional Thoughts\nDrawing samples from $p_{\\sigma_i}(\\mathbf{x})$ is easy, we can just draw a sample $x \\sim p(\\mathbf{x})$ from our dataset and add noise to get $\\mathbf{x} + \\sigma_i \\mathbf{z}$.\n\nWe can fit a neural network to the score function of each noisy distribution:\n\n$$\ns_\\theta (\\mathbf{x}, i) \\approx \\nabla_\\mathbf{x} \\log p_{\\sigma_i}(\\mathbf{x}), \\quad \\forall i = 1,\\dots,L\n$$\n\nThe training objective is:\n\n$$\n\\sum_{i=1}^L \\lambda(i) \\mathbb{E}_{\\mathbf{x} \\sim p_{\\sigma_i}}[\\lVert \\nabla_\\mathbf{x} \\log(p_{\\sigma_i}(\\mathbf{x})) - s_\\theta (\\mathbf{x}, i) \\rVert^2 _2]\n$$\n\nUsually, the loss weighting is $\\lambda(i) = \\sigma_i^2$. This would mean higher noise levels have a greater loss.\n\nThe ground truth score estimates are usually $\\frac{-\\mathbf{z}}{\\sigma}$\n\nWe can run Langevin dynamics in sequence for each noise level. This means runnning Langevin chains for all noise levels.\n\n\n#### Recommendations\nWe can choose the noise levels in a geometric progression (there is a common ratio). $\\sigma_L$ can be the maximum pairwise distance between two datapoints, and $L$ can be hundreds or thousands.\n\n## Next steps\nAs $L \\rightarrow \\infty$, the noise level becomes a continuous-time stochastic process, where noise is added. This will allow for generative modeling using [SDEs](Generative-Modeling-Using-SDEs.md). \n\nLast Reviewed: 2/4/25\n\n{% endraw %}\n"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Generative-Modeling-Using-SDEs/", "id": "Generative Modeling Using SDEs", "label": "Generative Modeling Using SDEs", "mass": 1.2500000000000002, "shape": "dot", "size": 11.180339887498949, "title": "# Generative Modeling Using SDEs\n{% raw %}\n\n## Forward-Time Diffusion\n\nFor a noise-conditional score network, there is a finite number of noise levels.\n\nAs the number of diffusion timesteps approaches infinity ($$L \\rightarrow \\infty$$), the noise level becomes a continuous-time stochastic process, where noise is added continuously.\n\n\nWe have:\n\n$$\nd\\mathbf{x} = \\mathbf{f}(\\mathbf{x}, t) dt + g(t)d\\mathbf{w}\n$$\n\n$$d\\mathbf{w}$$ can be viewed as infinitesimal white noise, and characterizes a [Wiener Process](Wiener-Process.md).\n\n$$\\mathbf{f}$$ is a vector-valued function representing the *drift coefficient* and $$g(t)$$ is a scalar-valued function called the *diffusion coefficient*.\n\nWe can imagine applying this diffusion process to our data. As $$t$$ changes, the distribution of our data changes. We can use $$p_t(\\mathbf{x})$$ to denote the probability distribution of our data at diffusion time $$t$$, where $$t \\in [0,T]$$. The data distribution (with no diffusion) is $$p_0(\\mathbf{x})$$.\n\nIn addition, we can let $$\\mathbf{X}_t$$ be a random variable representing the value of a datapoint at diffusion time $$t$$, and use $$\\mathbf{x}_t$$ to represent a realization of $$\\mathbf{X}_t$$.\n\nWe choose our diffusion process so that $$p_T$$ does not depend on $$p_0$$.\n\n#### Additional Thoughts\n- In DDPMs, the *drift coefficient* $$\\mathbf{f}(\\mathbf{x}, t)$$ would be analogous to $$\\sqrt{1-\\beta_t}$$ when defining the forward process. It is kind of like the derivative of the coefficient on $$\\mathbf{x}_0$$.\n- in DDPMs, the diffusion coefficient is analogous to $$\\sqrt{\\beta_t}$$. It is kind of like the derivative of $$\\sigma_i$$ or the derivative of the coefficent on the noise term in DDPMs. It represents the variance of the noise added at each step.\n\n\n### Choices of $$\\mathbf{f}(\\mathbf{x},t), g(t)$$\n\nWe can choose the diffusion and drift coefficients as hyperparameters. For instance, we can choose a forward diffusion process:\n\n$$\nd\\mathbf{x} = e^t d\\mathbf{w}\n$$\nWhich means that the variance grows exponentially as time increases, and is similar to choosing a geometric progression of noise scales $$\\sigma_1, \\ldots, \\sigma_L$$. For instance, the Variance Exploding SDE, the Variance Preserving SDE, and the sub-VP SDE work well for generating images.\n\n## Reverse-Time Diffusion\n\nThe reverse diffusion process is defined as:\n\n$$\nd\\mathbf{x} = \\left[\\mathbf{f}(\\mathbf{x},t) - g^2(t)\\nabla_\\mathbf{x} \\log(p_t(\\mathbf{x}))\\right] dt + g(t)d\\mathbf{\\bar{w}}\n$$\n\nIf we know the drift and diffusion coefficients (which are from the forward process), and can approximate the score function (the gradient of the data distributions at time $$t$$), then we can compute the reverse-time SDE.\n\nWe can sample from any marginal distribution $$p_t$$ (determined by the forward process) by sampling from $$p_T$$ (which should be doable, since this should approximate an easy-to-sample from distribution), then by integrating from $$T$$ to $$t$$. By integrating all the way to $$t=0$$, we should be able to generate a data sample.\n\n#### Additional Thoughts\n- Note that $$dt$$ represents an infinitesimal *negative* time step, since we are integrating from higher $$t$$ to lower $$t$$.\n- **What is a Reverse-Time SDE?** In this case, $$\\mathbf{\\bar{w}}$$ is described as \u0027reverse-time brownian motion\u0027. In practice, $$d\\mathbf{\\bar{w}}$$ is Gaussian Noise, just like $$d\\mathbf{w}$$ in the forward process.\n\n\n\n\n## Learning the Score Function\nWe would like to fit the score function such that\n\n$$\n\\mathbf{s}_\\theta(\\mathbf{x}, t) \\approx \\nabla_\\mathbf{x} \\log p_{t} (\\mathbf{x})\n$$\n\nThis should be a continuous weighted sum of:\n\n$$\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\mathbb{E}_{p_t(\\mathbf{x})}\n\\left[\n\\lambda(t)\n\\lVert\n\\nabla_\\mathbf{x} \\log p_{t} (\\mathbf{x}) -\n\\mathbf{s}_\\theta(\\mathbf{x}, t)  \\rVert^2_2\n\\right]\n$$\n\nWhere $$\\lambda(t)$$ is a weighting function. We would like to balance the losses across time, so we choose\n\n$$\n\\lambda(t) \\propto 1/\n\\mathbb{E}_{p_t(\\mathbf{x})}\n\\lVert\n\\nabla_{\\mathbf{x}_t}\n\\log p_{t} (\\mathbf{x}_t \\mid \\mathbf{x}_0) \\rVert^2_2\n$$\n\nTo implement this weighting, we also rescale the outputs of the network, so its outputs are on the same scale across time-steps.\n\nIn addition, we use the exponential moving average of the weights during sampling.\n\n#### Additional Thoughts:\n- The expectation here is proportional to the expected amount of noise added. \n\n- Note that in DDPM, the network is always predicting samples from $$\\mathcal{N}(0, I)$$ no matter the timestep, which is similar to rescaling the outputs of the network in DDIM.\n\n\nWe can use denoising score matching to optimize this objective.\n\n### Actual Optimization\nWe have\n\n$$\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\mathbb{E}_{p_t(\\mathbf{x})}\n\\left[\n\\lambda(t)\n\\lVert\n\\nabla_\\mathbf{x} \\log p_{\\sigma_t} (\\mathbf{x}) -\n\\mathbf{s}_\\theta(\\mathbf{x}, t)  \\rVert^2_2\n\\right]\n$$\n\nWe can estimate this objective as \n\n$$\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\lambda(t)\n\\left[\n\\mathbb{E}_{\\mathbf{x_0} \\sim p_0(\\mathbf{x})}\n\\mathbb{E}_{\\mathbf{x_t} p_t(\\mathbf{x_t} \\mid \\mathbf{x_0})}\n\\left[\n\\lVert\n\\nabla_\\mathbf{x} \\log p_{t} (\\mathbf{x_t} \\mid \\mathbf{x}_0) -\n\\mathbf{s}_\\theta(\\mathbf{x_t}, t)  \\rVert^2_2\n\\right]\n\\right]$$\n\n#### Additional Thoughts\n- We would like the score function to fit $$\\nabla_\\mathbf{x_t} \\log p_{t} (\\mathbf{x_t})$$, but this distribution may be complex and multimodal. However, $$\\log p_{t} (\\mathbf{x_t} \\mid \\mathbf{x}_0)$$ is just Gaussian.\n\n- Since the score function we are fitting does not see $$\\mathbf{x_0}$$, it will fit an expectation over all possible $$\\mathbf{x_0}$$, which will approximate the score function of $$p_t(\\mathbf{x_t})$$.\n- The probability distribution of $$p_t(\\mathbf{x_t})$$ is a mixture of Gaussians, which means that the conditional distribution $$\\log p_{t} (\\mathbf{x_t} \\mid \\mathbf{x}_0)$$ locally approximates $$\\log p_{t} (\\mathbf{x_t})$$ at the point $$\\mathbf{x_t}$$.\n\n### A derivation that both objectives are equivalent\nOur intended objective is:\n\n$$\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\mathbb{E}_{\\mathbf{x} \\sim p_t(\\mathbf{x})}\n\\left[\n\\lambda(t)\n\\lVert\n\\nabla_\\mathbf{x} \\log p_{\\sigma_t} (\\mathbf{x}) -\n\\mathbf{s}_\\theta(\\mathbf{x}, t)  \\rVert^2_2\n\\right]\n$$\n\nWe would like to show this objective is equivalent:\n\n$$\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\lambda(t)\n\\left[\n\\mathbb{E}_{\\mathbf{x_0} \\sim p_0(\\mathbf{x})}\n\\mathbb{E}_{\\mathbf{x_t} \\sim p_t(\\mathbf{x_t} \\mid \\mathbf{x_0})}\n\\left[\n\\lVert\n\\nabla_\\mathbf{x} \\log p_{t} (\\mathbf{x_t} \\mid \\mathbf{x}_0) -\n\\mathbf{s}_\\theta(\\mathbf{x_t}, t)  \\rVert^2_2\n\\right]\n\\right]\n$$\n\n\n#### Lemma\n\nWe first show that we can estimate the marginal distribution as an expecation of the conditionals:\n\n$$\n\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x_t}) = \\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0 \\mid \\mathbf{x_t})} \\left[\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0})\n\\right]\n$$\n\n\u003cspan style=\"color:blue\"\u003eTo Do: Prove this Lemma\u003c/span\u003e.\n\nIn other words, we can approximate the marginal score function by sampling from the posterior, and aggregating the score esimates that we can from single samples.\n\n#### Bias Variance Decomposition\nBy the law of total variance, we have:\n\n$$\n\\mathbb{E} \\| Z - a \\|^2_2 =  \\| \\mathbb{E}[Z] - a \\|^2_2 + \\mathbb{E} \\left[ \\| Z - \\mathbb{E}[Z] \\|^2_2 \\right] \\\\[10pt]\n= \\| \\mathbb{E}[Z] - a \\|^2_2 + \\operatorname{Var}(Z)\n$$\n\nFor any random variable $$Z$$.\n\n\u003cspan style=\"color:blue\"\u003eTo Do: Prove this Decomposition\u003c/span\u003e.\n\n#### Another Equality\nConsider the new expression\n\n$$\n\\tag{*}\n\\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0 \\mid \\mathbf{x_t})} \\left[\n\\left\\lVert\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 \\right]\n$$\n\nBy the bias-variance decomposition, this is equal to\n\n$$\n\\left\\lVert\n\\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0 \\mid \\mathbf{x_t})} \\left[\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0})\n\\right] - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 +\n\\operatorname{Var}\\left[\\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0})\\right] = \\\\[10pt]\n\\left\\lVert\n\\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0 \\mid \\mathbf{x_t})} \\left[\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0})\n\\right] - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 + C\n$$\n\nWhere we are use $$C$$ to denote the variance expression, which is constant given the forward process and does not depend on model parameters $$\\theta$$. By the Lemma, this is equal to\n\n$$\n\\tag{**}\n\\left\\lVert\n\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x_t}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 + C \n$$\n\nWhich is our score matching objective for a single timestep, plus a constant.\n\n#### Wrapping in expectation\nLet us wrap both starred expressions (which we have proven to be equal) in an expectation over $$\\mathbf{x}_t$$:\n\n$$\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t)}\n\\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0 \\mid \\mathbf{x_t})} \\left[\n\\left\\lVert\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 \\right] = \\\\[10pt]\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t)}\\left[\n\\left\\lVert\n\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x_t}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 + C \\right]\n$$\n\nBy using the chain rule:\n\n$$\n\\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0)}\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t \\mid \\mathbf{x}_0)}\n\\left[\n\\left\\lVert\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 \\right] = \\\\[10pt]\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t)}\\left[\n\\left\\lVert\n\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x_t}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 + C \\right]\n$$\n\nIf we wrap both in another expectation over $$t$$, and multiply by $$\\lambda(t)$$, we get\n\n$$\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\lambda(t)\n\\left[\n\\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0)}\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t \\mid \\mathbf{x}_0)}\n\\left[\n\\left\\lVert\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 \\right]\\right] = \\\\[10pt]\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\lambda(t)\n\\left[\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t)}\\left[\n\\left\\lVert\n\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x_t}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 + C \\right] \\right]\n$$\n\nThe $$C$$ term can be taken out of the inner expectation, and remains constant with repsect to $$\\theta$$ when it is taken outside of the outer expecation:\n\n$$\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\lambda(t)\n\\left[\n\\mathbb{E}_{\\mathbf{x}_0\\sim p(\\mathbf{x}_0)}\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t \\mid \\mathbf{x}_0)}\n\\left[\n\\left\\lVert\n    \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x_t} \\mid \\mathbf{x_0}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 \\right]\\right] = \\\\[10pt]\n\\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\lambda(t)\n\\left[\n\\mathbb{E}_{\\mathbf{x}_t\\sim p(\\mathbf{x}_t)}\\left[\n\\left\\lVert\n\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x_t}) - s_\\theta(\\mathbf{x}_t, t) \\right\\rVert^2_2 \\right] \\right] + C_2\n$$\n\nThe left hand side is the objective that we train with, and the right hand side is the optimization objective. Thus, we have that the two optimization objectives are equivalent - they differ by a constant that does not depend on our parameters $$\\theta$$.\n\n### \u0027Likelihood\u0027 Weighting\n\nIf $$\\lambda(t)$$ = $$g^2(t)$$, we have that\n\n$$\nD_{\\text{KL}}\\left(p_0(\\mathbf{x}) \\right) \\| p_\\theta(\\mathbf{x})) = \\frac{T}{2} \\mathbb{E}_{t \\sim \\mathcal{U}(0,T)}\n\\mathbb{E}_{\\mathbf{x} \\sim p_t(\\mathbf{x})}\n\\left[\n\\lambda(t)\n\\lVert\n\\nabla_\\mathbf{x} \\log p_{\\sigma_t} (\\mathbf{x}) - \n\\mathbf{s}_\\theta(\\mathbf{x}, t)  \\rVert^2_2\n\\right] + \\\\ D_{\\text{KL}}\\left(p_T\\left(\\mathbf{x}_t\\right) \\| \\pi \\left(\\mathbf{x}_T\\right) \\right)\n$$\n\nWhere $$p_\\theta(\\mathbf{x})$$ is the distribution we get for $$\\mathbf{x}_0$$ we get using our estimated score function, and $$\\pi$$ is a simple distribution that it is easy to sample from.\n\n\n## Solving the Reverse-Time SDE\n\n### The Integral\nTo generate samples, we can evaluate the reverse-time SDE. This is as simple as coming up with a numerical approximation to the integral of the reverse-time SDE process:\n\n$$\nd\\mathbf{x} = \\left[\\mathbf{f}(\\mathbf{x},t) - g^2(t)\\nabla_\\mathbf{x} \\log(p_t(\\mathbf{x}))\\right] dt + g(t)d\\mathbf{\\bar{w}}\n$$\n\n##### Additional Notes\n\nWe can express our SDE in integral form:\n\n$$\n\\mathbf{x_t} = \\mathbf{x}_{t_0} + \\int_{t_0}^t \\left[\\mathbf{f}(\\mathbf{x},s) - g^2(s)\\nabla_\\mathbf{x} \\log(p_s(\\mathbf{x}))\\right] ds + \\int_{t_0}^t g(s)d\\mathbf{\\bar{w}}(s)\n$$\n\nWhere\n\n$$\nd\\mathbf{\\bar{w}}(s) = \\mathcal{N}(0, ds) ds\n$$\n\n\u003cspan style=\"color:blue\"\u003eQuestion - can we get here by intergrating both sides of the SDE? If so, how?\u003c/span\u003e.\n\nParticularly, to get a sample from the marginal distribution, we can evaluate\n\n$$\n\\mathbf{x}_t = \\mathbf{x}_T + \\int_{T}^t \\left[\\mathbf{f}(\\mathbf{x},s) - g^2(s)\\nabla_\\mathbf{x} \\log(p_s(\\mathbf{x}))\\right] ds + \\int_{T}^t g(s)d\\mathbf{\\bar{w}}(s)\n$$\n\nSince we can sample $$\\mathbf{x}_T$$ in closed form.\n\nFinally, to generate a sample from the data distribution, we are interested in $$\\mathbf{x}_0$$. This be obtained by setting $$t=0$$ in the above equation.\n\n### Numerical Methods\nChoose a small $$\\Delta t \u003c 0$$.\n\nThen perform this process:\n\n1. $$\\mathbf{x}_T \\sim \\pi$$\n\n2. $$\n\\mathbf{x}_{t + \\Delta t} = \\mathbf{x}_t + \\left[ f(\\mathbf{x}_t,t) - g^2(t) s_\\theta(\\mathbf{x}_t, t) \\right] \\Delta t + g(t)\\mathbf{z}_t\\sqrt{| \\Delta t |}$$\n\nWhere $$\\mathbf{z}_t \\sim \\mathcal{N}(0, I)$$. \n\n##### Additional Notes\n-Note that the last term on the right side is a way to sample from $$\\mathcal{N}(0, | \\Delta t |)$$.\n\n-This is similar to Langevin Sampling or \"noisy gradient descent\" for when time was not continuous.\n\n#### Predictor-Corrector Methods\nThe predictor chooses a step size $$\\Delta t$$, then predicts $$\\mathbf{x}_{t + \\Delta t}$$ based on $$\\mathbf{x}_{t}$$. Then, we run several \u0027corrector\u0027 steps to improve our sample from $$p_{t + \\Delta t}$$, using our estimate of its score function $$\\mathbf{s}_\\theta(\\mathbf{x}_{t + \\Delta t}, t + \\Delta t)$$. We do this before moving on to the next time step.\n\nThis is similar to the discrete-time case, where we optimize the score at each noise level before moving to the next noise level.\n\n## ODE Version\n\nWe can change the SDE into an ODE without changing its marginal distributions $$p_t(\\mathbf{x}_t)$$. The formula is this:\n\n$$\nd\\mathbf{x} = \\left[f(\\mathbf{x}, t) - \\frac{1}{2} g^2(t)\\nabla_\\mathbf{x} \\log p_t(\\mathbf{x}_t) \\right] dt\n$$\n\nODE trajectories are actually smoother, and this process is invertible. We can directly convert $$\\mathbf{x}_0$$ to $$\\mathbf{x}_T$$ and convert it back. \n\nThis is a special case of a continuous time neural ODE and a normalizing flow, and allows for exact likelihood computation.\n\n## Inverse Problems\nSuppose we have a known forward process $$p(\\mathbf{y} \\mid \\mathbf{x})$$, and we would like to know $$p(\\mathbf{x} \\mid \\mathbf{y})$$.\n\nBayes\u0027 rule tells us\n\n$$\np(\\mathbf{x} \\mid \\mathbf{y}) = \\frac{p(\\mathbf{y} \\mid \\mathbf{x})p(\\mathbf{x})}{p(\\mathbf{y})}\n$$\n\nBy taking the gradient with respect to x of the log of both sides, we get a Bayes\u0027 rule for score functions:\n\n$$\n\\nabla_\\mathbf{x} \\log p(\\mathbf{x} \\mid \\mathbf{y}) = \\nabla_\\mathbf{x} \\log p(\\mathbf{y} \\mid \\mathbf{x}) + \\nabla_\\mathbf{x} \\log p(\\mathbf{x})\n$$\n\nNote that the denominator disappears since it does not depend on $$\\mathbf{x}$$. Both terms in the right side of the above expression can be found, the first is given by the known forward process, and the second is the score function of our unconditional distribution. This will let us sample from $$p(\\mathbf{x} \\mid \\mathbf{y})$$.\n\nFor instance, we can apply score-based models to the task of image inpainting. The masked image is given, as $$p(\\mathbf{y})$$, and the forward process is also given (how to mask the image).\n\n\n\u003cspan style=\"color:blue\"\u003eI am pretty sure $$\\nabla_\\mathbf{x} \\log p(\\mathbf{y} \\mid \\mathbf{x}) $$ is zero everywhere, but I suppose maximizing that will just be ensuring that the non-masked region stays the same.\n\u003c/span\u003e\n\n## Connection to DDPM\nOnce I finish my page on DDPM, you will be able to see that the ELBO objective that DDPM uses is the same as a the score-matching objective.\n\nLast Reviewed: 2/4/25\n\n{% endraw %}\n"}, {"color": {"background": "#ee802c", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Wiener-Process/", "id": "Wiener Process", "label": "Wiener Process", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Wiener Process\n\n{% raw %}\n\n## Standard Wiener Process\n\n\n### Intuition\nA Wiener process (also called Brownian motion) is a continuous time process that is like a random walk. \n\nImagine a discrete-time random process where you start at position $\\mathbf{x_{t=0}} = \\mathbf{0}$. Then, your position at time $t$ is determined by\n\n$$\n\\mathbf{x}_{t} = \\mathbf{x}_{t-\\Delta t} + \\mathcal{N}(0,\\Delta t I)\n$$\n\nOr equivalently,\n\n$$\n\\mathbf{x}_{t} = \\mathbf{x}_{t-\\Delta t} + \\sqrt{\\Delta t} \\cdot \\mathcal{N}(0,I)\n$$\n\nIn other words, every time step, you change your position by a vector sampled from $\\mathcal{N}(0,\\Delta t I)$, where $\\Delta t$ is how long each time step is.\n\nA Wiener process is the continuous limit of this as $\\Delta t \\rightarrow 0$.\n\n### Definition\nWe define a set of independent random variables, or a function mapping the time $t$ to a random variable. It satisfies the property that\n\n$$\nW_0 = \\mathbf{0}\n$$\n\nAnd \n\n$$\nW_{t_2} - W_{t_1} = \\mathcal{N}(0, (t_2 - t_1)I)\n$$\n\nNote that this restriction is only possible because the variance of the sum of two independent random variables is the sum of the two variances. In other words, since $W_{t_1} \\perp W_{t_2}$ for all $t_1 \\neq t_2 $, the variance accumulates linearly over time.\n\n\nAlso, under this formulation, we have\n\n$$\nW_{t} = \\mathcal{N}(0, tI)\n$$\n\n\n\nlinearly as $t$ increases.\n\nIn other words, at every time step, we take a infinitesimally small step in a random direction proportional to a vector sampled from the standard normal:\n\n$$\ndW \\sim \\mathcal{N}(0, I dt)\n$$\n\nLast Reviewed: 2/4/25\n{% endraw %}\n"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Classifier-Free-Guidance/", "id": "Classifier Free Guidance", "label": "Classifier Free Guidance", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Classifier Free Guidance\n\nBy Bayes\u0027 Rule:\n$$\np(\\mathbf{z | c}) = \\frac{p(\\mathbf{c | z})p(\\mathbf{z})}{p(\\mathbf{c)}}\n$$\n$$\n\\log p(\\mathbf{z | c}) = \\log p(\\mathbf{c | z}) + \\log p(\\mathbf{z}) - \\log p(\\mathbf{c)}\n$$\nTaking the gradient with respect to $\\mathbf{z}$, the third term is constant in $\\mathbf{z}$ and goes to zero.\n$$\n\\nabla_z \\log p(\\mathbf{z | c}) = \\nabla_z \\log p(\\mathbf{c | z}) + \\nabla_z \\log p(\\mathbf{z})\n$$\n\n\n\n$$\n\\nabla_z \\log p(\\mathbf{z | c}) = \\color{red} \\gamma \\color{black} \\nabla_z \\log p(\\mathbf{c | z}) + \\nabla_z \\log p(\\mathbf{z})\n$$\n\n\n\nSince we don\u0027t have a classifier, computing \n\n$$\n\\nabla_z \\log p(\\mathbf{c | z})\n$$\nIs not possible. But, we have \n$$\n\\nabla_z \\log p(\\mathbf{c | z}) = \n\\nabla_z \\log p(\\mathbf{z | c}) - \\nabla_z \\log p(\\mathbf{z})\n$$\nSubstituting in:\n$$\n\\nabla_z \\log p(\\mathbf{z | c}) = \\color{red} \\gamma \\color{black} \\nabla_z \\log p(\\mathbf{c | z}) + \\nabla_z \\log p(\\mathbf{z})\n$$\n$$\n\\nabla_z \\log p(\\mathbf{z | c}) = \n\\nabla_z \\log p(\\mathbf{z}) + \\color{red} \\gamma \\color{black} \\left(\\nabla_z \\log p(\\mathbf{z | c}) - \\nabla_z \\log p(\\mathbf{z})\\right) \n$$\nThe above is \\textbf{one} way people write CFG. We can also rewrite the formula this way:\n$$\n\\nabla_z \\log p(\\mathbf{z | c}) = \n(1 - \\gamma)\n\\nabla_z \\log p(\\mathbf{z}) + \\gamma \\color{black} \\nabla_z \\log p(\\mathbf{z | c}) \n$$\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\DDIM/", "id": "DDIM", "label": "DDIM", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# DDIM\n\n## Overview\nGeneralize DDPMs from Markovian to non-Markovian forward processes. The training objective is actually the same.\nThis improves:\n- Generated Quality\n- Consistency property - if we generate using a different number of steps, we get similar high-level features.\n- Semantically meaningful image interpolation via latent variable interpolation.\n\n## Derivation\n\nNote that the DDPM objective depends only on the marginal distributions $$ q(\\mathbf{x_t} \\mid \\mathbf{x_0}) $$ and not $$ q(\\mathbf{x_t} \\mid \\mathbf{x_0}, ... , \\mathbf{x_T}) $$\n\n\n\nWe can think of some reformulations of diffusion models:\n\n$$\n\\alpha_{t-1}\\left(\\frac{x_t - \\sigma_t \\epsilon}{\\alpha_t} \\right) + \\sqrt{\\sigma^2_{t-1} - \\eta^2_t}\\hat{\\epsilon} + \\eta_t \\epsilon_t\n$$\n\nIn this case, we are predicting the clean data (in parens). Then we are jumping back to noise level $t-1$, by scaling by $\\alpha_{t-1}$, and adding two noise terms.\n\nThe first noise term represents the noise that existed in $x_t$ (estimated). The second noise term is fresh noise.\n\nThe variance of the noise we add is still $\\sigma_{t-1}$.\n\nAlso, see\n\nhttps://www.overleaf.com/read/fgrhhpqmtbgm#a55fc4 \n\nLast Reviewed 4/30/25\n\n\n"}, {"color": {"background": "#ffd900", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Elucidated-Diffusion-Models/", "id": "Elucidated Diffusion Models", "label": "Elucidated Diffusion Models", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "# Elucidated Diffusion Models\n\n## Noisy Data Distributions\nLet $p(\\mathbf{y})$ denote the distribution of data examples $\\mathbf{y}$.Consider the family of distributions obtained by adding Gaussian noise of standard deviation $\\sigma$ to the data distribution. Note that $\\sigma$ is not restricted to being between $0$ and $1$, and it can actually be very large.\n\nWe can denote the distribution of noisy data $\\mathbf{x}$ at a noise level $\\sigma$ to be\n\n$$\np(\\mathbf{x} ; \\sigma)\n$$\n\n## ODE\nLet us continue by writing an ODE, which can be used to transform data into noise (forward process) or noise into data (backward process) deterministically. The ODE will represent both of these processes. We introduce a time variable $t$, and imagine a datapoint $\\mathbf{x}$ evolving as time changes. As $t$ increases, $\\mathbf{x}$ gets noiser, and as $t$ decreases, $\\mathbf{x}$ gets less noisy.\n\nWe can choose a function $\\sigma(t)$ that is monotonically increasing. Then, formulate the ODE below:\n\n$$\nd\\mathbf{x} = - \\dot \\sigma(t) \\sigma(t)\\nabla_{\\mathbf{x}} \\log p \\left( \\mathbf{x}; \\sigma(t) \\right) dt\n$$\n\nAt time $t$, a datapoint $\\mathbf{x}$ will be distributed according to $p(\\mathbf{x} ; \\sigma(t))$. Thus, $\\sigma(t)$ is the desired noise level at time $t$, which is monotonically increasing. For instance, $\\sigma(t) \\propto \\sqrt{t}$ is constant-speed heat diffusion. The formula includes the score function for the distribution at noise level $\\sigma(t)$.\n\n\u003cspan style=\"color:blue\"\u003eTo Do: Derive the ODE\u003c/span\u003e.\n\nDespite the fact that we are simulating the addition of noise, there is no randomness introduced into the forward or backwards process yet.\n\n## Score Function\nSuppose we have optimized a denoising function $D$ to our data at a certain noise level $\\sigma$. The denoising function minimizes this objective:\n\n$$\n\\mathbb{E}_{\\mathbf{y} \\sim p_{data}} \\mathbb{E}_{\\mathbf{n} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})} \\left| \\left| D(\\mathbf{y} + \\mathbf{n}  ; \\sigma ) - \\mathbf{y} \\right| \\right|^2 _2\n$$\n\nThis objective is analyzed more in [my notes on SDEs](Generative-Modeling-Using-SDEs.md). Our score function will be equal to \n\n$$\n\\nabla \\log_{\\mathbf{x}} p \\left( \\mathbf{x}; \\sigma(t) \\right) = \\left( D(\\mathbf{x};\\sigma) - \\mathbf{x} \\right) / \\sigma^2\n$$\n\nNote that we can derive this by recognizing the that distribution of the noisy data is a mixture of Gaussians. \n\n\u003cspan style=\"color:blue\"\u003eTo Do: Derive this, show that it is the score function for a single Gaussian, argue that it is linear.\u003c/span\u003e\n\n$D$ may not be a neural network itself. It may have some pre and post-processing steps. For instance, in DDPM, we predict the noise instead of the denoised image.\n\n## Scale functions\nSome versions have $\\mathbf{x} = s(t) \\hat{\\mathbf{x}}$, where $\\hat{\\mathbf{x}}$ is the unscaled variable (which can still be noisy). Let us try to derive the ODE when there is a scale function.\n\nDifferentiating both sides and using the product rule, we get\n\n$$\\frac{d\\mathbf{x}}{dt} = \\dot s(t) \\hat{\\mathbf{x}} +  s(t) \\frac{d\\hat{\\mathbf{x}}}{dt}$$\n\nOr that \n\n$$\\frac{d\\mathbf{x}}{dt} = \\dot s(t) \\frac{\\mathbf{x}}{s(t)} + s(t) \\frac{d\\hat{\\mathbf{x}}}{dt} $$\n\n$$\\frac{d\\mathbf{x}}{dt} = \\frac{\\dot s(t)}{s(t)} \\mathbf{x} + s(t) \\frac{d\\hat{\\mathbf{x}}}{dt}$$\n\n$$d\\mathbf{x} = \\left[ \\frac{\\dot s(t)}{s(t)} \\mathbf{x} + s(t) d\\hat{\\mathbf{x}} \\right]dt$$\n\nSubstituting our unscaled ODE for $\\hat{\\mathbf{x}}$:\n\n$$d\\mathbf{x} = \\left[ \\frac{\\dot s(t)}{s(t)} \\mathbf{x} + s(t) \\left[ - \\dot \\sigma(t) \\sigma(t)\\nabla_{ \\hat{\\mathbf{x}}} \\log p \\left( \\hat{\\mathbf{x}}; \\sigma(t) \\right) \\right] \\right]dt$$\n\n\n$$d\\mathbf{x} = \\left[ \\frac{\\dot s(t)}{s(t)} \\mathbf{x} - s(t) \\left[\\dot \\sigma(t) \\sigma(t)\\nabla_{ \\hat{\\mathbf{x}}} \\log p \\left( \\hat{\\mathbf{x}}; \\sigma(t) \\right) \\right] \\right]dt$$\n\nNow we make a change of variables:\n\n$$\nd\\mathbf{x} =  s(t) d\\hat{\\mathbf{x}}\n$$\n\n$$\n\\frac{d\\mathbf{x}}{s(t)} =   d\\hat{\\mathbf{x}}\n$$\n\n$$\nd\\hat{\\mathbf{x}} = \\frac{d\\mathbf{x}}{s(t)}\n$$\n\n\n$$\n\\frac{d}{d\\hat{\\mathbf{x}}} =   s(t) \\frac{d}{d\\mathbf{x}}\n$$\n\n\n$$\n\\nabla_{\\hat{\\mathbf{x}}} \\log p(\\hat{\\mathbf{x}} ; \\sigma(t)) =\n\\frac{d}{d\\hat{\\mathbf{x}}} \\log p(\\hat{\\mathbf{x}}, \\sigma(t)) = \ns(t) \\frac{d}{d\\mathbf{x}} \\log p(\\hat{\\mathbf{x}}, \\sigma(t))\n$$\n\n$$\n= s(t) \\frac{d}{d\\mathbf{x}} \\log p\\left(\\frac{\\mathbf{x}}{s(t)}, \\sigma(t)\\right)\n$$\n\n$$\n= s(t) \\nabla_{\\mathbf{x}} \\log p\\left(\\frac{\\mathbf{x}}{s(t)}, \\sigma(t)\\right)\n$$\n\nAfter substituting:\n\n$$\nd\\mathbf{x} = \\left[ \\frac{\\dot s(t)}{s(t)} \\mathbf{x} - s(t) \\left[\\dot \\sigma(t) \\sigma(t)\\nabla_{ \\hat{\\mathbf{x}}} \\log p \\left( \\hat{\\mathbf{x}}; \\sigma(t) \\right) \\right] \\right]dt\n$$\n\n$$\nd\\mathbf{x} = \\left[ \\frac{\\dot s(t)}{s(t)} \\mathbf{x} - s(t) \\left[ \\dot \\sigma(t) \\sigma(t) s(t) \\nabla_{\\mathbf{x}} \\log p\\left(\\frac{\\mathbf{x}}{s(t)}, \\sigma(t)\\right) \\right] \\right] dt\n$$\n\n$$\nd\\mathbf{x} = \\left[ \\frac{\\dot s(t)}{s(t)} \\mathbf{x} - s(t)^2 \\left[ \\dot \\sigma(t) \\sigma(t) \\nabla_{\\mathbf{x}} \\log p\\left(\\frac{\\mathbf{x}}{s(t)}, \\sigma(t)\\right) \\right] \\right] dt\n$$\n\nThe formula above is our ODE when we have a scaling function.\n\n### Estimating the score function after scaling\nWe would like to use our denoiser for the unscaled variable $\\hat{\\mathbf{x}}$ to estimate the score function, even when there is scaling. Supposing we have a denoiser $D(\\hat{\\mathbf{x}} ; \\sigma(t))$ for the unscaled variable $\\hat{\\mathbf{x}}$, then we have that \n\n$$\n\\nabla \\log_{\\hat{\\mathbf{x}}} p \\left( \\hat{\\mathbf{x}}; \\sigma(t) \\right) = \\frac{ D(\\hat{\\mathbf{x}};\\sigma) - \\hat{\\mathbf{x}} }{ \\sigma(t)^2}\n$$\n\nWe also have from above that\n\n$$\n\\nabla \\log_{\\hat{\\mathbf{x}}} p \\left( \\hat{\\mathbf{x}}; \\sigma(t) \\right) = s(t) \\nabla_{\\mathbf{x}} \\log p\\left(\\frac{\\mathbf{x}}{s(t)}, \\sigma(t)\\right)\n$$\n\nThus, \n\n$$\n\\nabla_{\\mathbf{x}} \\log p \\left( \\frac{ \\mathbf{x} }{ s(t) }, \\sigma(t)\\right) = \\frac{ D( \\hat{\\mathbf{x} } ; \\sigma) - \\hat{ \\mathbf{x} } }{\\sigma(t)^2 s(t)}\n= \\frac{ D( \\frac{ \\mathbf{x} }{ s(t) } ; \\sigma) - \\frac{ \\mathbf{x} }{s(t)} }{ \\sigma(t)^2 s(t)}\n$$\n\nWe can use this function to estimate the score function from the denoiser.\n\n\n### Computing the Time-Derivative of x\nIn order to sample, we should compute the derivative $\\frac{d\\mathbf{x}}{dt}$. This can be used to estimate the denoising trajectory.\n\n\n$$\n\\frac{ d \\mathbf{x} }{ dt } = \\frac{ \\dot{s} (t) }{ s(t) } \\mathbf{x} - s(t)^2 \\left[ \\dot{\\sigma}(t) \\sigma(t) \\nabla_{\\mathbf{x}} \\log p \\left( \\frac{ \\mathbf{x} }{ s(t) }, \\sigma(t) \\right) \\right]\n$$\n\n$$\n\\frac{d \\mathbf{x} }{ dt } = \\frac{\\dot{s}(t)}{s(t)} \\mathbf{x} - s(t)^2 \\left[ \\dot{\\sigma}(t) \\sigma(t) \\frac{ D\\left( \\frac{\\mathbf{x}}{s(t)} , \\sigma \\right) - \\frac{\\mathbf{x}}{s(t)} }{\\sigma(t)^2 s(t)}\n \\right]\n$$\n\n\n$$\n\\frac{d \\mathbf{x} }{ dt } =  \\mathbf{x} \\left( \\frac{\\dot s(t)}{s(t)}  + \\frac{\\dot \\sigma(t)}{\\sigma(t)} \\right) - \\frac{\\dot \\sigma(t) s(t)}{\\sigma(t)}  D\\left(\\frac{\\mathbf{x}}{s(t)} ; \\sigma(t) \\right)\n$$\n\nWe can use this derivative for sampling.\n\n## Sampling\n\nSet a maximum noise level. The distribution of noisy data at the maximum noise level is:\n\n$$\np(\\mathbf{x} ; \\sigma_{max})\n$$\n\nThis should be indistinguishable from pure Gaussian Noise, assuming $\\sigma_{max} \u003e\u003e \\sigma_{data}$. However, the values here may be quite large, which is to say that they are not samples from a *standard* Gaussian distribution, like they are in DDPM. \n\n**Side Note**: In fact, imposing the constraint that the largest noise level has variance 1 requires a curvature to the noise trajectory, which we do not want. This is because the score function is the derivative of the noise trajectory, which is a *linear* approximation. Thus, for this approximation to be maximally accurate, we should have *linear* noise trajectories.\n\nDuring the sampling process, we should start with a sample from $$\\mathbf{x}_{0} \\sim \\mathcal{N} ( \\mathbf{0}, \\sigma_{max}^2 \\mathbf{I} )$$, and denoise images sequentially such that we get $\\mathbf{x}_1, \\mathbf{x}_1, \\ldots \\mathbf{x}_N$, where\n\n$$\n\\mathbf{x}_i \\sim p(\\mathbf{x}_i ; \\sigma_i)\n$$\nAnd\n$$\n\\sigma_{0} = \\sigma_{max} \u003e \\sigma_1 \u003e \\cdots \u003e \\sigma_N = 0\n$$\n\nThen, $\\mathbf{x}_N$ will be distributed like the data.\n\n**Notes:**\n- In the ODE formulation, the only source of randomness in backwards sampling is the initial noise $\\mathbf{x_0}$.\n- Theorectically, sampling should be independent from fitting $D$. $D$ is simply a black box.\n\n\n\n\n\n\n### Truncation Error\nTruncation error accumulates by discretizing time during sampling, but total truncation error decreases when the number of steps increases. In other words, local error scales super-linearly with respect to step size, so increasing step size by a factor of 2 increases per-step error by a factor of more than 2, and thus increases error overall.\n\nThe Euler Method has error $O(h^2)$ with respect to the step size $h$. Huen\u0027s method has error $O(h^3)$.\n\n\n### Huen\u0027s Method\n- Use a second-order Huen sampler.\n- This measures the derivative $\\frac{d\\mathbf{x}}{dt}$ at $\\mathbf{x}$, and after taking one step.\n- The actual derivative used is the average between these two derivatives.\n- When stepping to $\\sigma = 0$, we revert to Euler to avoid dividing by zero.\n\n\n\n\n\u003cspan style=\"color:blue\"\u003eTo Do: Show the actual algorithm\u003c/span\u003e.\n\n### Spacing the Time Steps\nStep size should decrease monotonically as noise level decreases, and does not need to depend on the content of the sample.\n\nThe step sizes decrease as we get closer to no noise. In the paper we have $\\rho = 7$, and that the step sizes are \n\n$$\n\\sigma_{i \u003c N} = \\left(\\sigma_{max}^{1/\\rho} + \\frac{i}{N-1}\\left(\\sigma_{min}^{1/\\rho} - \\sigma_{max}^{1/\\rho}\\right) \\right)^\\rho\n$$\nIn other words, we are doing linear interpolation in the $x^{1/\\rho}$ domain. \n\n**Note:** Imagine the square root function, and taking points equally space on the $y$ axis. Linear spacing on the $y$ axis corresponds to more spacing on the $x$ axis as $x$ increases. The severity of this disparity depends on $\\rho$.\n\nWhile $\\rho = 3$ apparently nearly equalizes truncation error between steps, $\\rho = 7$ works better, meaning that we want to make the disparity even more severe, or that the steps at lower noise levels matter more.\n\n\n### Specific Scale and Variance Schedule\nAlso, in the EDM formulation, we have:\n- Set $s(t) = 1 $\n- Set $\\sigma(t) = t$\n\nThis means that $t$ and $\\sigma$ become interchangeable. Also, since the noise trajectories are linear, we have that a single step to $t = 0$ will give you the denoised image. The tangent line to the trajectory points towards the denoiser output. The plots show you that we only have slight curvature at some intermediate time steps, but at the first and last time steps, we are linear.\n\n\n\u003cimg src=\"image-1.png\" width=\"400\"\u003e\n\n![Trajectories](image-1.png)\n\n\u003cimg src=\"./image-1.png\" width=\"400\"\u003e\n\n\nIt should make sense that since the derivative is a linear approximation to the noise trajectory, the noise trajectory should be as linear as possible.\n\nIn addition, the formula for the derivative becomes much simpler:\n\n$$\n\\frac{d\\mathbf{x}}{dt} = \\frac{\\mathbf{x} - D(\\mathbf{x}; t)}{t}\n$$\n\n\n\n## Stochastic Differential Equation\nWe can generalize our ODE to an SDE. Let\u0027s assume the parameters in the EDM formulation, $s(t) = 1$ and $\\sigma(t) = t$.\n\n$$\nd\\mathbf{x}_{\\pm} = - \\dot \\sigma(t) \\sigma(t) \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x} ; \\sigma(t)) dt \\pm \\beta(t) \\sigma(t)^2 \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x} ; \\sigma(t)) dt + \\sqrt{2 \\beta(t)}\\sigma(t) d \\omega_t\n$$\n\nNote that we get Song\u0027s formulation when we have $\\beta(t) = \\dot \\sigma(t)/\\sigma(t)$, where the first two terms cancel out and there is no score in the forward process.\n\nAlso note that if $\\beta(t)$ is zero, we have the non-stochastic ODE.\n\nThe three terms are, respectively:\n1. Probability Flow ODE.\n2. Deterministic Noise Decay.\n3. Noise Injection.\n\n**Notes**:\n- Note that $dt$ is negative during the backwards process, meaning the second term goes towards the data distribution during denoising.\n- During the denoising process, we can see term 2 as adding \"more score\" to the distribution, while the third time simultaneously adds \"more noise\".\n- When we add these two terms together, the net contribution to the noise level cancels out.\n- We can also think of the second term as removing more \u0027existing noise\u0027 and the third term as adding \u0027new noise\u0027. $\\beta$ (which scales up both the second and third terms) actually represents the degree to which new noise is replaced by existing noise.\n- The second two terms help drive the sample towards the marginal distribution $p(\\mathbf{x} ; \\sigma(t))$, I suppose this is accomplished by adding more noise to keep the noise level the same, while also adding in some score.\n\n\u003cspan style=\"color:blue\"\u003eTo Do: Think more about the last bullet point.\u003c/span\u003e\n\n### EDM\u0027s Sampler\nThis sampler includes churn, which adds and removes noise during the sampling process. At each step, we \n- Add noise to go from current noise level $t_i$ to a higher noise level $\\hat{t}_i$\n- Perform a single step from the higher noise level $$ \\hat{t}_{i} $$ to the a new, even lower noise level $t_{i+1}$.\n\n\nThis is actually slightly different than approximating the SDE above. In the SDE above, we add noise, but we attempt to correct for it using the score function from the noise level $t_i$ *before* adding noiss. This is incorrect because the noise level that is input to the score fucntion is different than the actual noise level of the data. The EDM sampler uses the noise level $\\hat{t}_i$ *after* adding noise. This is more accurate, and works better with larger step sizes.\n\nWe still can combine this with the second order Huen method.\n\n#### Problems with Too Much Churn\n- Too much churn can actually cause a loss of detail and drift towards oversaturated colors at low and high noise levels, possibly due to the fact that the score estimated by the denoiser network is a slightly non-conservative vector field.\n- The true score function should ideally be conservative, meaning that integrating across a trajectory should be path-independent. However, adding noise might put you in a region of the vector field such that getting back to the same denoising trajectory isn\u0027t easy.\n- This non-conservativity could be explained by the denoiser trying to remove too much noise, due to regression towards the mean.\n- \u003cspan style=\"color:blue\"\u003eTo Do: Think more about conservative vector fields.\u003c/span\u003e\n- \u003cspan style=\"color:blue\"\u003eTo Do: Think more about why denoisers remove too much noise, and regression toward the mean.\u003c/span\u003e\n\nAs a result, EDM does these modifications:\n- We only churn within a range in the middle of the noise schedule. \n- When we choose the \"churning noise\", we should theoretically choose  $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$, and then add $\\sqrt{\\hat{t}_i^2 - t_i^2} \\epsilon$ to the sample.\n- However, in practice we do $$\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, S^2_{noise} \\mathbf{I}) $$ , where $$S^2_{noise} \u003e 1$$. This means we sample add a little more noise than we should, to counteract the bias in the denoiser to denoise too much.\n- We define $S_{churn}$ as the \"total\" amount of churning, and choose $$ \\hat{t}_{i} = \\gamma t_{i} $$, where $$ \\gamma = \\min (\\frac{S_{churn}}{N}, \\sqrt{2} -1)$$.\n\n\n\n## EDM\u0027s Preconditioning\n- Learning $D$ directly is difficult - the inputs vary greatly in terms of magnitude.\n- Many works predict $\\mathbf{n}$ scaled to unit variance - this keeps the output magnitudes consistent.\n- However, for very high noise levels, this is difficult. The scale of the input is huge, and the denoised output is very small compared to it. We are basically relying on the noise estimate to cancel out with the input precisely such that their difference is on the same scale of the denoised data. If it does not do this at any index, the error will be huge\n- Perhaps at high noise levels, it is actually easier to predict the denoised signal, so that we can consistently predict at the correct scale.\n- Actually, our network predicts a mix of data and noise.\n- At high noise levels, an $\\epsilon$ prediction network behaves like an identity mapping, and predicting the clean data behaves like a zero mapping. Preconditioning means we have it behave like a zero mapping.\n- At low noise levels, an $\\epsilon$ prediction network behaves like zero mapping, while predicting clean data behaves like identity mapping. EDM\u0027s Preconditioning means we also have it behave like a zero mapping.\n\n## Other Notes\n### More Improvements\n- Adjust Hyperparameters from previous work.\n- Redistribute model capacity - remove lowest-resolution layers, double capacity of higher-resolution layers.\n- Use their preconditioning.\n- Use their loss function.\n- Use non-leaking augmentation - which is augmentation that does not change the data distribution.\n\n### Important Other Notes\n- Using their preconditioning doesn\u0027t improve performance, but does make training more robust.\n- For the best models on CIFAR-10, any amount of stochasticity is bad. But, for more diverse datasets, we benefit.\n- Maybe the effect of changing the preconditioning is dubious, it seems to give worse results in many cases.\n\n# Implementation\n## Datasets\nThey implement their modifications on 4 different datasets:\n- 32 x 32 CIFAR 10\n- 64 x 64 FFHQ\n- 64 x 64 AFHQv2\n- 64 x 64 ImageNet\n\n\n\n### Config A\n| Hyperparameter    | 32 x 32          | 64 x 64          |\n| --------------    | -------          | --------------   |\n| Training Examples | 200 Million      | Same             |\n| Batch Size        | 128              | Same             |\n| NFE               | 35               | 79               |\n| LR                | 2e-4             | 2e-4               |\n\n- It would be 400k training iterations at batch size 512.\n- Learning rate\n### Config B\n| Hyperparameter    | 32 x 32          | 64 x 64          |\n| --------------    | -------          | --------------   |\n| Training Examples | 200 Million      | Same             |\n| Batch Size        | 512              | 256              |\n| NFE               | 35               | 79               |\n| LR (Max)          | 1e-3             | 2e-4             |\n| EMA half-life     | 0.5              | Same             |\n| Dropout           | 13%              | 5/25%, FFHQ/AFHQ |\n\nOther Notes:\n- 4 -\u003e 8 GPUs\n- No gradient clipping\n- Learning rate ramp up to 1e-3 for 10 million images (instead of 0.64 million images)\n\n### Config C\nAdjust Config A:\n- remove 4x4 layers (these overfit), double 16x16 layer capacity\n\n\n| Resolution     | # Chan. - Conf. B | # Chan. Conf. C |\n| -------------- | -------           |  -------------- |\n| 64 x 64        | 128/None          | 128/None        |\n| 32 x 32        | 128               | 256             |\n| 16 x 16        | 256               | 256             |\n| 8 x 8          | 256               | 256             |\n| 4 x 4          | 256               | None            |\n\nReduces trainable parameters:\n - 32 x 32 - 56 million\n - 64 x 64 - 62 million\n\n### Configs D, E, F\n- D uses new preconditioning\n- E uses new noise distribution/loss weighting\n- F uses non-leaking augmentation (helps with overfitting)\n\n### ImageNet\n- 32 GPUs, batch size 4096 (128 per GPU)\n- Mixed Precision\n    - Weights stored at fp32, cast to fp16 during training\n    - Embedding/Self attention layers have no casting\n- 2 weeks, 2500 million images, 600k iterations\n - 296 million parameters\n- No augmentation\n- EMA of 50 million images\n- lr 1e-4\n- I believe it is 296 million parameters, for the 64x64 model in the diffusion beats GANs paper. It is 554M for 256.\n- For a 1D UNet, you probably increase the capacity of the model to account for the channel and frequency axis being the same.\n- You also multiplyg these 65 million parameters 3x since the kernels are smaller. \n- 188 M for a 256 x 80 model seems reasonable.\n- [1,1,2,2,4,4] for the 256 x\n\n## Architectures\nIn Config A, we start with:\n- VP uses DDPM++\n- VE uses NSCN++\nAnd use two different formulations. However, after all changes, they are the same except for the architecture.\n- DDPM++ seems to perform better according to the table.\n- Differences:\n    - DDPM++ uses Box filter [1, 1], while NSCN++ uses bilinear [1 3 3 1]\n    - NSCN++ has skip/residual blocks\n    - DDPM++ has positional encoding scheme, NSCN++ uses random Fourier features\n    - NSCN++ has extra residual skip connnections\n\n- For ImageNet, Use ADM architecture, no changes.\n- Compared to DDPM: \n    - Shallower model: 3 residual blocks per resolution\n    - More self attention layers (22 instead of 6) interspersed throughout model\n    - More attention heads (12 in lowest res.)\n    - More channels (768 in the lowest res. instead of 256)\n\nConclusion: For smaller datasets, use DDPM++.\n\n## Single-Level\n\n## Architectural Notes\n- To do: write these down in website\n\n\n### Block Level\n- There is 1 more decoder block than encoder block per level.\n- There are skip connections for EVERY Encoder UNet block (20 of them). \n- There are skip connections connecting the beginning of the block to the end.\n\n\n## Test Encoder\nWe use channel_mult = [2, 3, 4, 5] for clarity. In reality, it is [1, 2, 2, 2]\n| Name           | In Channels | Out Channels | In Resolutions | Out Resolutions |\n|----------------|-------------|--------------|----------------|-----------------|\n| 64x64_conv     | 3           | 128          | 64             | 64              |\n| 64x64_block0   | 128         | 256          | 64             | 64              |\n| 64x64_block1   | 256         | 256          | 64             | 64              |\n| 64x64_block2   | 256         | 256          | 64             | 64              |\n| 64x64_block3   | 256         | 256          | 64             | 64              |\n| 32x32_down     | 256         | 256          | 64             | 32              |\n| 32x32_block0   | 256         | 384          | 32             | 32              |\n| 32x32_block1   | 384         | 384          | 32             | 32              |\n| 32x32_block2   | 384         | 384          | 32             | 32              |\n| 32x32_block3   | 384         | 384          | 32             | 32              |\n| 16x16_down     | 384         | 384          | 32             | 16              |\n| 16x16_block0   | 384         | 512          | 16             | 16              |\n| 16x16_block1   | 512         | 512          | 16             | 16              |\n| 16x16_block2   | 512         | 512          | 16             | 16              |\n| 16x16_block3   | 512         | 512          | 16             | 16              |\n| 8x8_down       | 512         | 512          | 16             | 8               |\n| 8x8_block0     | 512         | 640          | 8              | 8               |\n| 8x8_block1     | 640         | 640          | 8              | 8               |\n| 8x8_block2     | 640         | 640          | 8              | 8               |\n| 8x8_block3     | 640         | 640          | 8              | 8               |\n\n## Decoder\n| Name           | In Channels | Out Channels | In Resolutions | Out Resolutions | Skip From  |\n|----------------|-------------|--------------|----------------|-----------------|------------|\n| 8x8_in0        | 640         | 640          | 8              | 8               |            |\n| 8x8_in1        | 640         | 640          | 8              | 8               |            |\n| 8x8_block0     | 1280        | 640          | 8              | 8               | 8x8_block3 |\n| 8x8_block1     | 1280        | 640          | 8              | 8               | 8x8_block2 |\n| 8x8_block2     | 1280        | 640          | 8              | 8               | 8x8_block1 |\n| 8x8_block3     | 1280        | 640          | 8              | 8               | 8x8_block0 |\n| 8x8_block4     | 1152        | 640          | 8              | 8               | 8x8_down   |\n| 16x16_up       | 640         | 640          | 8              | 16              |            |\n| 16x16_block0   | 1152        | 512          | 16             | 16              | 16x16_block3 |\n| 16x16_block1   | 1024        | 512          | 16             | 16              | 16x16_block2 |\n| 16x16_block2   | 1024        | 512          | 16             | 16              | 16x16_block1 |\n| 16x16_block3   | 1024        | 512          | 16             | 16              | 16x16_block0 |\n| 16x16_block4   | 896         | 512          | 16             | 16              | 16x16_down |\n| 32x32_up       | 512         | 512          | 16             | 32              |            |\n| 32x32_block0   | 896         | 384          | 32             | 32              | 32x32_block3 |\n| 32x32_block1   | 768         | 384          | 32             | 32              | 32x32_block2 |\n| 32x32_block2   | 768         | 384          | 32             | 32              | 32x32_block1 |\n| 32x32_block3   | 768         | 384          | 32             | 32              | 32x32_block0 |\n| 32x32_block4   | 640         | 384          | 32             | 32              | 32x32_down |\n| 64x64_up       | 384         | 384          | 32             | 64              |            |\n| 64x64_block0   | 640         | 256          | 64             | 64              | 64x64_block3 |\n| 64x64_block1   | 512         | 256          | 64             | 64              | 64x64_block2 |\n| 64x64_block2   | 512         | 256          | 64             | 64              | 64x64_block1 |\n| 64x64_block3   | 512         | 256          | 64             | 64              | 64x64_block0 |\n| 64x64_block4   | 384         | 256          | 64             | 64              | 64x64_conv |\n| 64x64_aux_norm |             |              |                |                 |            |\n| 64x64_aux_conv | 256         | 3            | 64             | 64              |            |\n\n\n## More details\n---All linear and conv have bias 0\n---All conv1s (second conv in block) have weight 1e-6\n---the output projection from attention also have weight 1e-6\n---also aux conv\nFirst thing:\n- 64x64 Conv\n\n\n\n\n\n\nBlock kwargs\n- 512 embedding channels\n- 1 head\n- 0.1 dropout\n- eps 1e-06\n- xavier uniform initialization\n- for attention blocks\n    - sqrt(0.2) as init_weight\n    - xavier uniform initialization\n\n- for zero-initialized\n    - 1e-5 init weight\n- [1,1] resample filter\n- resample projection is true\n- skip scale is 1/sqrt(2)\n\n\nUNet Block\n- x = conv0(silu(norm0(x)))\n- first norm is group norm with in_channels, 1e-6\n- first conv has kernel size 3, potential upsampling/downsampling, xavier uniform initialization\n- linear from emb_channels to out_channels\n- x + linear(embedding)\n- silu(norm2(x))\n- dropout x\n- conv1 x, conv2 has out, out, kernel size 3, 1e-5 initial weight\n-  \n\n\n\n### Positional Encoding\n\n    Notes:\n        - log(sigma) lyings 6 stds outside the mean has prob. 1.973e-9,\n        - This is 1 in 506 Million.\n        With P_mean=-1.2, range is [-8.4, 6]\n        c_noise ranges from -2.1 to 1.5\n\nLast Reviewed: 2/11/25    "}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Training/", "id": "Training", "label": "Training", "mass": 1.98, "shape": "dot", "size": 14.071247279470288, "title": "# Training\n## Batch Size\n- SGD is a signal/noise ratio problem - bigger batches approximate the true gradient better\n- Small learning rate is mostly regularization against the noisiness of different batches - we are in the regime where the step sizes are smaller than the curves in the loss surface.\n- Noiseless gradients will allow for an increase in learning rate until we reach issues with loss curvature.\n- Noisy gradients make it harder to improve training loss, since the step will be in a slightly random direction.\n- Small models, since they overfit, need smaller batch sizes. But not all models.\n- A large model that only makes one pass through the dataset willl not overfit. If the train loss is decreasing, so is the valid loss.\n- regularization is not helpful in this case\n\n\n### My thoughts\n- Linear scaling of learning rate by batch size doesn\u0027t work, since the underlying loss landscape may still be complex.\n- Generalization: the network needs to learn something that will also work on the next batch, not just the data it is currently seeing (is this right)?\n\n\n\nLast Reviewed: 5/1/25"}, {"color": {"background": "#0000ff", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\posts\\A-Recipe-for-Training-Neural-Networks/", "id": "A Recipe for Training Neural Networks", "label": "A Recipe for Training Neural Networks", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# A Recipe for Training Neural Networks\n- NN are not off-the-shelf as soon as you deviate from training an imagenet classifier.\n- NN is a leaky abstractions - understand how they work (backprop blog post)\n- NNs fail silently - errors are logical, not syntatic.\n- Fast + Furious = Suffering\n- Visualize Everything\n- Don\u0027t add too much complexity at the same time.\n\n## 1. Data\n- Inspect the Data, duplicates, patterns, details, quality, noise. Write filtering code, inspect outliers.\n\n# 2. Training/Eval Skeleton\n- Start with an easy model\n- fix seed\n- Verify loss at init\n- initialize final layer well\n- human baseline, input independent baselines, overfit on one batch, training loss should go down as model increases in size, visualize exactly what goes into the net\n- Visualize prdictions on a test example, and see how the predictions jitter.\n- Use backprop to visualize dependencies\n\n# 3. Overfitting\n- Get a model large enough to overfit on the task\n- Don\u0027t be a hero with crazy model architectures\n- ADAM is safe - SGD is better if well-tuned, learning rate range is narrow though.\n- Complexify one at a time\n- use a constant LR at first\n\n# 4. Regularize\n- more data/augment\n- decrease batch size\n- dropout, but bet careful\n- weight decay (like L2)\n- early stopping\n- larger modlels, when early-stopped, can be better than small by a lot\n\n# 5. Tune\n- Random search, not grid search\n- Leave it training.\n\nLast Reviewed: 4/30/25"}, {"color": "#3FFF57", "fixed": false, "font": {"color": "white"}, "id": "Audio", "label": "Audio", "mass": 0.98, "shape": "dot", "size": 9.899494936611665, "title": ""}, {"color": {"background": "#baee38", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\DiffWave/", "id": "DiffWave", "label": "DiffWave", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# DiffWave\nLast Reviewed: 1/23/25"}, {"color": {"background": "#3fff57", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\DAC/", "id": "DAC", "label": "DAC", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# DAC\nResidual blocks with dilations 1, 3, 9 slash kernel size 7, and depthwise 1x1\nThese are chained together.\n\nDownsampling blocks that double channel\n\nUpsampling blocks that halve channel\n\nSnake activations\n\nfeature matching loss, multiscale STFT discriminator, mel-reconstruction loss\n\nLast Reviewed: 1/17/25"}, {"color": "#79443B", "fixed": false, "font": {"color": "white"}, "id": "Vision", "label": "Vision", "mass": 7.200000000000001, "shape": "dot", "size": 26.832815729997478, "title": ""}, {"color": {"background": "#bb8c35", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\VAR/", "id": "VAR", "label": "VAR", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# VAR\nWorks in VQ-VAE Latent Space\n\nEncoding:\nTake 64x64 latent, squash to 1x1, quantize.\nlookup, stretch to K x K, compute residual\nSquash residual to 4x4, quantize residual\nlookup, stretch to K x K, compute residual\n...\nvery similar to RVQ but with \u0027squashing\u0027\n\nAutoregressive \u0027next-scale\u0027 predictions,\npredict first scale, then all of next scale in parallel, etc.\n\nraster scan order bad - disrupts locality, makes infilling hard due to bidirectional correlation, inefficient.\n\nLast Reviewed: 1/17/25"}, {"color": {"background": "#79443b", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\Gaussian-Splatting/", "id": "Gaussian Splatting", "label": "Gaussian Splatting", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# Gaussian Splatting\nSuper Fast Rendering - realtime display rates at 1080p\nHigher quality than Mip-Nerf\nAvoids unnecessary computation in empty space\n\nNerf and voxel-based 3D representations require stochastic sampling for rendering - computationally expensive, result in noise\n\nuse sparse point clouds obtained from SFM - no need for MVS\n1-5 million gaussians per scene\n\nRendering:\n-respect visibility ordering\n-sorting\n-tile-based rasterization\n\n\nopacity-based rendering --- \n---the \u0027opacity\u0027 (light absorption) assigned to a point\ndepends on the distance from the previously sampled point, AND the density at the point\nbasically, you are assuming the \u0027point\u0027 is a block that absorbs light\n\nThese are continuous representations, (not voxels)\nworks with randomly initalized gaussians\ncan project gaussians to 2D and perform alpha blending\n\nParametrize each gaussian with:\n    opacity, anisotropic covariance, spherical harmonics\n    \nadaptive density control - add or remove gaussians during training\nLast Reviewed: 1/17/25\n"}, {"color": {"background": "#79443b", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\classes\\Advances-In-Computer-Vision/", "id": "Advances In Computer Vision", "label": "Advances In Computer Vision", "mass": 4.500000000000001, "shape": "dot", "size": 21.213203435596427, "title": "# Advances in Computer Vision\n\n## Image Formation\nWeek 1, Thursday\n### Pinhole Cameras\n-Why don\u0027t you see an image when you hold up a piece of paper?\n    - Each point in space reflects in all directions, which get spread out across the paper\n    - Conversely, each point on the paper gets light from multiple points in the scene.\n- Pinhole camera light in such that each point on the image corresponds to a point from an object\n- Get an image on the other side\n- Tradeoff between brightness and sharpness\n## Derivation\n- Establish 3D camera system.\n- center = pinhole cetner.\n- z = distance from pinhole center\n- x and y are up and side\n- right handed in real-world space\n- f = distance from pinhole to image plance\n\nTo do: better notes for lectures 1 (introduction), 2 (image formation), and 3 (fourier transform).\n"}, {"color": {"background": "#79443b", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\classes\\Image-Formation/", "id": "Image Formation", "label": "Image Formation", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Image Formation\n\nWeek 1, Thursday\n## Pinhole Cameras\n-Why don\u0027t you see an image when you hold up a piece of paper?\n    - Each point in space reflects in all directions, which get spread out across the paper\n    - Conversely, each point on the paper gets light from multiple points in the scene.\n- Pinhole camera light in such that each point on the image corresponds to a point from an object\n- Get an image on the other side\n- Tradeoff between brightness and sharpness\n### Derivation\n![alt text](image-1.png)\n- Establish 3D camera system.\n- center = pinhole cetner.\n- z = distance from pinhole center\n- x and y are up and side\n- right handed in real-world space\n- f = distance from pinhole to image plane\n\nWe can think of the unflipped image instead:\n\n![alt text](image.png)\nCan use similar triangles:\n![](image-2.png)\n\nNote that **depth** is $$Z$$, not the ray length (the hypotenuse).\n\n$$\n\\mathbf{x} = (x,y) = (X,Y) \\cdot \\frac{f}{Z}\n$$\n\nWhere $$(x,y)$$ are the image/pixel coordinates, and $$(X,Y)$$ are the world coordinates.\n\n### Properties Preserved\n- Lines that are straight in 3D are straight in 2D\n- Incidences - lines that meet in 3D will meet in 2D\n- Angles and lengths are distorted. Right angled tables are different\n- Parallel lines will not be there, there will be vanishing points (e.g.) railroad tracks.\n\n#### Example - a line.\n$$\nX(t) = X_0 + at\n$$\n$$\nY(t) = Y_0 + bt\n$$\n$$\nZ(t) = Z_0 + ct\n$$\nAfter computing $x$ and $y$ in pixel space and taking $$t \\rightarrow \\infty$$, we see that we get points in image space as \n\n$$\n\\frac{fa}{c}\n$$\n\n$$\n\\frac{fb}{c}\n$$\nWhich does not depend on $X_0$, or the \u0027initial point\u0027 on the line. All parallel lines intersect at the vanishing point, it does not matter the \u0027offset\u0027 of the line, just the \u0027slope\u0027\n\nUnless the lines are parallel to the image plane, or $$c=0$$, parallel lines will remain parallel. Lines that are parallel to the image plane will either have a single intersection or be parallel.\n\n![ ](image-3.png)\n\nDiffusion models also don\u0027t always have perspective geometry.\n\n### Homogeneous Coordinates\nThe $(X,Y,Z)$ to $(x,y)$ mapping is not linear.\n\nLets try having\n\n![alt text](image-4.png)\nAnd, all points are equivalent up to scalar multiplications.\n\n![alt text](image-5.png)\n\nNow, let\u0027s look for a transformation from 3D Homogenous coordinates to 2D image pixel coordinates:\n![alt text](image-7.png)\n\nIntrinsics Matrix:\n![alt text](image-8.png)\n\nWe should also have translations in x-y sspace \n\n![](image-9.png)"}, {"color": {"background": "#79443b", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\classes\\Linear-Image-Processing/", "id": "Linear Image Processing", "label": "Linear Image Processing", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Linear Image Processing\n\nAn image is a function maps a ray to a color, or a 2D coordinate to a color. We can discretize it, and represent the image as a vector.\n\nVector is sampled from a function.\n\n\nSpace of images is an inner product space.\n\n"}, {"color": {"background": "#762b9a", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\classes\\Geometric-Deep-Learning/", "id": "Geometric Deep Learning", "label": "Geometric Deep Learning", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "\n# Geometric Deep Learning\n## Representation Theory\n- We can build rotation-invariant CNNs. CNNs are already shift-invariant.\n- Equivariance helps you generalize - for instance, if at train-time you only have centered images, and at test time you have a shifted one, a shift-equivariant NN will generalize.\n- Usually, any type of generalization will result from some sort of s\n- Grid Cells in the brain help you encode your location in space relative to your environment.\n- A rat\u0027s environment is represented as a Torus. This environment is transformed such that the rat is always at the same location in space.\n\n\n## Symmetry\n- Objects in the real world remain unchanged when transformations are applied.\n- Typically we discuss rotations and mirroring.\n- In math, the \u0027symmetry\u0027 of an object is a transformation that preserves certain properties (like angle, distance, structure).\n- For instance, Fourier magnitues are invariant under circular shift.\n- SO is the orthogonal group, or group or orthogonal matrices.\n- SE is the Euclidean group, or matrices that preserve distances.\n\n## Groups\n### Definition\n- A group is a set of elements $G$ with a group operation that takes two elements in the group and gives another.\n- the operation is closed under the group.\n- the operation is associative.\n- there is an identity element such that $g \\cdot e = g$.\n- there is an inverse element for each $g \\in G$ such that $g \\cdot g^{-1} = e$\n\nExamples\n- Group of rotation matrices.\n- Group of translation vectors.\n- Roto-translation - combinations of rotation + translation (SE2).\n\n### Representation\nA map $p$ from $G$ to the general linear group (all invertible matrices). And, we have \n\n$$\np(g \\cdot g\u0027) = p(g) \\cdot p(g\u0027)\n$$\n\n#### Left Regular Representation\nTransforms functions by transforming their domains.\n\n#### Eigenspace\nIf you have two eigenvalues with the same eigenvector, they form an eigenspace, where everything there has an eigenvector."}, {"color": {"background": "#762b9a", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\CNNs/", "id": "CNNs", "label": "CNNs", "mass": 0.98, "shape": "dot", "size": 9.899494936611665, "title": "# CNNs\n\nIf we need to stride by more than two, we should do this later.\n\nEDM\u0027s lesson - if we want to aggregate spatial information, using attention instead of downsampling is better.\n\nZero-initialize layers before residual connections\n\nZero-initialize biases"}, {"color": {"background": "#762b9a", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\UNet/", "id": "UNet", "label": "UNet", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# UNet\n\n"}, {"color": {"background": "#bb8c35", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\PixelVAE/", "id": "PixelVAE", "label": "PixelVAE", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# PixelVAE\n## Single-Level\n- Imagine a VAE\n- Now imagine a VAE + PixelCNN, where the Pixel CNN operates in the image space, and is conditioned on $$z$$.\n- i.e., $$p(x_i \\mid x_{i-1}, \\ldots , x_1, z)$$\n- to condition on $$z$$, we pass it through upsampling layers so that it is the same dimension as the image.\n\n## Hierarchical Version\n- Each stage takes an upsampled latent variable map\n- Uses PixelCNN to generate more latent variables (for the next stage)\n- The PixelCNN outputs the mean and variances of these latent variables, assumed Gaussian.\n- Latent variables for the next stage are upsampled again\n- The last layer outputs pixel values according to softmax\n\nLast Reviewed 2/6/25    "}, {"color": "#00FF00", "fixed": false, "font": {"color": "white"}, "id": "Language Modeling", "label": "Language Modeling", "mass": 2.25, "shape": "dot", "size": 15.0, "title": ""}, {"color": {"background": "#00ff00", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\classes\\Language-Modeling-from-Scratch/", "id": "Language Modeling from Scratch", "label": "Language Modeling from Scratch", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "# Language Modeling from Scratch\n\nBPE\nFew-Shot/Zero Shot Generalization\nScaling Laws - parameters, data, training time, result in linear log-log curves with loss\n\nLast Reviewed: 6/1/24\n"}, {"color": {"background": "#00ff00", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Transformers/", "id": "Transformers", "label": "Transformers", "mass": 1.2500000000000002, "shape": "dot", "size": 11.180339887498949, "title": "# Transformers\n\nTransformer Basics\nRotary Embeddings (Review)\nLayerNorm, projecting latent onto hypersphere\nMQA, GQA\nSwiGLU\nPrenorm vs postnorm\nLast Reviewed: 6/1/24\n"}, {"color": {"background": "#00ff00", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Encoder-Decoder-Transformers/", "id": "Encoder Decoder Transformers", "label": "Encoder Decoder Transformers", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Encoder-Decoder Transformers\n"}, {"color": "#212129", "fixed": false, "font": {"color": "white"}, "id": "Software", "label": "Software", "mass": 4.0600000000000005, "shape": "dot", "size": 20.149441679609886, "title": ""}, {"color": {"background": "#212129", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\softwares\\Filesystems/", "id": "Filesystems", "label": "Filesystems", "mass": 0.01, "shape": "dot", "size": 5, "title": "# File Systems\n\n- nfs is faster than afs\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#212129", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\softwares\\Python/", "id": "Python", "label": "Python", "mass": 1.3, "shape": "dot", "size": 11.40175425099138, "title": "# Python \n\n## Classes\n\nStatic Methods can be called without initializing the class. It does not have access to the instance or the class. It is basically a regular python function that is under the class for organizational purpose.\n\nClass methods have access to the class, but not the instance. They can instantiate the class.\n\nLast Reviewed: 4/26/25    "}, {"color": {"background": "#212129", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\softwares\\Hydra/", "id": "Hydra", "label": "Hydra", "mass": 0.04, "shape": "dot", "size": 5, "title": "# Hydra"}, {"color": {"background": "#212129", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\softwares\\Slurm/", "id": "Slurm", "label": "Slurm", "mass": 0.04, "shape": "dot", "size": 5, "title": "# Slurm\n\n- Partition is broader than a node.\n- Account determines who gets charged\n\nLast Reviewed: 4/30/25\n"}, {"color": "#212129", "fixed": false, "font": {"color": "white"}, "id": "Vector Operations", "label": "Vector Operations", "mass": 0.49, "shape": "dot", "size": 7.0, "title": ""}, {"color": {"background": "#212129", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Einsum/", "id": "Einsum", "label": "Einsum", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "\n## Einsum\n\t-https://eli.thegreenplace.net/2025/understanding-numpys-einsum/\n\t-matrix multiplication: np.einsum(\"ij,jk -\u003e ik\", A, B)\n\t-comma separate list of inputs in the strings will match the operands in terms of the number of them, and the number of dimensions.\n\t-the number of letters in each label in the string must match the number of dimensions in the input\n\t-whenever a letter is repeated in the input, these must have the same length\n\t-a letter that is repeated in the input, and absent in the output, is finna be summed\n\t-any input label must be repeated twice then dropped, or repeated once then listed in the output.\n\t-we can reorder operands, as long as we reorder labels, and we will get the same result.\n\t-we can transpose dims\n\t-for instance, multi-head self attention key projection: np.einsum(\"bsd,hdk-\u003ebhsk\", x, w_k)\n\t-can contract multiple dims, not just one\n\t-can do A @ B @ C: np.einsum(\"ij,jk,km-\u003eim, A,B,C)\n\t-implementation\n\t\t-read shapes\n\t\t-initialize output to zero-array of the correct shape.\n\t\t-loop over all letters in the expression\n\t\t-the expression in the innermost loop indexes into all the inputs and outputs properly, and does\n\t\t-output[...] += inputs * inputs ..... This is a scalar multiplication\n\n\t- can also use broadcasting\n\n\n\tnp.einsum(i,ij-\u003ej, dL_dout, w) is like doing \n\n\tsum_i(dL_dout_i, w_ij). We can imagine taking the labels, and using them as subscripts on the rest of the arguments.\n\n\n\tdL / dx_j = sum_i [(dL / dout_i) * w_ij)]\n\n\tIn Einstein summation notation, this is\n\tdL / dx_j = (dL / dout_i) * w_ij\n\tAs the sum is over i implicitly.\n\n\tConverting this to einsum means taking the subscripts, and moving them\n\tInto the first argument:\n\tnp.einsum(i,ij-\u003ej, (dL / dout), w)\n\n\n\n"}, {"color": "#35208d", "fixed": false, "font": {"color": "white"}, "id": "ML Systems", "label": "ML Systems", "mass": 3.47, "shape": "dot", "size": 18.627936010197157, "title": ""}, {"color": {"background": "#35208d", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Mixed-Precision/", "id": "Mixed Precision", "label": "Mixed Precision", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# Mixed Precision\n- Weights are float32, but during forward pass, we use fp16 for everything.\n- bf16 is like fp16, but has better dynamic range\n- 1.5x - 3x speedup (for both, in my observation)\n- You can increase batch size as well.\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#35208d", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\softwares\\Wandb/", "id": "Wandb", "label": "Wandb", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Wandb\n"}, {"color": {"background": "#2f2259", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\softwares\\PyTorch/", "id": "PyTorch", "label": "PyTorch", "mass": 1.2500000000000002, "shape": "dot", "size": 11.180339887498949, "title": "# PyTorch\nDatasets:\n---need __len__ and __get_item__\n\nDataloaders\n---collate_fn defines how the different data examples should be turned into a batch\n\nBackwards:\n---fills \"grad\" field of every tensor that requires it\n\nZero_grad\n---turns \"grad\" field of every tensor that requires it to 0\n\noptimizer.step()\n---the optimizer has a bunch of parameters stored in it, and it looks at the gradient of all the parameters\nand then does a backward step\n\n\nUse register_buffer to add a desired tensor to the model, so it gets moved to the right device.\npersistent=False will make it not part of the state_dict.\n\n\n\n## In Place Operations\n- Be careful with in-place operations, for instance\nx = x + relu(x, inplace=True)\n- Will zero out the negative parts of both terms, since that happens before addition.\n\n## Useful Operations\n- torch.split splits tensors to the specified size.\n- torch.chunk splits tensors into a desired number of chunks\n- unbid removes a tensor dimension and returns a tuple of slices\n- To generate a boolean mask for an operation that varies based on batches, we can create a tensor of scores then use \u003e= and \u003c= to unmask certain elements.\n\n## Datasets\n- Map dataset load everything\n- Iterables define a way to iterate through the dataset.\n- convention is return a tuple of things per batch\n\n## Dataloaders\n- Pin memory only allocates when the dataloader is called, apparently\n- persistent workers keeps dataloader workers around, use for train, not valid\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#2f2259", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\softwares\\Lightning/", "id": "Lightning", "label": "Lightning", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Lightning\n\n## 1. Lightning Module\n- No need to loop over epochs/dataset, eval/train, enabling/disabling gradients\n\n## 2. DataModule\n- Hook\n\n## 3. Trainer\n- Gradient Clipping\n- DDP\n- min_epochs = minimum number of epochs (default 1)\n- max_epochs = 1000 (default 1000)\n- min_steps, max_steps (takes precedence over epochs)\n- check_val_every_n_epochs (default 1, maybe want 10, 100)\n- val_check_interval (in case epoch = a few days, integer after n steps, or float for percentage of epoch)\n- num_sanity_val_steps (sanity, default 2, 0 to turn off, -1 for full valid loop)\n- limit_train_batches, limit_val_batches, limit_test_batches, (10-20 epochs for an action, shorten length of train/valid loops)\n- limit_val_batches = 0.1 = 10% of valid batches, int = a number of batches\n- gpus = 8 (use 8 GPUs), or pass in a list of indices according to PCI ordering, -1 for ALL gpus\n- auto_select_gpus = True -\u003e pick the right number of GPUs\n- log_gpu_memory=\u0027all\u0027, \u0027min_max\u0027 -\u003e log memory usage for GPU, but may slow training, it uses nvidia-smi\n    - recommended to prevent memory leaks\n- benchmark=True -\u003e results in speedups, but if the inputs change in size, not good.\n- deterministic=True -\u003e reproducable, but slowdown.\n- num_nodes - number of compute nodes. \n- \"ddp\" - pytorch - syncs gradients.\n- batch_size = num_nodes * num_gpus * num_nodes\n- need to set the seed, since otherwise the model weights will all be different.\n- can\u0027t use DDP in notebook/colab, or if you do fit multiple times. Then you need ddp_spawn, but that pickles everything, and you can\u0027t have num_works \u003e 0, and model on the original process will not be updatd.\n- DDP not supported on windows.\n- DataParrallel - splits data between batches, transfers across data a lot\n- DDP2 - examples with negative samples/contrastive training.\n- ddp_cpu - useful for debugging.\n\n\n## GPU training\n- delete all .cuda(), .to(device) calls\n- initialize tensors with device=self.device, and use register_buffer\n- z.type_as(x, device=self.device)\n\n\n## Mixed Precision\n- Lightning also casts buffers\n\n\nLast Reviewed: 4/28/25"}, {"color": {"background": "#35208d", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Large-Scale-Deep-Learning/", "id": "Large Scale Deep Learning", "label": "Large Scale Deep Learning", "mass": 0.99, "shape": "dot", "size": 9.9498743710662, "title": "# Large Scale Deep Learning\n\n- Distributed Storage Systems, network communications\n- Use rotational drives instead of NVMe cheaper\n- node-to-node communications become an issue.\n- Storage is on distributed systems, this is only reachable over network.\n- Weight Sync is done using pytorch\n- Need Fast I/O - this is the hardest part.\n- Read from Disk -\u003e Memory -\u003e Network card -\u003e compute node network card -\u003e memory -\u003e GPU\n- Some parts are scalable, other\u0027s aren\u0027t. Can\u0027t just double memory bus\u0027s bandwidth.\n- optimal = 100% utilization of most expensive part (GPU)\n- GPUs consume 200-1000 MB per second.\n- nvidia-smi shows gpu utilization. \n- smaller datasets can fit on NVMe drive, which transfers at 3 GB/second, easily keeping 4 desktop GPUs busy.\n- Example: 100 TB, 16 GPUs, 200 MB/s/GPU\n    - NVMe:\n        - $35,000 alone for storage\n        - 300 GB/s\n        - cant fit in one machine\n    - Rotational\n        - 8 TB, 16, $320\n        - 3.2 GB/s bandwidth\n        - can fit into 1-2 machines\n## Key Principles\n- Sequential I/O\n- Pipelining\n- Sharding\n\n### Sequential IO\n- PyTorch, file-based - access samples in random order.\n    - random access requires moving the disk head, loading, moving again.\n    - 20 MB per second (on rotational)\n- Sequential IO\n    - Make a .tar archive of all the examples in the dataset\n    - reads are sequential.\n    - IterableDatasets\n    - 200 MB/s read speeds (on rotational)\n    - in-memory shuffle buffer:\n        - while True:\n            - sample = next(dataset)\n            - index = randint(0, len(buffer)-1)\n             - sample, buffer[index] = buffer[index], sample\n        - basically, the buffer does all the shuffling for us. We read examples sequentially into a buffer,\n        and then randomly sample from that buffer. Each time we feed a sample from the buffer to the model,\n        its spot in the buffer getsw replaced by the next thing on disk. All reads from disk are sequential. \n        - better for network storage\n    - TFRecord/tf.Example, Google GFS, Hadoop, FORTRAN\n### Pipelining\n- make one request, many examples flow\n- possible with sequential storage.\n- Caching? Doesn\u0027t help in deep learning, since we iterate through the entire epoch\n\n### Sharding\n- 100 TB file, read sequentially. 200 MB/s IO\n- All reading is sequential.\n- Sharding - 100 TB dataset, split it up into 10,000 shards of 10 TB each.\n- access shards randomly - read shards in random order, read each shard sequentially with in memory buffer for shuffling.\n![alt text](image.png)\n\n### Three Tiers\n- Dataservers send data to CPU nodes, which do preprocessing. \nThese are then sent to the GPU.\n- RDMA based transfers from CPU to GPU. stops PCI bus as bottleneck.\n- combine the middle tier into the storage system (AIStore, NVIDIA).\n- HDFS can be difficult to configure, it was designed for big data.\n\nLast Reviewed: 5/1/25"}, {"color": {"background": "#35208d", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\softwares\\Webdataset/", "id": "Webdataset", "label": "Webdataset", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "# Webdataset\n\n- .tar files store things in continuous chunks\n- HDP for distributed storage protocol - use any webserver.\n- works for local files.\n- implements IterableDataset\n- all datasets are POSIX tar archives\n    - create using tar, or the tar writer class in the library\n    - all I/O is sequential.\n- refer to webdatasets by using a filepattern.\n- consists of many shards.\n- url = ...\n- dataset = wds.Dataset(url) \n- you might want to do decompression\n- iterating:\n    - key is the file extension\n    - value = binary content\n    - __key__ is the sample itself, common basename that is for the sample.\n    - decodings are standard. use .decode() to turn into dictionary of all files comprised in a sample, which can be turned into a tuple, then apply transforms.\n    - Then you can shuffle.\n    dataset = (\n        wds.Dataset(url, length=10000)\n        .decode(\"pil\")\n        .to_tuple(\"ppm\", \"cls\")\n        .shuffle(10000)\n    )\n\n## Creating a Webdataset:\n- .tar file\n- files with a common basename make up a sample.\n- basename = same samples\n- speaker1/sample1.txt\n- speaker1/sample1.wav  \n- can just make a tar archive.\n- basename is ALL extensions removeL sample.input.png -\u003e sample\n    - the key is \u0027sample\u0027, the dictionary entries are input.png/ \n- use tar\n- for labels can look up metadata.\n\n## avoiding extra storage\n- instead of storing a ton of files,\n- create a recipe then use \"tarp create\"\n- filename/source table. \n\n\n## ShardWriter\n- ensures each file is less than a certain size and has less than a given number of samples.\n\n\n## More practical stuff\n- Use resample = True, which will mean that if a worker runs out of data on their shard, they\nsimply find (\"sample\") a new shard\n- batch in the webdataset\n- see fmdiffae code\nLast Reviewed: 5/1/25"}, {"color": {"background": "#2f18c4", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Distributed-Training/", "id": "Distributed Training", "label": "Distributed Training", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# Distributed Training\n\n- DDP Syncs Gradients.\n- Then each optimizer updates their copy of the weights\n- \n\nLast Reviewed: 4/28/25"}, {"color": "#a8326f", "fixed": false, "font": {"color": "white"}, "id": "Signal Processing", "label": "Signal Processing", "mass": 4.41, "shape": "dot", "size": 21.0, "title": ""}, {"color": "#8a16b5", "fixed": false, "font": {"color": "white"}, "id": "DDSP", "label": "DDSP", "mass": 0.49, "shape": "dot", "size": 7.0, "title": ""}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\PQMF/", "id": "PQMF", "label": "PQMF", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# PQMF\n\nFilter signal into low band and high band\nDownsample both\nupsample both\nlow pass low band, high pass subband\nin the downsampled signal, the high frequencies are mirrored, and occupy the low frequencies.\nSpecial case of audio CNNs\nSee PQMF.ipynb\nLast Reviewed: 1/2/2024\n"}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Downsampling-and-Stretching/", "id": "Downsampling and Stretching", "label": "Downsampling and Stretching", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# Downsampling and Stretching\nLast Reviewed: 1/2/2025\n\nDownsampling \u0027folds\u0027 the FFT spectrum on itself.\ne.g. downsampling by factor of 2 - imagine slicing spec. in half,\nthen overlaying them.\n\nStretching replicates the FFT spectrum (doubles length of FFT)\nstretching by x2 mirrors spectrum,\nstretching by x3 appends the forward spectrum again\netc."}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Convolution/", "id": "Convolution", "label": "Convolution", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# Convolution\n\nConvolution in Neural Networks:\nDilated Convolution = replicating spectrum of filter\ndilated kernel size = (kernel_size - 1) * dilation + 1\nleads to higher frequency resolution (number of unique points)\nstrided convolution = conv plus downsampling\n\n\n## Downsampling\nnote that before downsampling, trailing entries are discarded.\n# of trailing entires discarded = stride - 1\ntherefore, the target size after conv only needs to be size - (stride - 1).\nTherefore, the input size after padding needs to be size - (stride - 1) + (kernel - 1), since the kernel takes away kernel - 1 units\nThis is equal to size + kernel - stride, so the padding needs to be (kernel - stride)/2.\nif stride is even, we therefore want an even kernel.\n\n\nGraph:\n   X X X X X X X X    - input\n[] X X X X X X X X [] - input after padding\n    X X X X X X X     - after conv, kernel size 4\n    X   X   X   X     - after downsampling\n\n\n- Note that in EDM, we downsample and THEN convolve.\n\nOtherwise:\nTwo interpretations:\n1---reverb (overlapping kernels)\n2---flipping and shifting\nthe \u0027flipped\u0027 kernel is a function. The x is the \u0027offset\u0027 and the y is the \u0027weight\u0027.\ni.e., how does input at time t + x affect output at time t.\n\n\n## More Params\n- Num groups - the weight is in_channels/groups\n\nLast Reviewed: 1/3/25"}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Transpose-Convolution/", "id": "Transpose Convolution", "label": "Transpose Convolution", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# Transpose Convolution\n\nWhen stride = 1, it\u0027s overlaying kernels\nThis is the \u0027superposition\u0027 interpretation of convolution\nit is the same as convolution with a flipped kernels\n\nWhen stride \u003e 1, it\u0027s the same as conv(stretch(x))\n\nSee the notebook you emailed Julius\n\nFilter the \u0027mirrored\u0027 copy of the signal\n\nGradient of Conv (makes forward and backwards same, natural way of upsampling)\n\nthe padding parameter is the same as truncating on both sides\nLast Reviewed: 1/3/25\n"}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Discrete-Fourier-Transform/", "id": "Discrete Fourier Transform", "label": "Discrete Fourier Transform", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# Discrete Fourier Transform\n\n- It is a least-squares projection onto an orthogonal basis of complex sinusoids.\n- Thus, taking two mirror DFT bins provides the sinusoid of that frequency that minimizes the least-squares error to the original signal, in terms of amplitude and phase.\n\n## RFFT\n- The Real DFT on an even-length signal has 1 DC, 1 Nyquist.\n- For instance, it goes from 256 length to 129 RFFT length. Bin 0 is DC, last bin (127) is nyquist\n- Or, for a 4-point DFT, the frequencies are 0, 1, nyquist, -1.\n\n\n\nLast Reviewed: 4/30/25\n\n\n\n"}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Fourier-Dualities/", "id": "Fourier Dualities", "label": "Fourier Dualities", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# Fourier Dualities\n\nWhy is multiplication in time convolution in frequency?\n\nThink of representing your signal as a sum of complex sinusoids at different frequencies.\n\nThis is a polynomial!\n\nThe coefficients are the frequency domain representation\n\nWhat happens when you mutiply two polynomials?\n\nYou convolve their coefficients.\n\nSee emails with Julius Smith\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Pink-Frequency-Profiles/", "id": "Pink Frequency Profiles", "label": "Pink Frequency Profiles", "mass": 0.98, "shape": "dot", "size": 9.899494936611665, "title": "# Pink Frequency Profiles\n\n\n- Lots of things display $1/f^{\\alpha}$ frequency spectrums.\n- These are linear on a log-log scale (logscale both frequency and amplitude)\n\nThis corresponds to fractal structure:\n- Picture sinusoid at a low frequency. Now, imagine \"zooming out\" 2x on this sinusoid in only the x-direction. What you\u0027ll see will look like a sinusoid of the same amplitude, but 2x the frequency. If we keep doing this, we\u0027ll get sinusoids of 1x, 2x, 3x... the frequency, but of equal amplitude. This will give us a flat frequency profile if we add them all together.\n\n= Now, imagine \"zooming out\" in both the x and the y directions. We\u0027ll see a sinusoid of k times the frequency, and 1/k the amplitude if we zoom out by a factor of k. If we add these sinusoids together, for k from 0 to N, we\u0027ll get an overall signal with a 1/f frequency profile.\n\n- This behavior is closer to self-similarity or a fractal structure, since the summed signal is a sum of sinusoids that are all the same shape, just on different scales.\n\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#a8326f", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\1f-Noise-in-Music-and-Speech/", "id": "1f Noise in Music and Speech", "label": "1f Noise in Music and Speech", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# \u00271/f noise\u0027 in music and speech\n\nThe power spectrum is often $f^{-\\alpha}, 0.5 \\leq \\alpha \\leq 1.5.\n\nLoudness, pitch, melody exhibit this behavior.\n\n## Autocorrelation Functions\nIf $\\langle v(t), v(t+\\tau) \\rangle$ is correlated (non-zero expectation) for $|\\tau| \u003c T$, it is \"white\" for frequencies $\n\\frac{1}{2\\pi \\tau_c}$ and is decreasing rapidly $f^{-2}$ for frequencies $ \\geq \\frac{1}{2\\pi \\tau_c}$. $\\frac{1}{f}$ means some correlation over all time scales for which $\\frac{1}{f}$ holds.\n\nNote that $\\tau=3$ implies a period of $2\\pi * 3$, or that the angular frequency is $\\frac{1}{3}$.\n\nNegative slope for $S(f)$ implies correlation over scales of $\\frac{1}{2\\pi f}$.\n\n## Examples\nFor radio stations, spectrum flattens at lowest frequencies for some statistics\n\nPower spectrums of waveforms produce peaks, take PSD of wvaeform energy, after bandpassing from 100 Hz to 10 kHz.\n\n- Concerto: 1/f below 1 Hz, 1-10 Hz has rhythm.\n\n- 12 Hour radio:\n    - 1/f above 2e-3 Hz (no correlations beyond 100s) for music.\n    - 5e-4 is still 1/f, correlations over 5 min\n\n- Pitch can be measured by the rate of zero crossings\n- Classical - 1/f all the way, Jazz + blues 1/f down to selection length.\n- Speech - correlations at 0.1 sceonds, and announcer time of 100s.\n- White for individuals speaking, below 3 hz (as in, one individual the whole time.)\n- note that this function is relative, if we introduce more speakers, suddenly there are longer time scale correlations.\n\n## Music Generation\n- Replacing white noise with pink noise helps music generation, increases predictability. If the rolloff is too much, then it\u0027s too predictable.\n\n"}, {"color": "#808080", "fixed": false, "font": {"color": "white"}, "id": "Reinforcement Learning", "label": "Reinforcement Learning", "mass": 3.45, "shape": "dot", "size": 18.57417562100671, "title": ""}, {"color": "#808080", "fixed": false, "font": {"color": "white"}, "id": "ReaLChords", "label": "ReaLChords", "mass": 0.49, "shape": "dot", "size": 7.0, "title": ""}, {"color": "#808080", "fixed": false, "font": {"color": "white"}, "id": "CS 285", "label": "CS 285", "mass": 2.47, "shape": "dot", "size": 15.716233645501712, "title": ""}, {"color": {"background": "#808080", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\classes\\Actor-Critic/", "id": "Actor Critic", "label": "Actor Critic", "mass": 0.98, "shape": "dot", "size": 9.899494936611665, "title": "# Actor Critic\n\nTo do - transfer\n\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#808080", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\classes\\Policy-Gradient/", "id": "Policy Gradient", "label": "Policy Gradient", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# Policy Gradient\n\nTo do - transfer\n\n\nLast Reviewed: 4/30/25"}, {"color": {"background": "#808080", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\concepts\\Generalized-Advantage-Estimation/", "id": "Generalized Advantage Estimation", "label": "Generalized Advantage Estimation", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# Generalized Advantage Estimation\nDefine this:\n![alt text](image-2.png)\nThe first term is actor critic, next state minus current state\nThe last term is pure policy gradient, with value function as baseline.\n\nGAE takes a weighted sum between all these terms:\n![alt text](image-1.png)\n\n\n$\\lambda$ thus determines the bias-variance tradeoff. Policy gradient has high variance. One step actor critic has high bias due to reliance on value function, but low variance since we are only considering values between $t$ and $t+1$.\n\n"}, {"color": {"background": "#808080", "border": "white", "borderWidth": 1}, "fixed": false, "font": {"color": "white"}, "href": "https://maswang32.github.io/knowledgemap/notes\\papers\\RLOO/", "id": "RLOO", "label": "RLOO", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "# RLOO\n- Taking RL out of RLHF - preference training\n- RLHF - supervised fine tuning\n- Reward model, trained as a binary classifier\n- RL step - maximize subject to current distribution, with KL penalty, to prevent reward hacking.\n- PPO - unnecessarily complicated\n    - clipped loss prevents catastrophic gradient updates\n    - difficult to tune\n- REINFORCE - estimator, 1990s, provides update rule to maximize reward under a policy.\n    - unbiased baseline - expectation doesn\u0027t move when trying to optimize\n    - actor critic reduces variance.\n- no need to have a parametrized baseline - could have moving average - moving average of all rewards throughout training.\n    - not super strong.\n\n## RLOO\n- RLOO - leave one out - use additional samples to create a parameter free baseline.\n    - generate additional samples.\n- PPO \n    - Generally, GAE is the nob that controls bias-variance in PPO.\n    - lamba = 0.95, turning up all the way to 1, you get value function as return.\n- Generally, don\u0027t introduce bias to reduce variance. just vary lamba - \n    - smaller lamba = the worse optimization is\n- Clipping is not necessary\n    - large ratios are rare and not needed in RLHF.\n    - importance sampled ratio has high variance, large ratios are not something we see in RLHF.\n    - 1-3% this clipping is activated in RLHF.\n\n- Sequence as action - reward is only attributed to the EOS token, but all other tokens carry a KL penalty\n- does this really make sense?\n- Entire sequence is an action, instead of each token.\n- In LLM, initial policy is unusually strong.\n- All the probability mass is contained in top 32 tokens.\n- Not thta many actoins that are probabe."}]);
                  edges = new vis.DataSet([{"arrows": "to", "color": "#C41E3A", "from": "Group Theory", "to": "Math", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Linear Algebra", "to": "Math", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Calculus", "to": "Math", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Gradients", "to": "Calculus", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Gradients", "to": "Linear Algebra", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Chain Rule", "to": "Gradients", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Infinitesimals", "to": "Calculus", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Functions", "to": "Math", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Information Theory", "to": "Math", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "A Brief Introduction To Information", "to": "Information Theory", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Deep Learning Chapter 3", "to": "Information Theory", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "KL Divergence", "to": "Information Theory", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Six Interpretations of KL Divergence", "to": "KL Divergence", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Entropy", "to": "Deep Learning Chapter 3", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Cross Entropy", "to": "Deep Learning Chapter 3", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Info Theory Basics", "to": "Deep Learning Chapter 3", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Statistics", "to": "Math", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Biased vs Unbiased Estimates", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Uniform Width Sampling", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Random-Variables-and-Probability-Distributions", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Bayes", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Conditional Independence", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Optimization", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "Momentum, RMSProp, Adam", "to": "Optimization", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Machine Learning", "to": "Math", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Machine Learning", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#e24b31", "from": "Linear Classifiers", "to": "Machine Learning", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Backpropagation", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Backpropagation", "to": "Chain Rule", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Normalization", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Batchnorm", "to": "Normalization", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Positional Encodings", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Interpretability", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Concept Activation Vectors", "to": "Interpretability", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Fine Tuning", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Activation Functions", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Pocketed Activations", "to": "Activation Functions", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Gated Activations", "to": "Activation Functions", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Understanding Deep Learning", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "MLP Interpretation - UDL", "to": "Understanding Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Loss Functions - UDL", "to": "Understanding Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Optimization - UDL", "to": "Understanding Deep Learning", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "Optimization - UDL", "to": "Optimization", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Gradients and Initialization - UDL", "to": "Understanding Deep Learning", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "Gradients and Initialization - UDL", "to": "Optimization", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Gradients and Initialization - UDL", "to": "Chain Rule", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Generative Modeling", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "VAEs - UDL", "to": "Understanding Deep Learning", "width": 4}, {"arrows": "to", "color": "#FFD900", "from": "VAEs - UDL", "to": "Generative Modeling", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "ELBO", "to": "Optimization", "width": 4}, {"arrows": "to", "color": "#c876a3", "from": "ELBO", "to": "VAEs - UDL", "width": 4}, {"arrows": "to", "color": "#e87267", "from": "Jensens Inequality", "to": "ELBO", "width": 4}, {"arrows": "to", "color": "#FFD900", "from": "Energy Based Generative Models", "to": "Generative Modeling", "width": 4}, {"arrows": "to", "color": "#FFD900", "from": "Diffusion Models", "to": "Generative Modeling", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "DDPM - UDL", "to": "Understanding Deep Learning", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "DDPM - UDL", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#c876a3", "from": "DDPM - Math", "to": "DDPM - UDL", "width": 4}, {"arrows": "to", "color": "#c876a3", "from": "DDPM - Reparametrization", "to": "DDPM - UDL", "width": 4}, {"arrows": "to", "color": "#c876a3", "from": "DDPM - Noise Schedules", "to": "DDPM - UDL", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Diffusion Best Practices", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Diffusion Forcing", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Diffusion Beats GANs", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "History Guidance", "to": "Diffusion Forcing", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "DiT", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Edify Image", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Understanding Diffusion Models: A Unified Perspective", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Score Based Generative Models", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Generative Modeling Using SDEs", "to": "Score Based Generative Models", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Wiener Process", "to": "Calculus", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Wiener Process", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Wiener Process", "to": "Generative Modeling Using SDEs", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Classifier Free Guidance", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "DDIM", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Elucidated Diffusion Models", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Training", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "A Recipe for Training Neural Networks", "to": "Training", "width": 4}, {"arrows": "to", "color": "#3FFF57", "from": "DiffWave", "to": "Audio", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "DiffWave", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#3FFF57", "from": "DAC", "to": "Audio", "width": 4}, {"arrows": "to", "color": "#79443B", "from": "VAR", "to": "Vision", "width": 4}, {"arrows": "to", "color": "#FFD900", "from": "VAR", "to": "Generative Modeling", "width": 4}, {"arrows": "to", "color": "#79443B", "from": "Gaussian Splatting", "to": "Vision", "width": 4}, {"arrows": "to", "color": "#79443B", "from": "Advances In Computer Vision", "to": "Vision", "width": 4}, {"arrows": "to", "color": "#79443b", "from": "Image Formation", "to": "Advances In Computer Vision", "width": 4}, {"arrows": "to", "color": "#79443b", "from": "Linear Image Processing", "to": "Advances In Computer Vision", "width": 4}, {"arrows": "to", "color": "#79443B", "from": "Geometric Deep Learning", "to": "Vision", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Geometric Deep Learning", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#79443B", "from": "CNNs", "to": "Vision", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "CNNs", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#762b9a", "from": "UNet", "to": "CNNs", "width": 4}, {"arrows": "to", "color": "#FFD900", "from": "PixelVAE", "to": "Generative Modeling", "width": 4}, {"arrows": "to", "color": "#79443B", "from": "PixelVAE", "to": "Vision", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Language Modeling", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#00FF00", "from": "Language Modeling from Scratch", "to": "Language Modeling", "width": 4}, {"arrows": "to", "color": "#00FF00", "from": "Transformers", "to": "Language Modeling", "width": 4}, {"arrows": "to", "color": "#00ff00", "from": "Encoder Decoder Transformers", "to": "Transformers", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Filesystems", "to": "Software", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Python", "to": "Software", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Hydra", "to": "Python", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Slurm", "to": "Software", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Vector Operations", "to": "Software", "width": 4}, {"arrows": "to", "color": "#212129", "from": "Einsum", "to": "Vector Operations", "width": 4}, {"arrows": "to", "color": "#212129", "from": "ML Systems", "to": "Software", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "ML Systems", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#35208d", "from": "Mixed Precision", "to": "ML Systems", "width": 4}, {"arrows": "to", "color": "#35208d", "from": "Wandb", "to": "ML Systems", "width": 4}, {"arrows": "to", "color": "#35208d", "from": "PyTorch", "to": "ML Systems", "width": 4}, {"arrows": "to", "color": "#212129", "from": "PyTorch", "to": "Python", "width": 4}, {"arrows": "to", "color": "#2f2259", "from": "Lightning", "to": "PyTorch", "width": 4}, {"arrows": "to", "color": "#35208d", "from": "Large Scale Deep Learning", "to": "ML Systems", "width": 4}, {"arrows": "to", "color": "#35208d", "from": "Webdataset", "to": "Large Scale Deep Learning", "width": 4}, {"arrows": "to", "color": "#35208d", "from": "Distributed Training", "to": "Large Scale Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Distributed Training", "to": "Training", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "DDSP", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "DDSP", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "PQMF", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Downsampling and Stretching", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Convolution", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Transpose Convolution", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Discrete Fourier Transform", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Fourier Dualities", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Pink Frequency Profiles", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "1f Noise in Music and Speech", "to": "Pink Frequency Profiles", "width": 4}, {"arrows": "to", "color": "#808080", "from": "ReaLChords", "to": "Reinforcement Learning", "width": 4}, {"arrows": "to", "color": "#808080", "from": "CS 285", "to": "Reinforcement Learning", "width": 4}, {"arrows": "to", "color": "#808080", "from": "Actor Critic", "to": "CS 285", "width": 4}, {"arrows": "to", "color": "#808080", "from": "Policy Gradient", "to": "CS 285", "width": 4}, {"arrows": "to", "color": "#808080", "from": "Generalized Advantage Estimation", "to": "Actor Critic", "width": 4}, {"arrows": "to", "color": "#808080", "from": "RLOO", "to": "Reinforcement Learning", "width": 4}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {"nodes": {"borderWidth": 1, "borderWidthSelected": 3, "chosen": true, "shape": "dot", "font": {"size": 20, "color": "white"}}, "edges": {"color": {"inherit": true}, "smooth": false}, "physics": {"enabled": true, "solver": "hierarchicalRepulsion", "hierarchicalRepulsion": {"nodeDistance": 150, "centralGravity": 0.01, "springLength": 150, "springConstant": 0.001, "damping": 0.5}, "stabilization": {"enabled": true, "iterations": 2000, "fit": true}, "direction": "UD", "minVelocity": 0.75, "maxVelocity": 30}, "interaction": {"zoomView": true, "dragView": true, "zoomSpeed": 0.5, "mouseWheel": true}};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    
    <script type="text/javascript">
    // Wait until the network is fully initialized
    document.addEventListener("DOMContentLoaded", function() {
        // 'network' should be available as it is defined in the generated HTML
        network.on('click', function(params) {
            if (params.nodes.length > 0) {
                var nodeId = params.nodes[0];
                var nodeData = network.body.data.nodes.get(nodeId);
                if (nodeData.href) {
                    window.open(nodeData.href, '_blank');
                }
            }
        });
    });
    </script>
    </body>
    
</html>