<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 100vh;
                 background-color: #000000;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#C41E3A", "fixed": false, "font": {"color": "white"}, "id": "Math", "label": "Math", "mass": 7.87, "shape": "dot", "size": 28.053520278211074, "title": ""}, {"color": "#FF6F20", "fixed": false, "font": {"color": "white"}, "id": "Statistics", "label": "Statistics", "mass": 4.569999999999999, "shape": "dot", "size": 21.37755832643195, "title": ""}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "Information Theory", "label": "Information Theory", "mass": 1.29, "shape": "dot", "size": 11.357816691600547, "title": ""}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "Linear Algebra", "label": "Linear Algebra", "mass": 2.0000000000000004, "shape": "dot", "size": 14.142135623730951, "title": ""}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "Calculus", "label": "Calculus", "mass": 2.0000000000000004, "shape": "dot", "size": 14.142135623730951, "title": ""}, {"color": "#ff6f20", "fixed": false, "font": {"color": "white"}, "id": "Optimization", "label": "Optimization", "mass": 2.3200000000000003, "shape": "dot", "size": 15.231546211727817, "title": ""}, {"color": "#ff6f20", "fixed": false, "font": {"color": "white"}, "id": "Momentum, RMSProp, Adam", "label": "Momentum, RMSProp, Adam", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "Last Reviewed: 11/9/24\n\nNotes from \"A visual explanation\"\n\nMomentum in Physics - F = ma, a force will cause a constant change in velocity.\nSame as momentum in ML - momentum = velocity, \nforces = decay (friction), and the additional gradient\nderivative = applying a force for one time frame, leading to an acceleration (change in velocity)\nmomentum helps with plateaus and local minima\n\n\nAdaGrad - history of squared gradients for a direction accumulate, updates in that direction are divided by this\nencourages exploration in directions where not many changes have happened\nescapes saddle points better - regular GD optimizes steeper features first\nslow b/c squared gradient accumuates\n\n\nRMSProp - squared gradients decay, squared gradients have momentum\n\nAdam - gradients have momentum, so do squared gradients.\nmomentum allows for escaping local minima\nsum of squares = explore new directions\n\n\n\nNotes from Andrew NG:\nMomentum cancels oscillations\nCorrections are usually applied to Adam so things get rolling earlier\n\n\n\n\n"}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "Gradients", "label": "Gradients", "mass": 2.0000000000000004, "shape": "dot", "size": 14.142135623730951, "title": "Last Reviewed: 10/27/24\n\nGradient indicates direction of highest increase\nGradient specifies linearization (plane) of the function up to an offset (derivative gets rid of +C)\nGradient direction specifies plane orientation\nGradient magnitude specifies plane slope\nPlane tells you all directional derivatives"}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "Chain Rule", "label": "Chain Rule", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "Last Reviewed: 10/27/24\n\nReference Page: #1\nUnivariate Chain Rule - \u0027speeding up\u0027 interperation.\n\u0027boosting\u0027 at a point\n\nAll derivatives are evaluated at the same point, just in different input domains\n\nMultivariate chain rule, dx,dy can be separated due to linearization.\nIncreases accumulate across dx, and dy.\n\nextending to multi-in, multi-out\nviewing things in terms of unit changes after linearization.\n\nKey Idea: we can think of moving dx in x, and then moving dy in y,\nand seeing how much f changes. This will be the same as moving in the directional derivative,\nsince for linear functions, the slope is the same everywhere.\n\nKey Idea: to compute df/ds, linearize everything, move one unit in s, and see how much that affects f.\n\nThe linearity assumption is the assumes that changes in variables will affect the output independently.\n\nwherever a function has a derivative, it is locally linear\n\n\nReference Page: #1"}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "Functions", "label": "Functions", "mass": 0.01, "shape": "dot", "size": 5, "title": "Last Reviewed: 12/1/24\n\npolynomials are a linear combination of x, x**2, x**3, as functions\n"}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "A Brief Introduction To Information", "label": "\u201cA Brief Introduction To Information\u201d", "mass": 0.04, "shape": "dot", "size": 5, "title": "Last Reviewed: 11/9/24\n\nAll information is communication - it requires a method of decoding, must be interpreted.\nAll digital info = bits\nSetup: communicate a sequence of random events\nstreaming setting requires prefix to disambiguate - I proved this\n"}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "Deep Learning Chapter 3", "label": "\u201cDeep Learning Chapter 3\u201d", "mass": 0.7500000000000001, "shape": "dot", "size": 8.660254037844387, "title": ""}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "KL Divergence", "label": "KL Divergence", "mass": 0.5000000000000001, "shape": "dot", "size": 7.0710678118654755, "title": "Last Reviewed: 10/27/24\nAsymmetric\nDKL(P||Q) - symbols are drawn from P, but if we encode assuming drawn from Q, \nhow many extra bits on expectation are used\nEx~p[log(P(x))-log(Q(x))]\nAsymmetric b/c depends on which distribution you\u0027re sampling from - two examples\n(Should read more)\n\nhow much more suprised you\u0027d be seeing P while expecting Q\nReference Page: #2"}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "Six Interpretations of KL Divergence", "label": "\u201cSix Interpretations of KL Divergence\u201d", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "Last Reviewed: 1/20/25\n\nLottery game:\n\nFor a bet of c on an outcome x, the house pays you\nc / q(x)\nwhere q(x) is the likelihood they assign to the outcome x.\n\nSuppose you know the true distribution of outcomes p(x).\nTo maximize your winnings, you should bet proportional to p(x).\n\nSuppose you bet 1 dollar total. Then you optimally bet p(x) dollars for each outcome.\n\nThen the expected log-winnings are \nsum( p(x) log(p(x)/q(x))    )\n\nIn other words, KL(p,q) = max amount of log-money that can be made off one dollar,\nwhen the payoffs are assigned by the distribution q, but the real distribution is p.\n\nTo do: review other interpretations\n\n\n\n\n"}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "Entropy", "label": "Entropy", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "Last Reviewed: 10/27/24\n\nExpected information in a distribution\nmeasures uncertainty in a probability distribution\nBernoulli Example\nReference Sheet #3."}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "Cross Entropy", "label": "Cross Entropy", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "Last Reviewed: 1/20/25\n\n-\u00e2\u02c6\u00ab(p(x) log(q(x) dx))\n\nEntropy is:\n-\u00e2\u02c6\u00ab(p(x) log(p(x) dx))\nCall this Ent(p, p)\n\nCross Entropy is:\n-\u00e2\u02c6\u00ab(p(x) log(q(x) dx))\nCall this Ent(p, q)\n\nKL Divergence is:\n- \u00e2\u02c6\u00ab(p(x) log(q(x) dx)) - (-\u00e2\u02c6\u00ab(p(x) log(p(x) dx)))\nOr\n\u00e2\u02c6\u00ab(p(x) log(q(x) dx))  + \u00e2\u02c6\u00ab(p(x) log(p(x) dx))\n\nThis is Ent(p, q) - Ent(p,p)\n\nWhen we add KL divergence and entropy, we get cross entropy\nCross entropy = number of bits it takes to encode samples from P using an encoding trained on Q\nEntropy = number of bits it takes to encode samples from P using an encoding trained on P\nKL Divergence = number of extra bits it takes to encode samples from P using an encoding trained on Q.\nOr, KL divergence is cross entropy minus entropy."}, {"color": "#c41e3a", "fixed": false, "font": {"color": "white"}, "id": "Info Theory Basics", "label": "Info Theory Basics", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "Last Reviewed: 10/27/24\n\nInformation is -logP(x) for an event\nIndependent events have additive infomration\nLess likely events have higher Information\nknowing outcome of an event with 50% prob has 1 bit of information\nMeasured in nats or bits (recall logs of all bases are proportional)\n0 information if certain\n\nsetup: a bitstream encodes a sequence of random vairables. Prefix requirements impose a cost of 2^l\n\nReference Sheet #3, 3.1"}, {"color": "#ff6f20", "fixed": false, "font": {"color": "white"}, "id": "Random Variables", "label": "Random Variables", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "Last Reviewed: 1/19/25\n\nRandom Variable x - a variable that can take on different values. e.g, result of dice roll.\nDistribution Pr(x) - maps r.v. values to densities.\nN_x(0,I) - describes the distribution of x - as opposed to some other random variable.\nN_x(f(z), I) is a function mapping x to probability values, not some other random variable.\n\nExample:\nSuppose we have Pr(x) = integral( N_x(f(z), I), N_z(0,I)) dz.\nPr(x) is a function mapping x to probabilities\nOn the right hand side, for a given input x,\nWe would substitute that value of x in for N_x(x|z).\nThe value of z we would use is determined by the integrand.\n\nTo evaluate Pr(x=0.5), we would\n1. Iterate through all values of z\n2. Plug in f(z) for the these values to get the parameters (mean and variance) for the distribution\n3. Plug in x into this distribution to get a probability value.\n4. Plug in z into N_z(0,I) to get a probability value\n5. Multiply the outputs of the two normals and add it to the integral value.\n\nIn other words, the \u0027x\u0027 in N_x describes what we use for the function\u0027s input.\nThe parameters are how we describe the function."}, {"color": "#ff6f20", "fixed": false, "font": {"color": "white"}, "id": "Bayes", "label": "Bayes", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "Last Reviewed: 1/25/25\n\n\n\nWe can ignore terms that are constant with regard to the distribution we are computing.\nFor instance, for a fixed x*,\n\np(z | x*)  = (p(x* | z) p(z)) / p(x*)\n\nbut we can ignore p(x*) since we are interested in a distribution with respect to z.\nTo get this distribution, we can evaluate p(x* | z) p(z) at all z and ensure it integrates to 1\nby rescaling it by C\nignoring the need for p(x*) term (which is 1/C).\n\nTO DO: Find notes about \u0027evidence\u0027 in Bayes\n\n\n"}, {"color": "#ff6f20", "fixed": false, "font": {"color": "white"}, "id": "Conditional Independence", "label": "Conditional Independence", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "Last Reviewed: 1/25/25\n\ne.g., in diffusion models, we have\n\nq(z_2 | z_1, x) = q(z_2 | z_1)\n\nsince z_1 provides all information needed to compute z_2, thus given z_1 as information,\nz_2 is independent from x, or x provides no \u0027additional information\u0027."}, {"color": "#0000FF", "fixed": false, "font": {"color": "white"}, "id": "Deep Learning", "label": "Deep Learning", "mass": 13.609999999999998, "shape": "dot", "size": 36.89173349139343, "title": ""}, {"color": "#212129", "fixed": false, "font": {"color": "white"}, "id": "Software", "label": "Software", "mass": 1.0, "shape": "dot", "size": 10.0, "title": ""}, {"color": "#35208d", "fixed": false, "font": {"color": "white"}, "id": "PyTorch", "label": "PyTorch", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "Last Reviewed: 1/20/25\n\nDatasets:\n---need __len__ and __get_item__\n\nDataloaders\n---collate_fn defines how the different data examples should be turned into a batch\n\nBackwards:\n---fills \"grad\" field of every tensor that requires it\n\nZero_grad\n---turns \"grad\" field of every tensor that requires it to 0\n\noptimizer.step()\n---the optimizer has a bunch of parameters stored in it, and it looks at the gradient of all the parameters\nand then does a backward step"}, {"color": "#0000ff", "fixed": false, "font": {"color": "white"}, "id": "Activation Functions", "label": "Activation Functions", "mass": 0.08000000000000002, "shape": "dot", "size": 5, "title": ""}, {"color": "#0000ff", "fixed": false, "font": {"color": "white"}, "id": "Pocketed Activations", "label": "Pocketed Activations", "mass": 0.04, "shape": "dot", "size": 5, "title": "Last Reviewed: 1/3/25\n\nDead ReLU problem - activation ranges get super negative\nPocketed Activation (Swish, Mish) - activations get stuck in pocket, since it\u0027s a local minima\nEnough examples can remove from pocket\n\nGeLU is the same as setting the dropout probabilty to the CDF of the neuron value, and taking the expectation\n\nThink about this more"}, {"color": "#0000ff", "fixed": false, "font": {"color": "white"}, "id": "Gated Activations", "label": "Gated Activations", "mass": 0.04, "shape": "dot", "size": 5, "title": "Last Reviewed: 1/17/25\n\nGLU = (Ax + b)*sigma(Cx + D)\nSwiGLU = (Ax + b)*swish(Cx + D)\nSwiGLU has this squared part (derivative vanishes near zero)\nReLU^2 also does well, perhaps due to this square part\nSnake has an x^2 term in its expansion"}, {"color": "#0000ff", "fixed": false, "font": {"color": "white"}, "id": "Understanding Deep Learning", "label": "\u201cUnderstanding Deep Learning\u201d", "mass": 5.57, "shape": "dot", "size": 23.600847442411894, "title": ""}, {"color": "#0000ff", "fixed": false, "font": {"color": "white"}, "id": "MLP Interpretation - UDL", "label": "\u201cMLP Interpretation - UDL\u201d", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "Last Recall: 10/24/24\nLast Reviewed: 11/1/24\n\nShallow MLPs clip linear functions, rescale, and combine.\nD hidden units means D+1 Linear Regions\nMultivariate outputs are clipped at the same joints\nMultivariate Input Visualization\nAll ReLU MLPs split input space into Linear Regions\nFolding\nAdding a Layer is clipping Each Linear Region, and recombining\nBottlenecks are restricting weights to outer product\nDepth efficiency is exponential compared to width efficiency\nDepth generalizes and trains better\nSwishes solve Dying ReLU\nWeights can be rescaled as long as biases are too\nDepth approximation theorem"}, {"color": "#0000ff", "fixed": false, "font": {"color": "white"}, "id": "Loss Functions - UDL", "label": "\u201cLoss Functions - UDL\u201d", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "Last Reviewed: 11/1/24\n\nNegative Log Likelihood is Loss\nFormula: Model predicts parameters of a distribution, on which the probability of data is evaluated.\nMaximize probability of data, minimize negative log probability of data.\nAssume data is independent\nMSE results from assuming y is sampled from gaussians with means determined by x\nheterodastic = variance varies with input\n\nBCE loss comes from assuming the distribution y|x is Bernoulli (there\u0027s a visualization)\nMulticlass cross entropy loss is discussed\nTable of distributions from different tasks is presented\n\nin multi-output situations, assume different outputs are conditionally independent.\n\nNLL minimization is the same as minimizing cross entropy between (possibly conditional on input) data distributions.\n\nReference Sheet: UDL Chapter 5\n"}, {"color": "#FFD900", "fixed": false, "font": {"color": "white"}, "id": "Generative Modeling", "label": "Generative Modeling", "mass": 7.55, "shape": "dot", "size": 27.477263328068172, "title": ""}, {"color": "#c876a3", "fixed": false, "font": {"color": "white"}, "id": "VAEs - UDL", "label": "\u201cVAEs - UDL\u201d", "mass": 1.32, "shape": "dot", "size": 11.489125293076057, "title": "Last Reviewed: 1/19/25\n\nVAEs do not let you evaluate p(x)\nMLE training is not trivial, but can define a lower bound\nmodel an \u0027unobserved\u0027 latent variable, the thing that \u0027gives rise\u0027 to the image/sound\np(x) = integral(P(x,z)dz) = integral(p(x|z)p(z)dz)\nExample - Mixture of Gaussians, z describes which gaussian.\n\nVAE - P(z) is N(O,I), and P(x|z) is N(f(z), s^2I)\nIn other words, z maps to a gaussian\u0027s mean, and the distribution is a marginalization of all these gaussians over z.\n\nSee image where one distribution is made up of as a \u0027marginalization\u0027 (sum) of gaussians\n\nGeneration: sample from P(z) then P(x|z)\n\nEvaluating/maximizing p(x) slash sum(log(p(x)))is intractable.\n\nA model that maximizes it could try to assign big probabilities to your data, without being restricted to \nintegrating to 1.\n\nTrying to restrict it to integrate to 1 is intractable.\n\n\n\n\nNote:\np(z|x) is the \u0027posterior.\u0027 What could the latent variable be after observing x?\np(z) is the \u0027prior\u0027 on the latent variable.\np(x|z) is the \u0027likelihood\u0027. This helps us evaluate the Posterior, since we want to see, for each value of z,\nwhat is the probability we could have gotten that x? it\u0027s detective work.\nTo evaluate the posterior, Bayes Rule would say:\n\np(z|x) = p(x|z)p(z)/p(x).\nBut really, we only need the numerator, since we can make it integrate to 1, and p(x) does not determine the\nrelative probabilites of the z\u0027s.\n1---Compute p(x|z) for each value of z\n2---multiply by p(z)\n3---normalize so the posterior p(z|x) sums to 1\nThere\u0027s a diagram of this in UDL.\n\np(x) is called the evidence"}, {"color": "#e87267", "fixed": false, "font": {"color": "white"}, "id": "ELBO", "label": "ELBO", "mass": 0.32000000000000006, "shape": "dot", "size": 5.656854249492381, "title": "Last Reviewed: 1/19/25\n\nLots of math here. You can reprove this by hand from Jensen\u0027s inequality or look at your notes\n\nBasically, start with log(p(x)).\n---Then express it using a latent variable model decomposition\n---choose an arbitrary q(z) as your \u0027weighting\u0027\nint(  log(    q(z) * p(x,z)/q(z)         )  dz)\n---apply Jensen\u0027s inequality\nwhen you get to p(x,z) split it up into p(z|x) and p(x)\nthat will let you take out p(x), and also give you a KL term\n\nELBO = log(p(x)) - KL(q(z), p(z|x))) (this KL is assuming q is \u0027ground truth\u0027)\n\nMaximizing p(x) with respect to the parameters for q(z) and p(z|x) involves expectation maximization, this means\n---can improve ELBO\u0027s lower bound by changing p(z|x) slash p(x|z)\u0027s parametrization\nOR\n---can make ELBO bound more tight by changing q(z)\u0027s parametrization"}, {"color": "#e87267", "fixed": false, "font": {"color": "white"}, "id": "Jensens Inequality", "label": "Jensens Inequality", "mass": 0.16, "shape": "dot", "size": 5, "title": "Last Reviewed: 1/19/25\n\nImagine a bunch of datapoints lying on log(y)\n\nImagine a point (E[y], E[log[y]])\n\nThis is the midpoint of all the datapoints\nThis will lie under the log(y) curve by concavity:\nf((1-a)x + a*y) \u003e= (1-a)f(x) + a*f(y)\n\nThe midpoint will lie under the log curve\nIt is thus lower than\n\n(E[y], log(E[y])) which is on the curve.\n\nTo Do: Prove Jensen\u0027s Inequality\n\n\n"}, {"color": "#c63598", "fixed": false, "font": {"color": "white"}, "id": "Optimization - UDL", "label": "\u201cOptimization - UDL\u201d", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "Last Reviewed: 11/9/24\n\n\n\n\nReference Sheet: UDL Chapter 6\n"}, {"color": "#ffd900", "fixed": false, "font": {"color": "white"}, "id": "Diffusion Models", "label": "Diffusion Models", "mass": 5.739999999999998, "shape": "dot", "size": 23.958297101421877, "title": ""}, {"color": "#c876a3", "fixed": false, "font": {"color": "white"}, "id": "Diffusion Models - UDL", "label": "\u201cDiffusion Models - UDL\u201d", "mass": 1.2500000000000002, "shape": "dot", "size": 11.180339887498949, "title": "Last Reviewed: 1/23/25"}, {"color": "#c876a3", "fixed": false, "font": {"color": "white"}, "id": "Noise Schedule", "label": "Noise Schedule", "mass": 0.25, "shape": "dot", "size": 5.0, "title": "Last Reviewed: 1/23/25\n\nz_0 = x\nz_t = x*sqrt(1-b_t) + sqrt(b_t)*episilon\n\nNote that (sqrt(1-b_t))^2 + sqrt(b_t)^2 = 1\n\n\nNote: \nEach Diffusion step preserves the second moment:\nVar[X] = E[X^2] - E[X]^2 = \n"}, {"color": {"background": "#ffd900", "border": "white"}, "fixed": false, "font": {"color": "white"}, "id": "Understanding Diffusion Models: A Unified Perspective", "label": "\u201cUnderstanding Diffusion Models: A Unified Perspective\u201d", "mass": 4.0, "shape": "dot", "size": 20.0, "title": "Last Recall: 7/14/24"}, {"color": "#3FFF57", "fixed": false, "font": {"color": "white"}, "id": "Audio", "label": "Audio", "mass": 0.98, "shape": "dot", "size": 9.899494936611665, "title": ""}, {"color": "#baee38", "fixed": false, "font": {"color": "white"}, "id": "DiffWave", "label": "\u201cDiffWave\u201d", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "Last Reviewed: 1/23/25"}, {"color": "#3fff57", "fixed": false, "font": {"color": "white"}, "id": "DAC", "label": "\u201cDAC\u201d", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "Last Reviewed: 1/17/25\n\nResidual blocks with dilations 1, 3, 9 slash kernel size 7, and depthwise 1x1\nThese are chained together.\n\nDownsampling blocks that double channel\n\nUpsampling blocks that halve channel\n\nSnake activations\n\nfeature matching loss, multiscale STFT discriminator, mel-reconstruction loss\n"}, {"color": "#79443B", "fixed": false, "font": {"color": "white"}, "id": "Vision", "label": "Vision", "mass": 0.98, "shape": "dot", "size": 9.899494936611665, "title": ""}, {"color": "#bb8c35", "fixed": false, "font": {"color": "white"}, "id": "VAR", "label": "\u201cVAR\u201d", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "Last Reviewed: 1/17/25\n\nWorks in VQ-VAE Latent Space\n\nEncoding:\nTake 64x64 latent, squash to 1x1, quantize.\nlookup, stretch to K x K, compute residual\nSquash residual to 4x4, quantize residual\nlookup, stretch to K x K, compute residual\n...\nvery similar to RVQ but with \u0027squashing\u0027\n\nAutoregressive \u0027next-scale\u0027 predictions,\npredict first scale, then all of next scale in parallel, etc.\n\nraster scan order bad - disrupts locality, makes infilling hard due to bidirectional correlation, inefficient.\n"}, {"color": "#79443b", "fixed": false, "font": {"color": "white"}, "id": "Gaussian Splatting", "label": "\u201cGaussian Splatting\u201d", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "Last Reviewed: 1/21/25\n\nSuper Fast Rendering - realtime display rates at 1080p\nHigher quality than Mip-Nerf\nAvoids unnecessary computation in empty space\n\nNerf and voxel-based 3D representations require stochastic sampling for rendering - computationally expensive, result in noise\n\nuse sparse point clouds obtained from SFM - no need for MVS\n1-5 million gaussians per scene\n\nRendering:\n-respect visibility ordering\n-sorting\n-tile-based rasterization\n\n\nopacity-based rendering --- \n---the \u0027opacity\u0027 (light absorption) assigned to a point\ndepends on the distance from the previously sampled point, AND the density at the point\nbasically, you are assuming the \u0027point\u0027 is a block that absorbs light\n\nThese are continuous representations, (not voxels)\nworks with randomly initalized gaussians\ncan project gaussians to 2D and perform alpha blending\n\nParametrize each gaussian with:\n    opacity, anisotropic covariance, spherical harmonics\n    \nadaptive density control - add or remove gaussians during training"}, {"color": "#00FF00", "fixed": false, "font": {"color": "white"}, "id": "Language Modeling", "label": "Language Modeling", "mass": 1.0, "shape": "dot", "size": 10.0, "title": ""}, {"color": "#00ff00", "fixed": false, "font": {"color": "white"}, "id": "Language Modeling from Scratch", "label": "\u201cLanguage Modeling from Scratch\u201d", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "Last Recall: 6/14/24\nLast Reviewed: 6/1/24\n\nBPE\nFew-Shot/Zero Shot Generalization\nScaling Laws - parameters, data, training time, result in linear log-log curves with loss"}, {"color": "#00ff00", "fixed": false, "font": {"color": "white"}, "id": "Transformers", "label": "Transformers", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "Last Recall: 6/14/24\nLast Reviewed: 6/1/24\n\nTransformer Basics\nRotary Embeddings (Review)\nLayerNorm, projecting latent onto hypersphere\nMQA, GQA\nSwiGLU\nPrenorm vs postnorm"}, {"color": "#0000ff", "fixed": false, "font": {"color": "white"}, "id": "Interpretability", "label": "Interpretability", "mass": 0.49, "shape": "dot", "size": 7.0, "title": ""}, {"color": "#0000ff", "fixed": false, "font": {"color": "white"}, "id": "Concept Activation Vectors", "label": "\u201cConcept Activation Vectors\u201d", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "Last Reviewed: 10/27/24\n\nSpecify a concept by collecting examples\ntrain a classifier on these examples wrt random examples or another group (e.g. stripes vs dots)\ntake the orthogonal vector to this classifier (CAV)\ncompute the directional derivative of a class label (e.g. zebra) wrt CAV\ncan use to tell which concepts inform classifier decision\nother use cases (see notes)\nReference # 1"}, {"color": "#0000ff", "fixed": false, "font": {"color": "white"}, "id": "Fine Tuning", "label": "Fine Tuning", "mass": 0.0, "shape": "dot", "size": 5, "title": "Last Reviewed: 10/26/24\n\nLoRA\nControlNet\nSEGA"}, {"color": "#a8326f", "fixed": false, "font": {"color": "white"}, "id": "Signal Processing", "label": "Signal Processing", "mass": 2.45, "shape": "dot", "size": 15.652475842498529, "title": ""}, {"color": "#8a16b5", "fixed": false, "font": {"color": "white"}, "id": "DDSP", "label": "DDSP", "mass": 0.49, "shape": "dot", "size": 7.0, "title": ""}, {"color": "#a8326f", "fixed": false, "font": {"color": "white"}, "id": "PQMF", "label": "PQMF", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "Last Reviewed: 1/2/2024\n\nFilter signal into low band and high band\nDownsample both\nupsample both\nlow pass low band, high pass subband\nin the downsampled signal, the high frequencies are mirrored, and occupy the low frequencies.\nSpecial case of audio CNNs\nSee PQMF.ipynb"}, {"color": "#a8326f", "fixed": false, "font": {"color": "white"}, "id": "Downsampling and Stretching", "label": "Downsampling and Stretching", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "Last Reviewed: 1/2/2025\n\nDownsampling \u0027folds\u0027 the FFT spectrum on itself.\ne.g. downsampling by factor of 2 - imagine slicing spec. in half,\nthen overlaying them.\n\nStretching replicates the FFT spectrum (doubles length of FFT)\nstretching by x2 mirrors spectrum,\nstretching by x3 appends the forward spectrum again\netc."}, {"color": "#a8326f", "fixed": false, "font": {"color": "white"}, "id": "Convolution", "label": "Convolution", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "Last Reviewed: 1/3/25\n\nConvolution in Neural Networks:\nDilated Convolution = replicating spectrum of filter\ndilated kernel size = (kernel_size - 1) * dilation + 1\nleads to higher frequency resolution (number of unique points)\nstrided convolution = conv plus downsampling\n\nnote that before downsampling, trailing entries are discarded.\n# of trailing entires discarded = stride - 1\ntherefore, the target size after conv only needs to be size - (stride - 1).\nTherefore, the input size after padding needs to be size - (stride - 1) + (kernel - 1), since the kernel takes away kernel - 1 units\nThis is equal to size + kernel - stride, so the padding needs to be (kernel - stride)/2.\nif stride is even, we therefore want an even kernel.\n\n\nGraph:\n   X X X X X X X X    - input\n[] X X X X X X X X [] - input after padding\n    X X X X X X X     - after conv, kernel size 4\n    X   X   X   X     - after downsampling\n\nOtherwise:\nTwo interpretations:\n1---reverb (overlapping kernels)\n2---flipping and shifting\nthe \u0027flipped\u0027 kernel is a function. The x is the \u0027offset\u0027 and the y is the \u0027weight\u0027.\ni.e., how does input at time t + x affect output at time t.\n"}, {"color": "#a8326f", "fixed": false, "font": {"color": "white"}, "id": "Transpose Convolution", "label": "Transpose Convolution", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "Last Reviewed: 1/3/25\n\nWhen stride = 1, it\u0027s overlaying kernels\nThis is the \u0027superposition\u0027 interpretation of convolution\nit is the same as convolution with a flipped kernels\n\nWhen stride \u003e 1, it\u0027s the same as conv(stretch(x))\n\nSee the notebook you emailed Julius\n\nFilter the \u0027mirrored\u0027 copy of the signal\n\nGradient of Conv (makes forward and backwards same, natural way of upsampling)\n\nthe padding parameter is the same as truncating on both sides\n"}, {"color": "#808080", "fixed": false, "font": {"color": "white"}, "id": "Reinforcement Learning", "label": "Reinforcement Learning", "mass": 1.4899999999999998, "shape": "dot", "size": 12.206555615733702, "title": ""}, {"color": "#808080", "fixed": false, "font": {"color": "white"}, "id": "ReaLChords", "label": "\u201cReaLChords\u201d", "mass": 0.49, "shape": "dot", "size": 7.0, "title": "Last Recall: 9/25/24"}, {"color": "#808080", "fixed": false, "font": {"color": "white"}, "id": "CS 285", "label": "\u201cCS 285\u201d", "mass": 1.0, "shape": "dot", "size": 10.0, "title": "Last Recall: 9/25/24"}]);
                  edges = new vis.DataSet([{"arrows": "to", "color": "#C41E3A", "from": "Statistics", "to": "Math", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Information Theory", "to": "Math", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Linear Algebra", "to": "Math", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Calculus", "to": "Math", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Optimization", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "Momentum, RMSProp, Adam", "to": "Optimization", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Gradients", "to": "Calculus", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Gradients", "to": "Linear Algebra", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Chain Rule", "to": "Gradients", "width": 4}, {"arrows": "to", "color": "#C41E3A", "from": "Functions", "to": "Math", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "A Brief Introduction To Information", "to": "Information Theory", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Deep Learning Chapter 3", "to": "Information Theory", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "KL Divergence", "to": "Information Theory", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Six Interpretations of KL Divergence", "to": "KL Divergence", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Entropy", "to": "Deep Learning Chapter 3", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Cross Entropy", "to": "Deep Learning Chapter 3", "width": 4}, {"arrows": "to", "color": "#c41e3a", "from": "Info Theory Basics", "to": "Deep Learning Chapter 3", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Random Variables", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Bayes", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#FF6F20", "from": "Conditional Independence", "to": "Statistics", "width": 4}, {"arrows": "to", "color": "#212129", "from": "PyTorch", "to": "Software", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "PyTorch", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Activation Functions", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Pocketed Activations", "to": "Activation Functions", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Gated Activations", "to": "Activation Functions", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Understanding Deep Learning", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "MLP Interpretation - UDL", "to": "Understanding Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Loss Functions - UDL", "to": "Understanding Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Generative Modeling", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "VAEs - UDL", "to": "Understanding Deep Learning", "width": 4}, {"arrows": "to", "color": "#FFD900", "from": "VAEs - UDL", "to": "Generative Modeling", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "ELBO", "to": "Optimization", "width": 4}, {"arrows": "to", "color": "#c876a3", "from": "ELBO", "to": "VAEs - UDL", "width": 4}, {"arrows": "to", "color": "#e87267", "from": "Jensens Inequality", "to": "ELBO", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Optimization - UDL", "to": "Understanding Deep Learning", "width": 4}, {"arrows": "to", "color": "#ff6f20", "from": "Optimization - UDL", "to": "Optimization", "width": 4}, {"arrows": "to", "color": "#FFD900", "from": "Diffusion Models", "to": "Generative Modeling", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Diffusion Models - UDL", "to": "Understanding Deep Learning", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Diffusion Models - UDL", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#c876a3", "from": "Noise Schedule", "to": "Diffusion Models - UDL", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "Understanding Diffusion Models: A Unified Perspective", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#3FFF57", "from": "DiffWave", "to": "Audio", "width": 4}, {"arrows": "to", "color": "#ffd900", "from": "DiffWave", "to": "Diffusion Models", "width": 4}, {"arrows": "to", "color": "#3FFF57", "from": "DAC", "to": "Audio", "width": 4}, {"arrows": "to", "color": "#79443B", "from": "VAR", "to": "Vision", "width": 4}, {"arrows": "to", "color": "#FFD900", "from": "VAR", "to": "Generative Modeling", "width": 4}, {"arrows": "to", "color": "#79443B", "from": "Gaussian Splatting", "to": "Vision", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Language Modeling", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#00FF00", "from": "Language Modeling from Scratch", "to": "Language Modeling", "width": 4}, {"arrows": "to", "color": "#00ff00", "from": "Transformers", "to": "Language Modeling from Scratch", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Interpretability", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#0000ff", "from": "Concept Activation Vectors", "to": "Interpretability", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "Fine Tuning", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "DDSP", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#0000FF", "from": "DDSP", "to": "Deep Learning", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "PQMF", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Downsampling and Stretching", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Convolution", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#a8326f", "from": "Transpose Convolution", "to": "Signal Processing", "width": 4}, {"arrows": "to", "color": "#808080", "from": "ReaLChords", "to": "Reinforcement Learning", "width": 4}, {"arrows": "to", "color": "#808080", "from": "CS 285", "to": "Reinforcement Learning", "width": 4}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {"nodes": {"borderWidth": 2, "borderWidthSelected": 3, "chosen": true, "shape": "dot", "font": {"size": 20, "color": "white"}}, "edges": {"color": {"inherit": true}, "smooth": false}, "physics": {"enabled": true, "solver": "hierarchicalRepulsion", "hierarchicalRepulsion": {"nodeDistance": 150, "centralGravity": 0.01, "springLength": 150, "springConstant": 0.001, "damping": 0.5}, "stabilization": {"enabled": true, "iterations": 2000, "fit": true}, "direction": "UD", "minVelocity": 0.75, "maxVelocity": 30}, "interaction": {"zoomView": true, "dragView": true, "zoomSpeed": 0.5, "mouseWheel": true}};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>